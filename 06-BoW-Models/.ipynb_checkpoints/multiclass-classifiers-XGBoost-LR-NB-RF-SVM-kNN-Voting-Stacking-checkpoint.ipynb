{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-CLASS BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from random import randint\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from bbclf.bb_classifier import BinaryBiasClassifier\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "    '''\n",
    "        To use in Pipeline():\n",
    "        model = Pipeline( steps=[('vect', vectorizer), ('to_dense', DenseTransformer()), ('lda', lda)] )\n",
    "        model.fit( X_train, y_train )\n",
    "    '''\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        #return X.todense()\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix( cm, classes, title='Confusion matrix', figsize=(5,5),\n",
    "                           cmap=plt.cm.PuBu ):   # originally plt.cm.Blues; also good: BuPu,RdPu,PuRd,OrRd,Oranges\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix\n",
    "    \"\"\"\n",
    "    plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "    plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "            \n",
    "    plt.figure(figsize=figsize)\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.05)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True labels')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample( df_, to_oversample_, random_state_=None ):\n",
    "    '''\n",
    "        Upsample df_ by to_oversample_ more samples\n",
    "    '''        \n",
    "    # OVERSAMPLE AND CONCAT W/ORIGINAL DF_\n",
    "    replace = False\n",
    "    if len(df_) < to_oversample_:\n",
    "        replace = True        \n",
    "\n",
    "    df_upsampled = df_.sample( n=to_oversample_, replace=replace, random_state=random_state_ )\n",
    "        \n",
    "    return pd.concat([ df_, df_upsampled ]).sample(frac=1, random_state=random_state_).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_all( df_, random_state ):\n",
    "    '''\n",
    "        Upsample each class in df_ to the number of data points\n",
    "        in the majority class\n",
    "    '''\n",
    "        \n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_['target'].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_['target'] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "                \n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "        \n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "    \n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load, Deduplicate, Usample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10932, 15)\n",
      "(10786, 15)\n"
     ]
    }
   ],
   "source": [
    "# LOAD MAIN DATASET\n",
    "df = pd.read_pickle('./data/file_name.pkl')\n",
    "print(df.shape)\n",
    "df = df[ df['label'].isin(ml_categories) ]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "random_state  = 47\n",
    "label_to_key = {\n",
    "     'class0': 0,\n",
    "     'class1': 1,\n",
    "     'class2': 2,\n",
    "     'class3': 3,\n",
    "     'class4': 4,\n",
    "     'class5': 5,\n",
    "}\n",
    "ml_categories = list( label_to_key.keys() )\n",
    "print(len(label_to_key))\n",
    "print(len(ml_categories))\n",
    "\n",
    "key_to_label = {v: k for k,v in label_to_key.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     4609\n",
      "6     1994\n",
      "14    1110\n",
      "12     996\n",
      "10     620\n",
      "9      412\n",
      "7      314\n",
      "4      281\n",
      "11     278\n",
      "2      252\n",
      "8      208\n",
      "5      160\n",
      "3      150\n",
      "13     131\n",
      "1      109\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# GET TARGET FOR ENTIRE CONCATENATED DF (NOT BEFORE!)\n",
    "df['target'] = df['label'].map( label_to_key )\n",
    "df['target'] = df['target'].astype(int)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEDUPLICATE AT THE SENTENCE-LABEL LEVEL\n",
    "temp = df[ df.duplicated(subset=['sentence', 'label'], keep=False) ].sort_values(by='sentence')\n",
    "print(temp.shape)\n",
    "temp[['sentence', 'label',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11499, 18)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['sentence', 'label'], keep='first').reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before upsampling:\n",
      "0     3592\n",
      "6     1603\n",
      "14     914\n",
      "12     804\n",
      "10     506\n",
      "9      346\n",
      "7      261\n",
      "4      230\n",
      "11     230\n",
      "2      217\n",
      "8      177\n",
      "5      131\n",
      "3      128\n",
      "13     109\n",
      "1       96\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = df[ df['subset'] == 'train' ].copy()\n",
    "df_test  = df[ df['subset'] == 'test' ].copy()\n",
    "print('Before upsampling:\\n', df_train['target'].value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After upsampling:\n",
      "14    3592\n",
      "13    3592\n",
      "12    3592\n",
      "11    3592\n",
      "10    3592\n",
      "9     3592\n",
      "8     3592\n",
      "7     3592\n",
      "6     3592\n",
      "5     3592\n",
      "4     3592\n",
      "3     3592\n",
      "2     3592\n",
      "1     3592\n",
      "0     3592\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#### UPSAMPLE\n",
    "df_train = upsample_all( df_train, random_state )\n",
    "print('After upsampling:\\n', df_train['target'].value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get X, y, and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "['a', 'also', 'an', 'and', 'as', 'at', 'be', 'been', 'being', 'am', 'are', 'is', 'was', 'were', 'but', 'by', 'for', 'from', 'have', 'having', 'has', 'had', 'in', 'it', 'much', 'of', 'on', 'one', 'thank', 'thanks', 'that', 'those', 'the', 'this', 'these', 'to', 'very', 'will']\n"
     ]
    }
   ],
   "source": [
    "# GET STOPWORDS\n",
    "file = 'data/stopwords_no_lemmas.txt'\n",
    "with open(file) as f:\n",
    "    sw = f.readlines()\n",
    "sw = [i.strip() for i in sw]\n",
    "print(len(sw))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets: (53880,) (2155,) (53880,) (2155,)\n",
      "Labels in train set:   {0: 3592, 1: 3592, 2: 3592, 3: 3592, 4: 3592, 5: 3592, 6: 3592, 7: 3592, 8: 3592, 9: 3592, 10: 3592, 11: 3592, 12: 3592, 13: 3592, 14: 3592}\n",
      "Labels in test set:    {0: 918, 1: 13, 2: 33, 3: 22, 4: 49, 5: 28, 6: 386, 7: 52, 8: 30, 9: 63, 10: 110, 11: 45, 12: 189, 13: 21, 14: 196}\n"
     ]
    }
   ],
   "source": [
    "feature_col = 'sentence'\n",
    "X_train = df_train[feature_col].values\n",
    "y_train = df_train['target'].values\n",
    "X_test  = df_test[feature_col].values\n",
    "y_test  = df_test['target'].values\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle(X_train, y_train, random_state=random_state)\n",
    "X_test, y_test   = sklearn.utils.shuffle(X_test, y_test, random_state=random_state)\n",
    "print('Shape of datasets:', X_train.shape, X_test.shape, y_train.shape, y_test.shape )\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print('Labels in train set:  ', dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print('Labels in test set:   ', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# GET STOPWORDS\n",
    "file = 'data/stopwords_no_lemmas.txt'\n",
    "with open(file) as f:\n",
    "    sw = f.readlines()\n",
    "sw = [i.strip() for i in sw]\n",
    "print(len(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_orig  = ['a', 'an', 'the', 'thank',]\n",
    "sw_prec  = [ 'achieve', 'one', 'do', 'success', 'team', 'continue', 'hard', 'beyond', 'focus', 'deliver',\n",
    "             'project', 'much', 'and', 'thank', 'other', 'like', 'happy', 'make', 'say', 'want', 'of',\n",
    "             'role', 'good', 'with', 'we', 'month', 'an', \"'s\", 'some', 'very', 'what', 'can', 'sure',\n",
    "             'she', 'which', 'a', 'everything', 'again', 'you', 'move', 'your', 'change', 'at', 'ensure',\n",
    "             'effort', 'best', 'by', 'take', 'the', 'get', 'work', 'lot', 'his', 'proud', 'for', ]     # all\n",
    "sw_prec2 = [ 'a', 'an', 'the', 'thank', \"'s\", 'at', 'team', 'month', 'hard', 'work', 'your', 'can',\n",
    "             'for', 'make', 'best', 'continue', 'of', 'very', 'what', 'we', 'one', 'she', 'want', ]    # short\n",
    "all_15   = [ 'a', 'an', 'the', 'thank', 'you', 'with', 'to', 'time', 'this', 'that', 'take', 'person',\n",
    "              'our', 'on', 'not', 'it', 'in', 'have', 'great', 'good', 'ellipsis', 'do', 'be', 'as', 'and', 'always',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_xgb = 47\n",
    "sw2 = [ 'a','an','the','of','thank','you' ]\n",
    "\n",
    "clf_params_xgb = {\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': None,\n",
    "    'learning_rate': 0.3,                                # eta\n",
    "    'objective': 'multi:softmax',                        # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'mlogloss',                           # multiclass - merror, mlogloss\n",
    "    'base_score': 0.25,\n",
    "    'booster': 'gbtree',                                 # gbtree, dart\n",
    "    'tree_method': 'approx',                             # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                           # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0.25,                                       # larger - more conservative, [0, inf], default 0\n",
    "    'reg_alpha': 0.5,                                    # L1 reg., larger - more conservative, default 0\n",
    "    'reg_lambda': 1,                                     # L2 rreg., larger - more conservative, default 1\n",
    "    'sampling_method': 'uniform',                        # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                 # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.75,                                   # 0-1  (lower values prevent overfitting)    \n",
    "    'colsample_bylevel': 0.8,                            # 0-1\n",
    "    'colsample_bynode': 0.75,                            # optimized for higher recall\n",
    "    'colsample_bytree': 0.75,                            # 0-1  \n",
    "    'seed': 5,\n",
    "    'num_class': 15,\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': random_state_xgb,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "vect_params_xgb = {\n",
    "    'max_df': 1.0,                             \n",
    "    'min_df': 5,    \n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,8), \n",
    "    'binary': True,\n",
    "    'stop_words': None,\n",
    "}\n",
    "\n",
    "#vectorizer = TfidfVectorizer( **vect_params_xgb )\n",
    "vectorizer = CountVectorizer( **vect_params_xgb )\n",
    "\n",
    "clf = XGBClassifier( **clf_params_xgb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT AND TEST MODEL\n",
    "model_xgb = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model_xgb.fit( X_train, y_train )\n",
    "y_pred = model_xgb.predict( X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION REPORT\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFUSION MATRIX\n",
    "cm = confusion_matrix( y_test, y_pred )\n",
    "plot_confusion_matrix( cm, labels, figsize=(10,10), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'model_binaries/20220122_xgboost_char_071_061.pkl'\n",
    "with open(file,'wb') as f:\n",
    "    pickle.dump(model_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = [ 'of', 'a','an','the', 'thank' ]\n",
    "\n",
    "# VECTORIZER PARAMETERS\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 2,    \n",
    "    'analyzer': 'word',\n",
    "    'ngram_range': (1,2),\n",
    "    'binary': False,\n",
    "    'stop_words': None,#sw,\n",
    "}\n",
    "\n",
    "# CLASSIFIER PARAMETERS\n",
    "clf_params = {\n",
    "    \n",
    "    'C': 1.0,                      # default=1.0\n",
    "    'kernel': 'linear',            # default=’rbf’, {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}\n",
    "    'degree': 3,                   # default=3, degree for polynomial f(x)\n",
    "    'tol': 1e-3,                   # stopping criteria, default=1e-3\n",
    "    'gamma': 'scale',              # default=’scale’, kernel coeff for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "                                   # 'scale' => 1 / (n_features * X.var()), ‘auto’ => 1 / n_features\n",
    "    'coef0': 0.0,                  # default=0.0, independent term in kernel function in ‘poly’ and ‘sigmoid’\n",
    "    'shrinking': True,             # default=True'\n",
    "    'cache_size': 200,             # default=200,   size of the kernel cache (in MB)\n",
    "    'decision_function_shape': 'ovr',    # default=’ovr’, {‘ovo’, ‘ovr’}, multiclass => always 'ovo'\n",
    "    'break_ties': False,           # default=False, for decision_function_shape='ovr' and num classes>2 (longer)\n",
    "    'max_iter': -1,                # default=-1,    limit on iterations\n",
    "    'class_weight': None,          # default=None,  dict or ‘balanced'\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer( **vect_params )\n",
    "#vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "clf = SVC( **clf_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT AND TEST MODEL\n",
    "model_svm = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model_svm.fit( X_train, y_train )\n",
    "\n",
    "y_pred = model_svm.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y_test, y_pred )\n",
    "plot_confusion_matrix( cm, labels, figsize=(10,10), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw, sw_prec, sw_prec2, all_15\n",
    "sw2 = [ 'of', 'a','an','the', 'thank' ]\n",
    "\n",
    "# VECTORIZER PARAMETERS\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 2,    \n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,8),\n",
    "    'binary': False,\n",
    "    'stop_words': sw2,\n",
    "}\n",
    "\n",
    "# CLASSIFIER PARAMETERS\n",
    "clf_params_rf = {   \n",
    "    \n",
    "    'solver': 'liblinear',          # default=’lbfgs’ {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "                                    # small dataset => ‘liblinear’ big dataset => ‘sag’ and ‘saga’ (faster);\n",
    "                                    # multiclass => ‘newton-cg’, ‘sag’, ‘saga’, ‘lbfgs’; ‘liblinear’ only for ovr\n",
    "                                    # supported penalties by solver: ‘newton-cg’, ‘lbfgs’, ‘sag’ - [‘l2’, ‘none’],\n",
    "                                    # ‘liblinear’ - [‘l1’, ‘l2’],\n",
    "                                    # ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]\n",
    "\n",
    "    'penalty': 'l2',                # ‘l1’, ‘l2’, ‘elasticnet’ (both), ‘none’, default=’l2’ (not for al solvers)\n",
    "    'class_weight': None,           # default=None, dict or ‘balanced'\n",
    "    \n",
    "    'multi_class': 'auto',          #  default=’auto’, {‘auto’, ‘ovr’, ‘multinomial’},\n",
    "                                    # 'ovr’ => binary problem fit for each label\n",
    "                                    # ‘multinomial’ => multinom. loss fit across entire prob distrib.\n",
    "                                    # ‘auto’ selects ‘ovr’ for binary classif. or solver=’liblinear’,\n",
    "                                    # otherwise ‘multinomial’\n",
    "        \n",
    "    'max_iter': 500,                # default=100, iters for solvers to converge\n",
    "    'C': 1.0,                       # default 1.0, inverse regularization strength, smaller => stronger regularization\n",
    "    'dual': False,                  # default=False (dual formulation only for l2  with liblinear solver\n",
    "                                    # Prefer dual=False when n_samples > n_features\n",
    "    'tol': 1e-4,                    # stopping criteria, default=1e-4\n",
    "    'fit_intercept': True,          # default True; whether to fit bias / interceptbe added to the decision function\n",
    "    'intercept_scaling': 1,         # default=1, for solver ‘liblinear’ and self.fit_intercept=True (additional term)\n",
    "    'l1_ratio': None,               # default = None, elastic-Net mixing param, [0,1],\n",
    "                                    # only for penalty='elasticnet'. l1_ratio=0 => penalty='l2',\n",
    "                                    # l1_ratio=1 => penalty='l1', combination of L1 and L2 if in between    \n",
    "    'verbose': 0,\n",
    "    'warm_start': False,    \n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "clf = LogisticRegression( **clf_params_rf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT AND TEST MODEL\n",
    "model_lr = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model_lr.fit( X_train, y_train )\n",
    "\n",
    "# testset 1\n",
    "y_pred = model_lr.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y_test, y_pred )\n",
    "plot_confusion_matrix( cm, labels, figsize=(10,10), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'model_binaries/20220122_lr_char_15way_071_060.pkl'\n",
    "with open(file,'wb') as f:\n",
    "    pickle.dump(model_lr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47 \n",
    "clf_params_xgb = {\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': None,\n",
    "    'learning_rate': 0.3,                                # eta\n",
    "    'objective': 'multi:softmax',                        # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'mlogloss',                           # multiclass - merror, mlogloss\n",
    "    'base_score': 0.25,\n",
    "    'booster': 'gbtree',                                 # gbtree, dart\n",
    "    'tree_method': 'approx',                             # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                           # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0.25,                                       # larger - more conservative, [0, inf], default 0\n",
    "    'reg_alpha': 0.5,                                    # L1 reg., larger - more conservative, default 0\n",
    "    'reg_lambda': 1,                                     # L2 rreg., larger - more conservative, default 1\n",
    "    'sampling_method': 'uniform',                        # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                 # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.75,                                   # 0-1  (lower values prevent overfitting)    \n",
    "    'colsample_bylevel': 0.8,                            # 0-1\n",
    "    'colsample_bynode': 0.75,                            # optimized for higher recall\n",
    "    'colsample_bytree': 0.75,                            # 0-1  \n",
    "    'seed': 5,\n",
    "    'num_class': 15,\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "vect_params_xgb = {\n",
    "    'max_df': 1.0,                             \n",
    "    'min_df': 5,    \n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,8), \n",
    "    'binary': True,\n",
    "    'stop_words': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sentence', CountVectorizer()\n",
    "random_state    = 47\n",
    "vect_params_lr1 = {\n",
    "                    'max_df': 1.0,\n",
    "                    'min_df': 2,    \n",
    "                    'analyzer': 'word',\n",
    "                    'ngram_range': (1,2),\n",
    "                    'binary': True,\n",
    "                    'stop_words': None,\n",
    "}\n",
    "clf_params_lr1 = {\n",
    "                    'solver': 'liblinear',\n",
    "                    'penalty': 'l1',        \n",
    "                    'max_iter': 500,\n",
    "                    'random_state': random_state,\n",
    "                    'n_jobs': -1,\n",
    "}\n",
    "vect_params_lr2 = {\n",
    "                    'max_df': 1.0,\n",
    "                    'min_df': 2,    \n",
    "                    'analyzer': 'char',\n",
    "                    'ngram_range': (1,8),\n",
    "                    'binary': False,\n",
    "                    'stop_words': None,\n",
    "}\n",
    "clf_params_lr2 = {\n",
    "                    'solver': 'liblinear',\n",
    "                    'penalty': 'l2',        \n",
    "                    'max_iter': 500,\n",
    "                    'random_state': random_state,\n",
    "                    'n_jobs': -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sentence', TfidfVectorizer()\n",
    "vect_params_svm = {\n",
    "                    'max_df': 1.0,\n",
    "                    'min_df': 2,    \n",
    "                    'analyzer': 'word',\n",
    "                    'ngram_range': (1,2),\n",
    "                    'binary': False,\n",
    "                    'stop_words': None,\n",
    "}\n",
    "\n",
    "# CLASSIFIER PARAMETERS\n",
    "clf_params_svm = {    \n",
    "                    'kernel': 'linear',\n",
    "                    'random_state': random_state,\n",
    "                    'probability': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD EACH CLASSIFIER\n",
    "lr   = Pipeline( steps=[ ('vect', CountVectorizer( **vect_params_lr1 )),\n",
    "                         ('clf',  LogisticRegression( **clf_params_lr1 )) ] )\n",
    "\n",
    "lr2  = Pipeline( steps=[ ('vect', CountVectorizer( **vect_params_lr2 )),\n",
    "                         ('clf',  LogisticRegression( **clf_params_lr2 )) ] )\n",
    "\n",
    "svm  = Pipeline( steps=[ ('vect', TfidfVectorizer( **vect_params_svm )),\n",
    "                         ('clf',  SVC( **clf_params_svm )) ] )\n",
    "\n",
    "xgb  = Pipeline( steps=[ ('vect', CountVectorizer( **vect_params_xgb )),\n",
    "                         ('clf',  XGBClassifier( **clf_params_xgb )) ] )\n",
    "\n",
    "estimators = [ ('lr', lr), ('lr2', lr2), ('svm', svm), ('xgb', xgb), ]\n",
    "weights    = [ 0.25, 0.25, 0.25, 0.25 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              Pipeline(steps=[('vect',\n",
       "                                               CountVectorizer(binary=True,\n",
       "                                                               min_df=2,\n",
       "                                                               ngram_range=(1,\n",
       "                                                                            2))),\n",
       "                                              ('clf',\n",
       "                                               LogisticRegression(max_iter=500,\n",
       "                                                                  n_jobs=-1,\n",
       "                                                                  penalty='l1',\n",
       "                                                                  random_state=47,\n",
       "                                                                  solver='liblinear'))])),\n",
       "                             ('lr2',\n",
       "                              Pipeline(steps=[('vect',\n",
       "                                               CountVectorizer(analyzer='char',\n",
       "                                                               min_df=2,\n",
       "                                                               ngram_range=(1,\n",
       "                                                                            8))),\n",
       "                                              ('clf',\n",
       "                                               LogisticRegression(max_iter=500,\n",
       "                                                                  n...\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             n_estimators=150,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             num_class=15,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             objective='multi:softmax',\n",
       "                                                             random_state=47,\n",
       "                                                             reg_alpha=0.5,\n",
       "                                                             reg_lambda=1,\n",
       "                                                             sampling_method='uniform',\n",
       "                                                             scale_pos_weight=None,\n",
       "                                                             seed=5,\n",
       "                                                             subsample=0.75,\n",
       "                                                             tree_method='approx',\n",
       "                                                             use_label_encoder=False, ...))]))],\n",
       "                 n_jobs=-1, voting='soft', weights=[0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILD VOTING CLASSIFIER ON TOP OF BASE CLASSIFIERS\n",
    "model_vc = VotingClassifier( estimators        = estimators,\n",
    "                             weights           = weights,\n",
    "                             voting            = 'soft',              # soft, hard\n",
    "                             flatten_transform = True,     \n",
    "                             n_jobs            = -1,\n",
    "                           )\n",
    "model_vc.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_vc.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels, digits=4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y_test, y_pred )\n",
    "plot_confusion_matrix( cm, labels, figsize=(10,10), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'model_binaries/20220125_voting_classifier_4_models_073_062.pkl'\n",
    "with open(file,'wb') as f:\n",
    "    pickle.dump(model_vc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators = [ ('lr', lr), ('lr2', lr2), ('svm', svm), ('xgb', xgb), ]\n",
    "estimators      = [ ('lr', lr), ('svm', svm) ]\n",
    "#final_estimator = LogisticRegression(max_iter=1000)        # 0.69/0.54\n",
    "#final_estimator = SVC()                                    # 0.68, 0.5\n",
    "#final_estimator = RandomForestClassifier()                 # 0.65/0.4\n",
    "#final_estimator = XGBClassifier()                          # 0.65, 0.39\n",
    "\n",
    "#final_estimator = LinearDiscriminantAnalysis()\n",
    "#final_estimator = KNeighborsClassifier\n",
    "#final_estimator = MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('lr',\n",
       "                                Pipeline(steps=[('vect',\n",
       "                                                 CountVectorizer(binary=True,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              2))),\n",
       "                                                ('clf',\n",
       "                                                 LogisticRegression(max_iter=500,\n",
       "                                                                    n_jobs=-1,\n",
       "                                                                    penalty='l1',\n",
       "                                                                    random_state=47,\n",
       "                                                                    solver='liblinear'))])),\n",
       "                               ('svm',\n",
       "                                Pipeline(steps=[('vect',\n",
       "                                                 TfidfVectorizer(min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              2))),\n",
       "                                                ('clf',\n",
       "                                                 SVC(kernel='linear',\n",
       "                                                     probability=True,\n",
       "                                                     random_state=47))]))],\n",
       "                   final_estimator=RandomForestClassifier(), n_jobs=-1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sc = StackingClassifier( estimators=estimators,\n",
    "                               final_estimator=final_estimator,\n",
    "                               stack_method='auto',            #auto, predict_proba, decision_function, predict\n",
    "                               cv=None,                        # num folds (5 if None) \n",
    "                               passthrough=False,              # whether pass data to final estimator or just preds\n",
    "                               n_jobs=-1,\n",
    "                              )\n",
    "model_sc.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_sc.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels, digits=4, ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y_test, y_pred )\n",
    "plot_confusion_matrix( cm, labels, figsize=(10,10), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZER PARAMETERS\n",
    "sw_orig  = ['a', 'an', 'the', 'thank',]\n",
    "sw_prec  = [ 'achieve', 'one', 'do', 'success', 'team', 'continue', 'hard', 'beyond', 'focus', 'deliver',\n",
    "             'project', 'much', 'and', 'thank', 'other', 'like', 'happy', 'make', 'say', 'want', 'of',\n",
    "             'role', 'good', 'with', 'we', 'month', 'an', \"'s\", 'some', 'very', 'what', 'can', 'sure',\n",
    "             'she', 'which', 'a', 'everything', 'again', 'you', 'move', 'your', 'change', 'at', 'ensure',\n",
    "             'effort', 'best', 'by', 'take', 'the', 'get', 'work', 'lot', 'his', 'proud', 'for', ]     # all\n",
    "sw_prec2 = [ 'a', 'an', 'the', 'thank', \"'s\", 'at', 'team', 'month', 'hard', 'work', 'your', 'can',\n",
    "             'for', 'make', 'best', 'continue', 'of', 'very', 'what', 'we', 'one', 'she', 'want', ]    # short\n",
    "all_15   = [ 'a', 'an', 'the', 'thank', 'you', 'with', 'to', 'time', 'this', 'that', 'take', 'person',\n",
    "              'our', 'on', 'not', 'it', 'in', 'have', 'great', 'good', 'ellipsis', 'do', 'be', 'as', 'and', 'always',]\n",
    "\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 4,    \n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,8),\n",
    "    'binary': False,\n",
    "    'stop_words': all_15,\n",
    "}\n",
    "\n",
    "# CLASSIFIER PARAMETERS\n",
    "clf_params = {\n",
    "    'alpha': 1.0,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "\n",
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "clf = MultinomialNB( **clf_params )\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "lda = LinearDiscriminantAnalysis()    #solver='lsqr','eigen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF LOOKS SLIGHTLY BETTER? OR IS IT JUST NO CRODD-VALIDATED?\n",
    "model_nb = Pipeline( steps=[ ('vect', vectorizer), ('svd', svd),\n",
    "                               ('lda', lda), ], )\n",
    "model_nb.fit( X_train, y_train )\n",
    "\n",
    "# testset 1\n",
    "y_pred = model_nb.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance in NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 166143)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOTAL NUMBER OF CLASSES AND FEATURES\n",
    "classifier = model_nb['clf']\n",
    "classifier.feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT MOST IMPORTANT FEATURES PER CATEGORY\n",
    "print('PRINT 100 MOST IMPORTANT FEATURES FOR EACH CLASS:\\n')\n",
    "for i in range(len(classifier.classes_)):    \n",
    "    this_class = key_to_label[classifier.classes_[i]]\n",
    "    print('**********' + this_class.upper() + '********************')\n",
    "    prob_sorted = classifier.feature_log_prob_[i, :].argsort()\n",
    "    print(np.take(model_nb['vect'].get_feature_names(), prob_sorted[-100:]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET N MOST IMPORTANT FEATURES FOR EACH CATEGORY AND FIND FEATURES OCCURRING IN MAX NUMBER OF FEE_TYPES\n",
    "n = 1000\n",
    "important_features = dict()\n",
    "for i in range(len(classifier.classes_)):    \n",
    "    prob_sorted = classifier.feature_log_prob_[i, :].argsort()\n",
    "    important_features[clf.classes_[i]] = np.take(model_nb['vect'].get_feature_names(), prob_sorted[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES OCCURRING IN EACH CATEGORY\n",
    "feature_frequency = []\n",
    "vocabulary = set(np.concatenate(list(important_features.values()), axis=None))\n",
    "\n",
    "for feature in vocabulary:\n",
    "    count = 0\n",
    "    for key in important_features:\n",
    "        if feature in important_features[key]:\n",
    "            count += 1    \n",
    "    feature_frequency.append((count, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER IN EACH TUPLE MEANS THE NUMBER OF CATEGORIES IN WHICH THIS NGRAM OCCURS\n",
    "sorted(feature_frequency, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw2 = [ 'of', 'a','an','the', 'thank' ]\n",
    "\n",
    "# VECTORIZER PARAMETERS\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 2,    \n",
    "    'analyzer': 'word',\n",
    "    'ngram_range': (1,3),\n",
    "    'binary': True,\n",
    "    'stop_words': sw2,\n",
    "}\n",
    "\n",
    "# CLASSIFIER PARAMETERS\n",
    "clf_params_rf = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'gini',                         # “gini”, “entropy”\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'auto',                      # “auto”, “sqrt”, “log2”\n",
    "    'class_weight': 'balanced',                  # dict, 'balanced', 'balanced_subsample', None\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "\n",
    "clf = RandomForestClassifier( **clf_params_rf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT AND TEST MODEL\n",
    "model_rf = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model_rf.fit( X_train, y_train )\n",
    "\n",
    "# testset 1\n",
    "y_pred = model_rf.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZER PARAMETERS\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 5,    \n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,5),\n",
    "    'binary': True,\n",
    "    'stop_words': sw,\n",
    "}\n",
    "\n",
    "# CLASSIFIER PARAMETERS\n",
    "clf_params = {    \n",
    "    'n_neighbors': 5,\n",
    "    'weights': 'distance',     # default=’uniform’, {‘uniform’, ‘distance’}\n",
    "    'algorithm': 'auto',      # default=’auto’, {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}\n",
    "    'metric': 'minkowski',    # default=’minkowski’{ 'euclidean', 'cosine', } + sklearn.neighbors.VALID_METRICS['brute']\n",
    "    'p': 2,                   # default=2, p for minkowski distance\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "clf = KNeighborsClassifier( **clf_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT AND TEST MODEL\n",
    "model_knn = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model_knn.fit( X_train, y_train )\n",
    "\n",
    "y_pred = model_knn.predict( X_test )\n",
    "labels = [i[1] for i in sorted([(k,v) for k,v in key_to_label.items()])]\n",
    "print( classification_report( y_test, y_pred, target_names=labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'precomputed']"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sklearn.neighbors.VALID_METRICS_SPARSE['brute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4MB\n",
    "with open('model_pickle.pkl','wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_joblib.pkl']"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4MB, saves slower than pickle (stackoverflow - better for numpy arrays which are part of the model)\n",
    "joblib.dump(pipe, \"model_joblib.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pickle.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads slightly slower than pickle\n",
    "pipe = joblib.load(\"model_joblib.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
