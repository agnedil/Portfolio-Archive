{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa10723c",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Text Classification (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "## The Association for Computational Linguistics\n",
    "## WASSA 2023 Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages\n",
    "See more details [here](https://codalab.lisn.upsaclay.fr/competitions/10864#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import time\n",
    "import zipfile\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "import random\n",
    "import tiktoken\n",
    "import backoff\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8aaef7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91371be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    '''Return number of tokens used in a list of messages for ChatGPT'''\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        #print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        #print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        #print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56940aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new new version (Dec 2022)\n",
    "def upsample_all( df_, labels_col='target', random_state=47 ):\n",
    "    '''\n",
    "        Upsample each class in column labels_col of pandas dataframe df_\n",
    "        to the number of data points in majority class\n",
    "    '''\n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_[labels_col].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_[labels_col] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "\n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "\n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "\n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 4) (1191, 10) (1191, 1) (1191, 1)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/mcec_train_translated.pkl'\n",
    "df_train = pd.read_pickle(file1)\n",
    "\n",
    "file2    = 'data/mcec_dev_translated.pkl'\n",
    "df_dev   = pd.read_pickle(file2)\n",
    "\n",
    "file3    = 'data/mcec_test.csv'\n",
    "df_test  = pd.read_csv(file3)\n",
    "\n",
    "file4    = 'data/sample_submission/predictions_MCEC.csv'\n",
    "sample_submission = pd.read_csv(file4)\n",
    "\n",
    "print(df_train.shape, df_dev.shape, df_test.shape, sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5771f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                        Tension lene ki koi baat ni   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   \n",
       "2            Nai mje nai mili mail..mene check ki ti   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   \n",
       "\n",
       "                                          text_clean    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti  pessimism       0   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       0   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \n",
       "0                         Any talk of taking tangoes  \n",
       "1              I have gone home punch and now dreams  \n",
       "2                                Ni Ni Ni Mille Mail  \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  \n",
       "4                              But Wu runs the cedar  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy manually arbitrated translation into English from column 'gpt_translated2_corrected'\n",
    "#file = 'data/mcec_dev.xlsx'\n",
    "#df_dev2 = pd.read_excel( file )\n",
    "#print(df_dev2.shape)\n",
    "#df_dev2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a90fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dev['gpt_translated2_corrected'] = df_dev2['gpt_translated2_corrected'].values\n",
    "\n",
    "#file2  = 'data/mcec_dev_translated.pkl'\n",
    "#df_dev.to_pickle( file2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8a2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pessimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotion\n",
       "0    neutral\n",
       "1    neutral\n",
       "2  pessimism\n",
       "3    disgust\n",
       "4       fear"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission format\n",
    "print( type(sample_submission) )\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = df_train['emotion'].apply( lambda x: 0 if x=='neutral' else 1 )\n",
    "df_dev['target']   = df_dev['emotion'].apply( lambda x: 0 if x=='neutral' else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         3262\n",
      "trust           1118\n",
      "joy             1022\n",
      "optimism         880\n",
      "anticipation     832\n",
      "disgust          687\n",
      "sadness          486\n",
      "fear             453\n",
      "anger            226\n",
      "surprise         199\n",
      "love             187\n",
      "pessimism        178\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "1    6268\n",
      "0    3262\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes.I am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yes.i am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>Y. Um in Fap Lab Cabin. Butt Fap Presentations...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yar insan ka bcha bn chawliyn na mar :p</td>\n",
       "      <td>joy</td>\n",
       "      <td>Dude become a child of a human being, do not die.</td>\n",
       "      <td>Dude human beings do not die: P: P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terai uncle nai kahna hai kai ham nai to bahr ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>Your Uncle Nai says that we had sent out money</td>\n",
       "      <td>Your Ankali says that we sent out money and wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr ajao I m cming in the club</td>\n",
       "      <td>neutral</td>\n",
       "      <td>YR AJAO I'M Coming in the Club</td>\n",
       "      <td>Yer organs were the club</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...</td>\n",
       "      <td>joy</td>\n",
       "      <td>Mje wes nimra ahmad ka qur'aan ki aayaat k bar...</td>\n",
       "      <td>Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion  \\\n",
       "0  Yes.I am in fyp lab cabin.but fyp presentation...  neutral   \n",
       "1           Yar insan ka bcha bn chawliyn na mar :p       joy   \n",
       "2  Terai uncle nai kahna hai kai ham nai to bahr ...  disgust   \n",
       "3                      Yr ajao I m cming in the club  neutral   \n",
       "4  Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...      joy   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0  Yes.i am in fyp lab cabin.but fyp presentation...   \n",
       "1  Dude become a child of a human being, do not die.   \n",
       "2     Your Uncle Nai says that we had sent out money   \n",
       "3                     YR AJAO I'M Coming in the Club   \n",
       "4  Mje wes nimra ahmad ka qur'aan ki aayaat k bar...   \n",
       "\n",
       "                                       translated_ur  target  \n",
       "0  Y. Um in Fap Lab Cabin. Butt Fap Presentations...       0  \n",
       "1                 Dude human beings do not die: P: P       1  \n",
       "2  Your Ankali says that we sent out money and wa...       1  \n",
       "3                           Yer organs were the club       0  \n",
       "4  Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['emotion'].value_counts(), '\\n')\n",
    "print(df_train['target'].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1742539f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         388\n",
      "joy             131\n",
      "trust           125\n",
      "disgust         113\n",
      "optimism        110\n",
      "anticipation     94\n",
      "sadness          62\n",
      "fear             52\n",
      "surprise         35\n",
      "anger            35\n",
      "pessimism        29\n",
      "love             17\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "1    803\n",
      "0    388\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>gpt_pred_num</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       0   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       0   \n",
       "2            Nai mje nai mili mail..mene check ki ti  pessimism       1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       1   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       1   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \\\n",
       "0                         Any talk of taking tangoes   \n",
       "1              I have gone home punch and now dreams   \n",
       "2                                Ni Ni Ni Mille Mail   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                              But Wu runs the cedar   \n",
       "\n",
       "                                          text_clean  gpt_pred  gpt_pred_num  \\\n",
       "0                        Tension lene ki koi baat ni   neutral             1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   neutral             1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti   neutral             1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  negative             0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   neutral             1   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \n",
       "0                          There's no need to worry.  \n",
       "1   I have reached home and now I am going to sleep.  \n",
       "2      I didn't receive any new mail. I had checked.  \n",
       "3  I was busy the whole day on that day, they wer...  \n",
       "4       But he still walks with fear and hesitation.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_dev['emotion'].value_counts(), '\\n')\n",
    "print(df_dev['target'].value_counts())\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# light text cleaning (should I use clean regex for better accuracy?)\n",
    "pad_punct    = re.compile('([^a-zA-Z ]+)')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "#clean        = re.compile('[^a-zA-Z0-9,.?!\\'\\s]+')\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = pad_punct.sub(r' \\1 ', s)\n",
    "    #s = clean.sub(' ', s)\n",
    "    s = multi_spaces.sub(' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply( clean_text )\n",
    "df_dev['text_clean']   = df_dev['text'].apply( clean_text )\n",
    "df_test['text_clean']  = df_test['Text'].apply( clean_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e42251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 6)\n",
      "(4222, 6)\n",
      "(4221, 6)\n",
      "(4221, 6)\n"
     ]
    }
   ],
   "source": [
    "# 2K duplicates - these may affect claa imbalance during training! TO BE REDUCED\n",
    "print(df_train.shape)\n",
    "temp1 = df_train[ df_train.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_train[ df_train.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_train[ df_train.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7c2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 11)\n",
      "(82, 11)\n",
      "(82, 11)\n",
      "(68, 11)\n"
     ]
    }
   ],
   "source": [
    "# 82 duplicates ['clean_text', 'emotion'] - can't reduce because this is a dev set\n",
    "print(df_dev.shape)\n",
    "temp1 = df_dev[ df_dev.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_dev[ df_dev.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_dev[ df_dev.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c179df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 2)\n",
      "(93, 2)\n",
      "(93, 2)\n"
     ]
    }
   ],
   "source": [
    "# 93 complete duplicates - can't reduce because this is a test set\n",
    "print(df_test.shape)\n",
    "temp1 = df_test[ df_test.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp3 = df_test[ df_test.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f49ff482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 554, 526, 526)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train vs. df_dev: half of the dev set is in train set\n",
    "overlap1 = [t for t in df_train['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "overlap2 = [t for t in df_dev['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap1), len(overlap2), len(set(overlap1)), len(set(overlap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae49f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 584, 557, 557)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. rest\n",
    "overlap3 = [ t for t in df_train['text_clean'].tolist() + df_dev['text_clean'].tolist()\\\n",
    "             if t in df_test['text_clean'].tolist() ]\n",
    "overlap4 = [ t for t in df_test['text_clean'].tolist() if t in\\\n",
    "             df_train['text_clean'].tolist() + df_dev['text_clean'].tolist() ]\n",
    "len(overlap3), len(overlap4), len(set(overlap3)), len(set(overlap4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b78132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 97, 88, 88)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_dev: half of the dev set is in train set\n",
    "overlap5 = [t for t in df_dev['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap6 = [t for t in df_test['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "len(overlap5), len(overlap6), len(set(overlap5)), len(set(overlap6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13bff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727, 540, 519, 519)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_train: half of the dev set is in train set\n",
    "overlap7 = [t for t in df_train['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap8 = [t for t in df_test['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap7), len(overlap8), len(set(overlap7)), len(set(overlap8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a84db",
   "metadata": {},
   "source": [
    "The reason why baseline ML models perform better than ChatGPT is because they get a lot of hints due to duplicates from the training set! ChatGPT doesn't have this knowledge because it's doing a zero-shot classification! The number of duplicates is such that they would not fit the context window of ChatGPT anyway.\n",
    "\n",
    "The only way to compare ML and ChatGPT correctly is to remove all the duplicates from the TRAINING SET, then train ML model and test it the dev set and compare with ChatGPT! (also, deduplicate the training set)\n",
    "\n",
    "Submission: use non-overfit ML or ChatGPT (whichever is better) on those samples from the test set that don't have duplicates in the training or dev set. Use training/dev set labels for the duploicates in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fbd6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 2206\n",
      "(9530, 6)\n",
      "(8151, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove overlap with validation sets\n",
    "val_sets = df_dev['text_clean'].tolist() + df_test['text_clean'].tolist()\n",
    "print(len(val_sets), len(set(val_sets)))\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train = df_train[ ~df_train['text_clean'].isin(val_sets) ]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1293d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates from train set\n",
    "df_train = df_train.drop_duplicates(subset=['text_clean', 'emotion'])\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40905d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 2127),\n",
       " ('k', 1244),\n",
       " ('to', 1231),\n",
       " ('ha', 1214),\n",
       " ('hai', 804),\n",
       " ('ho', 793),\n",
       " ('ka', 726),\n",
       " ('me', 640),\n",
       " ('?', 615),\n",
       " ('b', 604),\n",
       " ('kr', 568),\n",
       " ('ga', 559),\n",
       " ('ni', 553),\n",
       " ('ko', 543),\n",
       " ('ki', 532),\n",
       " ('tha', 528),\n",
       " (',', 518),\n",
       " ('...', 502),\n",
       " ('na', 497),\n",
       " ('hn', 473),\n",
       " ('hy', 464),\n",
       " ('wo', 461),\n",
       " ('ma', 453),\n",
       " ('nai', 450),\n",
       " ('..', 450),\n",
       " ('a', 446),\n",
       " ('se', 415),\n",
       " ('p', 409),\n",
       " ('yar', 401),\n",
       " ('or', 392),\n",
       " ('yr', 389),\n",
       " ('h', 388),\n",
       " ('i', 385),\n",
       " ('han', 385),\n",
       " ('tu', 371),\n",
       " ('e', 331),\n",
       " (':', 327),\n",
       " ('ne', 324),\n",
       " ('kia', 321),\n",
       " ('he', 287),\n",
       " ('hain', 284),\n",
       " ('main', 281),\n",
       " ('ab', 254),\n",
       " ('koi', 252),\n",
       " ('us', 251),\n",
       " ('nae', 250),\n",
       " ('ap', 250),\n",
       " ('sir', 250),\n",
       " ('sy', 248),\n",
       " ('tm', 237),\n",
       " ('is', 223),\n",
       " ('nahi', 223),\n",
       " ('hi', 222),\n",
       " ('raha', 220),\n",
       " ('kal', 218),\n",
       " ('rha', 214),\n",
       " ('ja', 202),\n",
       " ('ny', 200),\n",
       " ('aj', 199),\n",
       " ('g', 199),\n",
       " ('m', 198),\n",
       " ('phr', 195),\n",
       " (':-', 193),\n",
       " ('aur', 192),\n",
       " ('mai', 192),\n",
       " ('....', 187),\n",
       " ('gya', 184),\n",
       " ('d', 183),\n",
       " ('bht', 181),\n",
       " ('u', 173),\n",
       " ('pta', 172),\n",
       " ('kar', 171),\n",
       " ('the', 169),\n",
       " ('mein', 168),\n",
       " ('jana', 168),\n",
       " ('ya', 167),\n",
       " ('time', 166),\n",
       " ('ok', 165),\n",
       " ('ye', 163),\n",
       " ('nhi', 161),\n",
       " ('kya', 161),\n",
       " ('jo', 161),\n",
       " ('in', 157),\n",
       " ('pe', 157),\n",
       " ('n', 154),\n",
       " ('0', 153),\n",
       " ('hu', 152),\n",
       " ('hun', 151),\n",
       " ('kam', 150),\n",
       " ('aa', 149),\n",
       " ('bhi', 147),\n",
       " ('abi', 142),\n",
       " (\"'\", 140),\n",
       " ('do', 138),\n",
       " ('you', 135),\n",
       " ('mera', 135),\n",
       " ('kuch', 135),\n",
       " ('uni', 134),\n",
       " ('o', 131),\n",
       " ('msg', 127),\n",
       " ('ke', 125),\n",
       " ('aya', 124),\n",
       " ('sb', 123),\n",
       " ('!', 122),\n",
       " ('pata', 119),\n",
       " ('say', 119),\n",
       " ('bhai', 118),\n",
       " ('tk', 118),\n",
       " ('sath', 118),\n",
       " ('le', 117),\n",
       " ('gy', 117),\n",
       " ('send', 116),\n",
       " ('thi', 114),\n",
       " ('ana', 114),\n",
       " ('ge', 112),\n",
       " ('kaha', 111),\n",
       " ('bta', 110),\n",
       " ('2', 110),\n",
       " ('pas', 109),\n",
       " ('ly', 109),\n",
       " ('plz', 108),\n",
       " ('gi', 107),\n",
       " ('ghr', 106),\n",
       " ('bs', 105),\n",
       " ('??', 104),\n",
       " ('r', 103),\n",
       " ('bat', 103),\n",
       " ('no', 102),\n",
       " ('and', 101),\n",
       " ('dy', 100),\n",
       " ('meri', 98),\n",
       " ('keh', 97),\n",
       " ('mje', 96),\n",
       " ('din', 96),\n",
       " ('allah', 96),\n",
       " ('krna', 95),\n",
       " ('for', 95),\n",
       " ('kiya', 95),\n",
       " ('agr', 94),\n",
       " ('1', 92),\n",
       " ('mere', 92),\n",
       " ('bt', 91),\n",
       " ('acha', 90),\n",
       " ('lo', 89),\n",
       " ('ghar', 87),\n",
       " ('hon', 86),\n",
       " ('but', 85),\n",
       " ('thy', 85),\n",
       " ('baat', 85),\n",
       " ('de', 84),\n",
       " ('so', 84),\n",
       " ('par', 82),\n",
       " ('hota', 82),\n",
       " ('of', 82),\n",
       " ('my', 82),\n",
       " ('liye', 82),\n",
       " ('tak', 81),\n",
       " ('mjy', 81),\n",
       " ('be', 80),\n",
       " ('s', 80),\n",
       " ('yaar', 80),\n",
       " ('khud', 79),\n",
       " ('abhi', 79),\n",
       " ('class', 78),\n",
       " ('hua', 78),\n",
       " ('at', 77),\n",
       " ('aik', 77),\n",
       " ('py', 77),\n",
       " ('t', 76),\n",
       " ('kha', 76),\n",
       " ('jao', 76),\n",
       " ('kisi', 75),\n",
       " ('gai', 74),\n",
       " ('on', 73),\n",
       " ('wese', 73),\n",
       " ('mil', 73),\n",
       " ('may', 73),\n",
       " ('sa', 73),\n",
       " ('pr', 73),\n",
       " ('kro', 73),\n",
       " ('ra', 72),\n",
       " ('wala', 71),\n",
       " ('bi', 71),\n",
       " ('thk', 71),\n",
       " ('will', 71),\n",
       " ('have', 70),\n",
       " (':)', 70),\n",
       " ('c', 70),\n",
       " ('???', 70),\n",
       " ('it', 69),\n",
       " ('jani', 69),\n",
       " ('kb', 69),\n",
       " ('tum', 68),\n",
       " ('.....', 68),\n",
       " ('bus', 66),\n",
       " ('hum', 66),\n",
       " ('wali', 65),\n",
       " ('hay', 65),\n",
       " ('log', 64),\n",
       " ('call', 63),\n",
       " ('krta', 63),\n",
       " ('tw', 63),\n",
       " ('rhi', 63),\n",
       " ('we', 62),\n",
       " ('phir', 62),\n",
       " ('free', 61),\n",
       " ('kis', 61),\n",
       " ('hahaha', 61),\n",
       " ('3', 61),\n",
       " ('apni', 61),\n",
       " ('thek', 60),\n",
       " ('lab', 59),\n",
       " ('un', 59),\n",
       " ('subha', 59),\n",
       " ('th', 59),\n",
       " ('dia', 58),\n",
       " ('chal', 58),\n",
       " ('jb', 58),\n",
       " ('mama', 57),\n",
       " ('w', 57),\n",
       " ('q', 57),\n",
       " ('that', 56),\n",
       " ('aye', 56),\n",
       " (':-)', 55),\n",
       " ('ata', 54),\n",
       " ('teri', 54),\n",
       " ('v', 54),\n",
       " ('paper', 53),\n",
       " ('ek', 53),\n",
       " ('min', 53),\n",
       " ('bna', 53),\n",
       " ('jaye', 53),\n",
       " ('bjy', 52),\n",
       " ('papa', 52),\n",
       " ('sai', 52),\n",
       " ('pass', 52),\n",
       " ('mene', 52),\n",
       " ('gae', 52),\n",
       " ('dil', 52),\n",
       " ('yad', 51),\n",
       " ('nd', 51),\n",
       " ('haha', 51),\n",
       " ('mjhe', 51),\n",
       " ('oye', 50),\n",
       " ('dena', 50),\n",
       " ('8', 50),\n",
       " ('mery', 50),\n",
       " ('pa', 50),\n",
       " ('wahan', 50),\n",
       " ('ur', 49),\n",
       " ('dekh', 48),\n",
       " ('ao', 48),\n",
       " ('tera', 48),\n",
       " ('rhy', 48),\n",
       " ('laga', 48),\n",
       " ('hoti', 48),\n",
       " ('jaldi', 48),\n",
       " ('rahi', 48),\n",
       " ('karna', 48),\n",
       " ('mujy', 47),\n",
       " ('office', 47),\n",
       " ('am', 46),\n",
       " ('hm', 46),\n",
       " ('gay', 46),\n",
       " ('late', 46),\n",
       " ('kch', 46),\n",
       " ('eid', 46),\n",
       " ('khana', 45),\n",
       " ('not', 45),\n",
       " ('kahan', 45),\n",
       " ('da', 45),\n",
       " ('tou', 45),\n",
       " ('check', 45),\n",
       " ('yeh', 45),\n",
       " ('reply', 45),\n",
       " ('skta', 45),\n",
       " ('wapis', 44),\n",
       " ('bje', 44),\n",
       " ('if', 44),\n",
       " ('gaya', 44),\n",
       " ('jata', 44),\n",
       " ('use', 44),\n",
       " ('apna', 43),\n",
       " ('10', 43),\n",
       " ('ay', 43),\n",
       " ('bad', 43),\n",
       " ('kafi', 43),\n",
       " ('thora', 43),\n",
       " ('aaj', 43),\n",
       " ('krwa', 42),\n",
       " ('sab', 42),\n",
       " ('lye', 42),\n",
       " (':(', 42),\n",
       " ('good', 42),\n",
       " ('theek', 42),\n",
       " ('mn', 42),\n",
       " ('di', 42),\n",
       " ('nikal', 42),\n",
       " ('nay', 41),\n",
       " ('ae', 41),\n",
       " ('ku', 41),\n",
       " ('sorry', 41),\n",
       " ('day', 40),\n",
       " ('usy', 40),\n",
       " ('li', 40),\n",
       " ('4', 40),\n",
       " ('muje', 40),\n",
       " ('5', 40),\n",
       " ('ai', 39),\n",
       " ('lahore', 39),\n",
       " ('plan', 39),\n",
       " ('dua', 39),\n",
       " ('start', 38),\n",
       " (',,,', 38),\n",
       " ('7', 38),\n",
       " ('hui', 38),\n",
       " ('fb', 38),\n",
       " ('agar', 38),\n",
       " ('test', 38),\n",
       " ('bike', 38),\n",
       " ('nh', 38),\n",
       " ('chala', 37),\n",
       " ('krny', 37),\n",
       " ('jae', 37),\n",
       " ('lia', 37),\n",
       " ('kbi', 37),\n",
       " ('lga', 37),\n",
       " ('your', 37),\n",
       " ('dost', 37),\n",
       " ('mila', 37),\n",
       " ('net', 37),\n",
       " ('masla', 36),\n",
       " ('last', 36),\n",
       " ('try', 36),\n",
       " ('rat', 36),\n",
       " ('kaam', 36),\n",
       " ('lena', 36),\n",
       " ('sakta', 36),\n",
       " ('per', 36),\n",
       " ('are', 35),\n",
       " ('la', 35),\n",
       " ('hoon', 35),\n",
       " ('assignment', 35),\n",
       " ('haan', 35),\n",
       " ('rahy', 35),\n",
       " ('change', 35),\n",
       " ('lag', 35),\n",
       " ('krni', 35),\n",
       " ('ta', 35),\n",
       " ('pay', 35)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is additional text cleaning necessary? I don't see why\n",
    "from collections import Counter\n",
    "train_words = ' '.join( df_train['text_clean'].tolist() ).lower().split()\n",
    "c = Counter( train_words )\n",
    "c.most_common(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afcaeedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "102 318 129\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/owaisraza009/roman-urdu-sentiment-analysis/notebook\n",
    "stopwords1 = [ 'ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
    "               'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
    "               'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se',\n",
    "               'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski',\n",
    "               'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya',\n",
    "               'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi',\n",
    "               'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain',\n",
    "               'krny', 'tou', ]\n",
    "\n",
    "# https://github.com/haseebelahi/roman-urdu-stopwords.git\n",
    "file = 'data/stopwords.txt'\n",
    "stopwords2 = open(file).read().split()\n",
    "print(stopwords2 == stopwords1)\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "stopwords_en  = _stop_words.ENGLISH_STOP_WORDS\n",
    "# selected from stopwords_en\n",
    "stopwords_en2 = [ 'a', 'about', 'also', 'am', 'an', 'and', 'are', 'as', 'at', 'be', \n",
    "                  'been', 'being', 'by', 'co', 'con', 'de', 'eg', 'eight', 'eleven', 'else', 'etc', \n",
    "                  'fifteen', 'fifty', 'five', 'for', 'forty', 'four', 'from', 'had',\n",
    "                  'has', 'hasnt', 'have', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', \n",
    "                  'his', 'how', 'i', 'ie', 'if', 'in', 'inc', 'into', 'is', 'it', 'its', 'itself',\n",
    "                  'ltd', 'me', 'mine', 'my', 'myself', 'nine', 'no', 'now', 'of', 'off', 'on',\n",
    "                  'once', 'one', 'onto', 'or', 'other', 'others', 'our', 'ours', 'ourselves',\n",
    "                  'out', 'part', 'per', 're', 'several', 'she', 'side', 'since', 'six', 'sixty',\n",
    "                  'so', 'ten', 'than', 'that', 'the', 'their', 'them',\n",
    "                  'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', \n",
    "                  'three', 'to', 'twelve', 'twenty', 'two', 'un','us', 'very',\n",
    "                  'via', 'was', 'we', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', \n",
    "                  'who', 'whom', 'whose', 'why', 'with', 'within', 'would', 'yet', 'you', 'your', 'yours',\n",
    "                   'yourself', 'yourselves', ]\n",
    "\n",
    "print( len(stopwords1), len(stopwords_en), len(stopwords_en2), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22878c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcf7f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train before upsampling:\n",
      "1    4070\n",
      "0    2097\n",
      "Name: target, dtype: int64\n",
      "Num neutral datapoints to add: 1973\n",
      "df_train after upsampling:\n",
      "0    4070\n",
      "1    4070\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# upsample neutral\n",
    "print('df_train before upsampling:\\n', df_train['target'].value_counts(), sep='')\n",
    "df_train_neutral = df_train[ df_train['target']==0 ]\n",
    "to_upsample      = df_train[ df_train['target']==1 ].shape[0] - df_train_neutral.shape[0]\n",
    "print('Num neutral datapoints to add:', to_upsample)\n",
    "\n",
    "df_additional = df_train_neutral.sample( n=to_upsample, random_state=random_state )\n",
    "df_train = pd.concat([ df_train.copy(), df_additional.copy() ])\\\n",
    "             .sample(frac=1, random_state=random_state)\\\n",
    "             .reset_index(drop=True)\n",
    "print('df_train after upsampling:\\n', df_train['target'].value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (8140,) (8140,) (1191,) (1191,) \n",
      "\n",
      "['Nahi yrr me ni jna . . jao tum log'\n",
      " 'Yar baba jan Lahore a rahy hain tu puch raha k ap k pass time hy tu kpry wagera leny jana hy q k main or hassan rehman tu ja rahy ap jao gy tu koi achy ly lain gy'\n",
      " 'Mughal Sahib .. Ghar ka kya bna ???'\n",
      " 'jan time wasting kaam hai kuch our kar le ,'\n",
      " 'Pagalwagal to nai ho es time bazar kun ja rhi ho ??'] [0 0 1 1 1] \n",
      "\n",
      "['Tension lene ki koi baat ni'\n",
      " 'Main ghar punch gya hun or ab spny laga hun'\n",
      " 'Nai mje nai mili mail .. mene check ki ti'\n",
      " 'Yr us din mai pura din bzy rahe vo mujy awne hi nai dy rahe the or kal b aisa hi raha koe naw koe aw raha tha aj to mn soba sy dekh rahe hn k tm aw jaao lkn tm to dada jee ke taraf ..'\n",
      " 'Lakin wo abhe dar dar ka chalata ha'] [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['text_clean'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "X_dev = df_dev['text_clean'].values\n",
    "y_dev = df_dev['target'].values\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle( X_train, y_train, random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, '\\n')\n",
    "print(X_train[:5], y_train[:5], '\\n')\n",
    "print(X_dev[:5], y_dev[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76dbc948",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c3541e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_nb = {\n",
    "    'alpha': 1.0,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "clf_params_lr = {\n",
    "    'C': 0.8,\n",
    "    'solver': 'liblinear',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter': 500,\n",
    "    'random_state': random_state,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "ca0e4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.3,                                 # eta\n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # defaultgain,weight,cover,total_gain,total_cover\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,                             # 0-1\n",
    "    'colsample_bynode': 1.0,                              # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 2,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "e92e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char_wb',\n",
    "    'ngram_range': (1,7),\n",
    "    'binary': False,\n",
    "    'stop_words': stopwords1 + stopwords_en2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-85 {color: black;background-color: white;}#sk-container-id-85 pre{padding: 0;}#sk-container-id-85 div.sk-toggleable {background-color: white;}#sk-container-id-85 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-85 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-85 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-85 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-85 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-85 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-85 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-85 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-85 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-85 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-85 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-85 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-85 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-85 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-85 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-85 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-85 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-85 div.sk-item {position: relative;z-index: 1;}#sk-container-id-85 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-85 div.sk-item::before, #sk-container-id-85 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-85 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-85 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-85 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-85 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-85 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-85 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-85 div.sk-label-container {text-align: center;}#sk-container-id-85 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-85 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-85\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char_wb&#x27;, ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;,\n",
       "                                             &#x27;ki&#x27;, &#x27;tha&#x27;, &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;,\n",
       "                                             &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                                             &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;,\n",
       "                                             &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;, &#x27;kar&#x27;, &#x27;lye&#x27;,\n",
       "                                             &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                                             &#x27;gaya&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsampl...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=5, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=2,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-253\" type=\"checkbox\" ><label for=\"sk-estimator-id-253\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char_wb&#x27;, ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;,\n",
       "                                             &#x27;ki&#x27;, &#x27;tha&#x27;, &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;,\n",
       "                                             &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                                             &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;,\n",
       "                                             &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;, &#x27;kar&#x27;, &#x27;lye&#x27;,\n",
       "                                             &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                                             &#x27;gaya&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsampl...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=5, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=2,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-254\" type=\"checkbox\" ><label for=\"sk-estimator-id-254\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char_wb&#x27;, ngram_range=(1, 7),\n",
       "                stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;, &#x27;ki&#x27;, &#x27;tha&#x27;,\n",
       "                            &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;, &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                            &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;, &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;,\n",
       "                            &#x27;kar&#x27;, &#x27;lye&#x27;, &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                            &#x27;gaya&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-255\" type=\"checkbox\" ><label for=\"sk-estimator-id-255\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;merror&#x27;, feature_types=None, gamma=0, gpu_id=None,\n",
       "              grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
       "              max_depth=5, max_leaves=None, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
       "              num_class=2, num_parallel_tree=None, objective=&#x27;multi:softmax&#x27;, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 7),\n",
       "                                 stop_words=['ai', 'ayi', 'hy', 'hai', 'main',\n",
       "                                             'ki', 'tha', 'koi', 'ko', 'sy',\n",
       "                                             'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
       "                                             'rha', 'hota', 'ho', 'ga', 'ka',\n",
       "                                             'le', 'lye', 'kr', 'kar', 'lye',\n",
       "                                             'liye', 'hotay', 'waisay', 'gya',\n",
       "                                             'gaya', ...])),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsampl...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=5, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=2,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective='multi:softmax', ...))])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer( **vect_params )\n",
    "#vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "#clf = MultinomialNB( **clf_params_nb )\n",
    "#clf = LogisticRegression( **clf_params_lr )\n",
    "#clf = SVC()\n",
    "#clf = RidgeClassifierCV()\n",
    "clf = XGBClassifier( **clf_params_xgb )\n",
    "\n",
    "\n",
    "model = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_dev   = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "cfd5f419",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 7),\n",
      "                stop_words=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha',\n",
      "                            'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
      "                            'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr',\n",
      "                            'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
      "                            'gaya', ...])\n",
      "\n",
      "Classifier:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric='merror', feature_types=None, gamma=0, gpu_id=None,\n",
      "              grow_policy=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
      "              max_depth=5, max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
      "              num_class=2, num_parallel_tree=None, objective='multi:softmax', ...)\n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9239    0.9968    0.9590      4070\n",
      "           1     0.9965    0.9179    0.9556      4070\n",
      "\n",
      "    accuracy                         0.9574      8140\n",
      "   macro avg     0.9602    0.9574    0.9573      8140\n",
      "weighted avg     0.9602    0.9574    0.9573      8140\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4947    0.6057    0.5446       388\n",
      "           1     0.7863    0.7011    0.7413       803\n",
      "\n",
      "    accuracy                         0.6700      1191\n",
      "   macro avg     0.6405    0.6534    0.6429      1191\n",
      "weighted avg     0.6913    0.6700    0.6772      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification reports\n",
    "print('Vectorizer:\\n', model['vect'], '\\n', sep='')\n",
    "print('Classifier:\\n', model['clf'], '\\n', sep='')\n",
    "\n",
    "print('\\nTRAINSET')\n",
    "print( classification_report( y_train, y_pred_train, digits=4 ) )\n",
    "\n",
    "print('DEVSET')\n",
    "print( classification_report( y_dev, y_pred_dev, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f32b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68504126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['clf_pred'] = y_pred_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2306e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af9ace6",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char', ngram_range=(1, 7),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "LogisticRegression(C=0.8, max_iter=500, random_state=47, solver='liblinear')\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9366    0.9690    0.9525      4070\n",
    "           1     0.9679    0.9344    0.9509      4070\n",
    "\n",
    "    accuracy                         0.9517      8140\n",
    "   macro avg     0.9523    0.9517    0.9517      8140\n",
    "weighted avg     0.9523    0.9517    0.9517      8140\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5607    0.4639    0.5078       388\n",
    "           1     0.7609    0.8244    0.7914       803\n",
    "\n",
    "    accuracy                         0.7070      1191\n",
    "   macro avg     0.6608    0.6442    0.6496      1191\n",
    "weighted avg     0.6957    0.7070    0.6990      1191\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char', min_df=5, ngram_range=(1, 5),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "MultinomialNB()\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.8755    0.8482    0.8616      4070\n",
    "           1     0.8528    0.8794    0.8659      4070\n",
    "\n",
    "    accuracy                         0.8638      8140\n",
    "   macro avg     0.8641    0.8638    0.8637      8140\n",
    "weighted avg     0.8641    0.8638    0.8637      8140\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5153    0.5206    0.5179       388\n",
    "           1     0.7672    0.7634    0.7653       803\n",
    "\n",
    "    accuracy                         0.6843      1191\n",
    "   macro avg     0.6413    0.6420    0.6416      1191\n",
    "weighted avg     0.6851    0.6843    0.6847      1191\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "\n",
    "Needs fine-tuning\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 7),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "XGBClassifier( max_depth=5, ...)\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9239    0.9968    0.9590      4070\n",
    "           1     0.9965    0.9179    0.9556      4070\n",
    "\n",
    "    accuracy                         0.9574      8140\n",
    "   macro avg     0.9602    0.9574    0.9573      8140\n",
    "weighted avg     0.9602    0.9574    0.9573      8140\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.4947    0.6057    0.5446       388\n",
    "           1     0.7863    0.7011    0.7413       803\n",
    "\n",
    "    accuracy                         0.6700      1191\n",
    "   macro avg     0.6405    0.6534    0.6429      1191\n",
    "weighted avg     0.6913    0.6700    0.6772      1191\n",
    "\n",
    "\n",
    "SVC() accu on training set = 0.99 with LR features\n",
    "RF and Ridge - same with even ngram(1,5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7a589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396d0fce",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad9e7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as a text classifier. Classify the text below into one most relevant category from this list of categories: emotional, neutral. Use the emotional category only if the text below describes any emotions; use the neutral category only if the text below does not speak about emotions at all. Output only one word: 'emotional' or 'neutral', whichever is more relevant. Text: \"This is a text sample\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prompt_one   = '''The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English only. Then classify the translated text as 'emotional' if it contains emotions or 'neutral' if it does not contain emotions. Output only 'emotional' or 'neutral' and nothing else. Text: \"{}\"'''\n",
    "prompt_one   = '''Act as a text classifier. Classify the text below into one most relevant category from this list of categories: emotional, neutral. Use the emotional category only if the text below describes any emotions; use the neutral category only if the text below does not speak about emotions at all. Output only one word: 'emotional' or 'neutral', whichever is more relevant. Text: \"{}\"'''\n",
    "s = 'This is a text sample'\n",
    "print(prompt_one.format(s), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc99c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using followup Q1 can improve the reponse. If the reponse has multiple words, first parse it and try to find\n",
    "# the category in it. Only if this doesn't work, send followup Q2. ChatGPT can offer the second category in reponse\n",
    "# to Q1, but can change its mind again and offer a third category if asked Q2\n",
    "followup1 = 'Are you sure about that? Again, output only one most relevant category'\n",
    "followup2 = 'Output only the category and nothing else'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7583ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral', 'emotional'}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model          = 'gpt-3.5-turbo'\n",
    "labels_set     = {'emotional', 'neutral'}\n",
    "clean = re.compile(r'[^a-zA-Z ]+')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee57217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_label(label_):\n",
    "    '''\n",
    "       Verify if label_ contains any of the categories\n",
    "       from the predefined set of labels\n",
    "    '''\n",
    "    label_ = clean.sub(' ', label_)\n",
    "    label_ = multi_spaces.sub(' ', label_).lower().split()\n",
    "    res    = [i for i in label_ if i in labels_set]\n",
    "    res    = list(set(res))\n",
    "    return '/'.join(res) if res else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7adff9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_num_tokens(model, messages):\n",
    "    '''Check that there is enough tokens available for a ChatGPT repsonse'''\n",
    "    num_tokens_tiktoken = num_tokens_from_messages(messages, model)\n",
    "    if num_tokens_tiktoken > 3950:\n",
    "        print(f'Number of tokens is {num_tokens_tiktoken} which exceeds 3950')\n",
    "        print(f'TEXT: {text_}\\n')\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError, max_time=10)\n",
    "def get_response(model, messages, temperature=0, max_tokens=None):\n",
    "    '''Send request, return reponse'''\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = temperature,        # range(0,2), the more the less deterministic / focused\n",
    "        top_p = 1,                        # top probability mass, e.g. 0.1 = only tokens from top 10% proba mass\n",
    "        n = 1,                            # number of chat completions\n",
    "        #max_tokens = max_tokens,          # tokens to return\n",
    "        stream = False,        \n",
    "        stop=None,                        # sequence to stop generation (new line, end of text, etc.)\n",
    "        )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    #num_tokens_api = response['usage']['prompt_tokens']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0eb629b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text_, prompt_):\n",
    "    '''Translate text_ using prompt_ and ChatGPT API'''    \n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [            \n",
    "            { \"role\": \"system\", \"content\": \"You are an accurate translator from Roman Urdu.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    return get_response(model, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f9fa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text_, prompt_):\n",
    "    '''Classify text_ using prompt_ and ChatGPT API'''\n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            { \"role\": \"system\", \"content\": \"You are a very accurate text classifier.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    label_    = get_response(model, messages)\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)        # get just the category if response is too long\n",
    "        \n",
    "    # if label not found in response text - second, extended chat\n",
    "    if label_ is None:\n",
    "        messages += [\n",
    "            { \"role\": \"assistant\", \"content\": old_label, },\n",
    "            { \"role\": \"user\", \"content\": followup1, }\n",
    "            ]        \n",
    "        label_    = get_response(model, messages)        \n",
    "        old_label = label_\n",
    "        label_    = verify_label(label_)        # get just the category if response is too long\n",
    "            \n",
    "    return label_ if label_ is not None else old_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15cfa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_with_clarifying(text_, prompt_):\n",
    "    '''\n",
    "       Classify text_ using prompt_ and ChatGPT API,\n",
    "       then clarify response with followup1 question -\n",
    "       this can help make the response more precise\n",
    "    '''\n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            #{ \"role\": \"system\", \"content\": \"You are a very accurate text classifier.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    label_    = get_response(model, messages)\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)                      # get just the category if response is too long\n",
    "        \n",
    "    # ask additional clarifying question - sometimes it helps\n",
    "    messages += [\n",
    "        { \"role\": \"assistant\", \"content\": old_label, },\n",
    "        { \"role\": \"user\", \"content\": followup1, }\n",
    "        ]\n",
    "    #time.sleep( random.uniform(1.1, 1.8) )                # wait not to overload ChatGPT\n",
    "    label2_    = get_response(model, messages)\n",
    "    old_label2 = label2_\n",
    "    label2_    = verify_label(label2_)                    # get just the category if response is too long\n",
    "\n",
    "    return old_label, label_, old_label2, label2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "845232ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>gpt_pred_num</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       0   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       0   \n",
       "2            Nai mje nai mili mail..mene check ki ti  pessimism       1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       1   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       1   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \\\n",
       "0                         Any talk of taking tangoes   \n",
       "1              I have gone home punch and now dreams   \n",
       "2                                Ni Ni Ni Mille Mail   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                              But Wu runs the cedar   \n",
       "\n",
       "                                          text_clean  gpt_pred  gpt_pred_num  \\\n",
       "0                        Tension lene ki koi baat ni   neutral             1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   neutral             1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti   neutral             1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  negative             0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   neutral             1   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \n",
       "0                          There's no need to worry.  \n",
       "1   I have reached home and now I am going to sleep.  \n",
       "2      I didn't receive any new mail. I had checked.  \n",
       "3  I was busy the whole day on that day, they wer...  \n",
       "4       But he still walks with fear and hesitation.  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8bb18d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as a text classifier. Classify the text below into one most relevant category from this list of categories: emotional, neutral. Use the emotional category only if the text below describes any emotions; use the neutral category only if the text below does not speak about emotions at all. Output only one word: 'emotional' or 'neutral', whichever is more relevant. Text: \"Dude, when did I ever say no to you guys? Come on over, I'm free right now anyway.\"\n",
      "\n",
      "GROUNDTRUTH LABEL:\n",
      "n/e/u/t/r/a/l\n",
      "\n",
      "PREDICTED LABEL:\n",
      "('Neutral', 'neutral', 'Emotional.', 'emotional')\n"
     ]
    }
   ],
   "source": [
    "# test as single prompt\n",
    "idx = 11\n",
    "text, groundtruth_labels = df_dev[['gpt_translated2_corrected', 'emotion']].values[idx]\n",
    "label  = classify_text(text, prompt_one)\n",
    "labels = classify_text_with_clarifying(text, prompt_one)\n",
    "\n",
    "print(prompt_one.format( text ))\n",
    "print(f\"\\nGROUNDTRUTH LABEL:\\n{'/'.join( groundtruth_labels )}\")\n",
    "print(f\"\\nPREDICTED LABEL:\\n{labels}\")\n",
    "#print(f'\\nTOTAL TOKENS: {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e957b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58886b895c74e9d8dfe901f25d2bef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                         0\n",
       "emotion                      0\n",
       "target                       0\n",
       "gtp_translated               0\n",
       "translated_hi                0\n",
       "translated_ur                0\n",
       "text_clean                   0\n",
       "gpt_pred                     0\n",
       "gpt_pred_num                 0\n",
       "gpt_translated2              0\n",
       "gpt_translated2_corrected    0\n",
       "gpt_pred_binary              0\n",
       "gpt_pred_clarified           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 1 tqdm results - 1191/1191 [25:38<00:00, 1.18s/it]\n",
    "def apply_func_with_exception(text_, prompt_):\n",
    "    try:\n",
    "        return classify_text_with_clarifying(text_, prompt_)\n",
    "    except openai.error.RateLimitError:\n",
    "        return np.nan\n",
    "    \n",
    "df_dev['gpt_pred_clarified'] = df_dev['gpt_translated2_corrected'].progress_apply( lambda x: apply_func_with_exception(x, prompt_one) )\n",
    "df_dev.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c9c3f",
   "metadata": {},
   "source": [
    "RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID acd05580deb4c3bb950df495e847dbbc in your message.)\n",
    "\n",
    "RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9107e5654b5a2c4e6353e668734c88f1 in your message.)\n",
    "\n",
    "RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f90bb036d1ac47518f90a229dd48788d in your message.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92811b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(neutral, neutral, neutral, neutral)                                                                                                                                                                            374\n",
       "(Neutral, neutral, Neutral., neutral)                                                                                                                                                                           191\n",
       "(Neutral., neutral, Neutral., neutral)                                                                                                                                                                          141\n",
       "(neutral, neutral, emotional, emotional)                                                                                                                                                                         84\n",
       "(Neutral, neutral, Emotional., emotional)                                                                                                                                                                        80\n",
       "(emotional, emotional, emotional., emotional)                                                                                                                                                                    72\n",
       "(Neutral., neutral, Emotional., emotional)                                                                                                                                                                       55\n",
       "(Emotional., emotional, Neutral., neutral)                                                                                                                                                                       53\n",
       "(emotional, emotional, neutral, neutral)                                                                                                                                                                         44\n",
       "(Emotional., emotional, Emotional., emotional)                                                                                                                                                                   44\n",
       "(emotional, emotional, emotional, emotional)                                                                                                                                                                     24\n",
       "(Neutral, neutral, Yes, I am sure. The text is neutral., neutral)                                                                                                                                                 4\n",
       "(neutral, neutral, Yes, I am sure. The text is neutral as it does not describe any emotions., neutral)                                                                                                            2\n",
       "(Emotional, emotional, Emotional., emotional)                                                                                                                                                                     2\n",
       "(Neutral, neutral, Yes, I am sure. The text is neutral as it does not describe any emotions., neutral)                                                                                                            2\n",
       "(neutral, neutral, neutral., neutral)                                                                                                                                                                             2\n",
       "(Neutral, neutral, Yes, I am sure. The text does not describe any emotions, so the most relevant category is neutral., neutral)                                                                                   2\n",
       "(neutral, neutral, Yes, I am sure. The text is providing information and instructions without expressing any emotions, so it falls under the neutral category. Therefore, the output is 'neutral'., neutral)      1\n",
       "(Neutral, neutral, Yes, I am sure. The text does not describe any emotions, so the most relevant category is 'neutral'., neutral)                                                                                 1\n",
       "(Neutral., neutral, Yes, I am sure. The most relevant category for the given text is 'Neutral'., neutral)                                                                                                         1\n",
       "(Neutral., neutral, Yes, I am sure. The text is neutral., neutral)                                                                                                                                                1\n",
       "(Neutral, neutral, Yes, I am sure. The text is discussing a practical problem and does not express any emotions, so the most relevant category is 'neutral'., neutral)                                            1\n",
       "(neutral, neutral, Yes, I am sure. The most relevant category for the given text is 'neutral'., neutral)                                                                                                          1\n",
       "(Neutral, neutral, Yes, I am sure. The text is a simple message about meeting up with someone and does not express any emotions. Therefore, the most relevant category is 'neutral'., neutral)                    1\n",
       "(Neutral, neutral, Yes, I am sure. The text is a simple instruction and does not express any emotions, so the most relevant category is 'neutral'., neutral)                                                      1\n",
       "(Neutral, neutral, Yes, I am sure. The text is neutral as it only provides information about the timetable and does not express any emotions. Therefore, the most relevant category is 'neutral'., neutral)       1\n",
       "(Neutral, neutral, Yes, I am sure. The most relevant category for the given text is 'neutral'., neutral)                                                                                                          1\n",
       "(Emotional, emotional, Neutral, neutral)                                                                                                                                                                          1\n",
       "(neutral, neutral, Yes, I am sure. The text does not describe any emotions, so the most relevant category is 'neutral'., neutral)                                                                                 1\n",
       "(Neutral, neutral, Yes, I am sure. The text is neutral and does not describe any emotions., neutral)                                                                                                              1\n",
       "(Neutral, neutral, Yes, I am sure. The text is a simple request for information and does not express any emotions. Therefore, the most relevant category is 'neutral'., neutral)                                  1\n",
       "(Neutral, neutral, I apologize for the mistake. The text is Neutral., neutral)                                                                                                                                    1\n",
       "Name: gpt_pred_clarified, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['gpt_pred_clarified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca209010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                         0\n",
       "emotion                      0\n",
       "target                       0\n",
       "gtp_translated               0\n",
       "translated_hi                0\n",
       "translated_ur                0\n",
       "text_clean                   0\n",
       "gpt_pred                     0\n",
       "gpt_pred_num                 0\n",
       "gpt_translated2              0\n",
       "gpt_translated2_corrected    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dedb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['gpt_pred_binary'] = df_dev['gpt_pred'].apply( lambda x: 0 if x=='neutral' else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5961083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ChatGPT made no prediction, choose the prediction coming from the classifier\n",
    "'''def improve_predictions(row):\n",
    "    if row['gpt_pred_binary'] is None:\n",
    "        row['gpt_pred_binary'] = row['clf_pred']\n",
    "    return row\n",
    "\n",
    "df_dev = df_dev.apply( improve_predictions, axis=1 )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f659274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3775    0.9253    0.5362       388\n",
      "           1     0.8792    0.2628    0.4046       803\n",
      "\n",
      "    accuracy                         0.4786      1191\n",
      "   macro avg     0.6283    0.5940    0.4704      1191\n",
      "weighted avg     0.7157    0.4786    0.4475      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev      = df_dev['target'].values\n",
    "y_dev_pred = df_dev['gpt_pred_binary'].values\n",
    "print( classification_report( y_dev, y_dev_pred, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1996804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147144"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens in train set\n",
    "num_tokens_from_messages([ {'content': ' '.join(df_train['text_clean'].tolist())} ], \"gpt-3.5-turbo-0301\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e08d39fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.923828125"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of batches to ChatGPT\n",
    "147144/4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfd118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3c932f",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693df45",
   "metadata": {},
   "source": [
    "### Prompts and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c1326",
   "metadata": {},
   "source": [
    "_Promp_: The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English only. Then classify the translated text as 'emotional' if it contains emotions or 'neutral' if it does not contain emotions. Output only 'emotional' or 'neutral' and nothing else. Text: \"This is a text sample\"\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3662    0.9278    0.5252       388\n",
    "           1     0.8654    0.2242    0.3561       803\n",
    "\n",
    "    accuracy                         0.4534      1191\n",
    "   macro avg     0.6158    0.5760    0.4406      1191\n",
    "weighted avg     0.7028    0.4534    0.4112      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26093b",
   "metadata": {},
   "source": [
    "_Prompt on the first English translation (sero shot)_: Act as a careful and accurate text classifier. Classify the text below as 'emotional' only if it contains emotions; lassify the text below as 'neutral' only if it does not contain emotions. Output only one word: 'emotional' or 'neutral' whichever is more relevant. Text: \"This is a text sample\"\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3789    0.9510    0.5419       388\n",
    "           1     0.9124    0.2466    0.3882       803\n",
    "\n",
    "    accuracy                         0.4761      1191\n",
    "   macro avg     0.6456    0.5988    0.4650      1191\n",
    "weighted avg     0.7386    0.4761    0.4383      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f410c",
   "metadata": {},
   "source": [
    "_Prompt on the corrected English translation (zero shot)_: Act as a text classifier. Classify the text below into one most relevant category from this list of categories: emotional, neutral. Use the emotional category only if the text below describes any emotions; use the neutral category only if the text below does not speak about emotions at all. Output only one word: 'emotional' or 'neutral', whichever is more relevant. Text: \"This is a text sample.\"\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3775    0.9253    0.5362       388\n",
    "           1     0.8792    0.2628    0.4046       803\n",
    "\n",
    "    accuracy                         0.4786      1191\n",
    "   macro avg     0.6283    0.5940    0.4704      1191\n",
    "weighted avg     0.7157    0.4786    0.4475      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c9fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b55fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc36b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7ecba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
