{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b687706c",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "## The Association for Computational Linguistics\n",
    "## WASSA 2023 Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages\n",
    "See more details [here](https://codalab.lisn.upsaclay.fr/competitions/10864#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import time\n",
    "import zipfile\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "import random\n",
    "import tiktoken\n",
    "import backoff\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56940aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new new version (Dec 2022)\n",
    "def upsample_all( df_, labels_col='target', random_state=47 ):\n",
    "    '''\n",
    "        Upsample each class in column labels_col of pandas dataframe df_\n",
    "        to the number of data points in majority class\n",
    "    '''\n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_[labels_col].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_[labels_col] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "\n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "\n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "\n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 4) (1191, 10) (1191, 1) (1191, 1)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/mcec_train_translated.pkl'\n",
    "df_train = pd.read_pickle(file1)\n",
    "\n",
    "file2    = 'data/mcec_dev_translated.pkl'\n",
    "df_dev   = pd.read_pickle(file2)\n",
    "\n",
    "file3    = 'data/mcec_test.csv'\n",
    "df_test  = pd.read_csv(file3)\n",
    "\n",
    "file4    = 'data/sample_submission/predictions_MCEC.csv'\n",
    "sample_submission = pd.read_csv(file4)\n",
    "\n",
    "print(df_train.shape, df_dev.shape, df_test.shape, sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d448c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                        Tension lene ki koi baat ni   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   \n",
       "2            Nai mje nai mili mail..mene check ki ti   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   \n",
       "\n",
       "                                          text_clean    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti  pessimism       0   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       0   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \n",
       "0                         Any talk of taking tangoes  \n",
       "1              I have gone home punch and now dreams  \n",
       "2                                Ni Ni Ni Mille Mail  \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  \n",
       "4                              But Wu runs the cedar  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy manually arbitrated translation into English from column 'gpt_translated2_corrected'\n",
    "#file = 'data/mcec_dev.xlsx'\n",
    "#df_dev2 = pd.read_excel( file )\n",
    "#print(df_dev2.shape)\n",
    "#df_dev2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e84bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dev['gpt_translated2_corrected'] = df_dev2['gpt_translated2_corrected'].values\n",
    "\n",
    "#file2  = 'data/mcec_dev_translated.pkl'\n",
    "#df_dev.to_pickle( file2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8a2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pessimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotion\n",
       "0    neutral\n",
       "1    neutral\n",
       "2  pessimism\n",
       "3    disgust\n",
       "4       fear"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission format\n",
    "print( type(sample_submission) )\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = df_train['emotion'].apply( lambda x: 0 if x=='neutral' else 1 )\n",
    "df_dev['target']   = df_dev['emotion'].apply( lambda x: 0 if x=='neutral' else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         3262\n",
      "trust           1118\n",
      "joy             1022\n",
      "optimism         880\n",
      "anticipation     832\n",
      "disgust          687\n",
      "sadness          486\n",
      "fear             453\n",
      "anger            226\n",
      "surprise         199\n",
      "love             187\n",
      "pessimism        178\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "1    6268\n",
      "0    3262\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes.I am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yes.i am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>Y. Um in Fap Lab Cabin. Butt Fap Presentations...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yar insan ka bcha bn chawliyn na mar :p</td>\n",
       "      <td>joy</td>\n",
       "      <td>Dude become a child of a human being, do not die.</td>\n",
       "      <td>Dude human beings do not die: P: P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terai uncle nai kahna hai kai ham nai to bahr ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>Your Uncle Nai says that we had sent out money</td>\n",
       "      <td>Your Ankali says that we sent out money and wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr ajao I m cming in the club</td>\n",
       "      <td>neutral</td>\n",
       "      <td>YR AJAO I'M Coming in the Club</td>\n",
       "      <td>Yer organs were the club</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...</td>\n",
       "      <td>joy</td>\n",
       "      <td>Mje wes nimra ahmad ka qur'aan ki aayaat k bar...</td>\n",
       "      <td>Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion  \\\n",
       "0  Yes.I am in fyp lab cabin.but fyp presentation...  neutral   \n",
       "1           Yar insan ka bcha bn chawliyn na mar :p       joy   \n",
       "2  Terai uncle nai kahna hai kai ham nai to bahr ...  disgust   \n",
       "3                      Yr ajao I m cming in the club  neutral   \n",
       "4  Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...      joy   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0  Yes.i am in fyp lab cabin.but fyp presentation...   \n",
       "1  Dude become a child of a human being, do not die.   \n",
       "2     Your Uncle Nai says that we had sent out money   \n",
       "3                     YR AJAO I'M Coming in the Club   \n",
       "4  Mje wes nimra ahmad ka qur'aan ki aayaat k bar...   \n",
       "\n",
       "                                       translated_ur  target  \n",
       "0  Y. Um in Fap Lab Cabin. Butt Fap Presentations...       0  \n",
       "1                 Dude human beings do not die: P: P       1  \n",
       "2  Your Ankali says that we sent out money and wa...       1  \n",
       "3                           Yer organs were the club       0  \n",
       "4  Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['emotion'].value_counts(), '\\n')\n",
    "print(df_train['target'].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1742539f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         388\n",
      "joy             131\n",
      "trust           125\n",
      "disgust         113\n",
      "optimism        110\n",
      "anticipation     94\n",
      "sadness          62\n",
      "fear             52\n",
      "surprise         35\n",
      "anger            35\n",
      "pessimism        29\n",
      "love             17\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "1    803\n",
      "0    388\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>gpt_pred_num</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       0   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       0   \n",
       "2            Nai mje nai mili mail..mene check ki ti  pessimism       1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       1   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       1   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \\\n",
       "0                         Any talk of taking tangoes   \n",
       "1              I have gone home punch and now dreams   \n",
       "2                                Ni Ni Ni Mille Mail   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                              But Wu runs the cedar   \n",
       "\n",
       "                                          text_clean  gpt_pred  gpt_pred_num  \\\n",
       "0                        Tension lene ki koi baat ni   neutral             1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   neutral             1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti   neutral             1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  negative             0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   neutral             1   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \n",
       "0                          There's no need to worry.  \n",
       "1   I have reached home and now I am going to sleep.  \n",
       "2      I didn't receive any new mail. I had checked.  \n",
       "3  I was busy the whole day on that day, they wer...  \n",
       "4       But he still walks with fear and hesitation.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_dev['emotion'].value_counts(), '\\n')\n",
    "print(df_dev['target'].value_counts())\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# light text cleaning (should I use clean regex for better accuracy?)\n",
    "pad_punct    = re.compile('([^a-zA-Z ]+)')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "#clean        = re.compile('[^a-zA-Z0-9,.?!\\'\\s]+')\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = pad_punct.sub(r' \\1 ', s)\n",
    "    #s = clean.sub(' ', s)\n",
    "    s = multi_spaces.sub(' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply( clean_text )\n",
    "df_dev['text_clean']   = df_dev['text'].apply( clean_text )\n",
    "df_test['text_clean']  = df_test['Text'].apply( clean_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e42251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 6)\n",
      "(4222, 6)\n",
      "(4221, 6)\n",
      "(4221, 6)\n"
     ]
    }
   ],
   "source": [
    "# 2K duplicates - these may affect claa imbalance during training! TO BE REDUCED\n",
    "print(df_train.shape)\n",
    "temp1 = df_train[ df_train.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_train[ df_train.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_train[ df_train.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7c2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 11)\n",
      "(82, 11)\n",
      "(82, 11)\n",
      "(68, 11)\n"
     ]
    }
   ],
   "source": [
    "# 82 duplicates ['clean_text', 'emotion'] - can't reduce because this is a dev set\n",
    "print(df_dev.shape)\n",
    "temp1 = df_dev[ df_dev.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_dev[ df_dev.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_dev[ df_dev.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c179df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 2)\n",
      "(93, 2)\n",
      "(93, 2)\n"
     ]
    }
   ],
   "source": [
    "# 93 complete duplicates - can't reduce because this is a test set\n",
    "print(df_test.shape)\n",
    "temp1 = df_test[ df_test.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp3 = df_test[ df_test.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f49ff482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 554, 526, 526)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train vs. df_dev: half of the dev set is in train set\n",
    "overlap1 = [t for t in df_train['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "overlap2 = [t for t in df_dev['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap1), len(overlap2), len(set(overlap1)), len(set(overlap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae49f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 584, 557, 557)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. rest\n",
    "overlap3 = [ t for t in df_train['text_clean'].tolist() + df_dev['text_clean'].tolist()\\\n",
    "             if t in df_test['text_clean'].tolist() ]\n",
    "overlap4 = [ t for t in df_test['text_clean'].tolist() if t in\\\n",
    "             df_train['text_clean'].tolist() + df_dev['text_clean'].tolist() ]\n",
    "len(overlap3), len(overlap4), len(set(overlap3)), len(set(overlap4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b78132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 97, 88, 88)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_dev: half of the dev set is in train set\n",
    "overlap5 = [t for t in df_dev['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap6 = [t for t in df_test['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "len(overlap5), len(overlap6), len(set(overlap5)), len(set(overlap6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13bff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727, 540, 519, 519)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_train: half of the dev set is in train set\n",
    "overlap7 = [t for t in df_train['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap8 = [t for t in df_test['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap7), len(overlap8), len(set(overlap7)), len(set(overlap8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a84db",
   "metadata": {},
   "source": [
    "The reason why baseline ML models perform better than ChatGPT is because they get a lot of hints due to duplicates from the training set! ChatGPT doesn't have this knowledge because it's doing a zero-shot classification! The number of duplicates is such that they would not fit the context window of ChatGPT anyway.\n",
    "\n",
    "The only way to compare ML and ChatGPT correctly is to remove all the duplicates from the TRAINING SET, then train ML model and test it the dev set and compare with ChatGPT! (also, deduplicate the training set)\n",
    "\n",
    "Submission: use non-overfit ML or ChatGPT (whichever is better) on those samples from the test set that don't have duplicates in the training or dev set. Use training/dev set labels for the duploicates in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fbd6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 2206\n",
      "(9530, 6)\n",
      "(8151, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove overlap with validation sets\n",
    "val_sets = df_dev['text_clean'].tolist() + df_test['text_clean'].tolist()\n",
    "print(len(val_sets), len(set(val_sets)))\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train = df_train[ ~df_train['text_clean'].isin(val_sets) ]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1293d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates from train set\n",
    "df_train = df_train.drop_duplicates(subset=['text_clean', 'emotion'])\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40905d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 2127),\n",
       " ('k', 1244),\n",
       " ('to', 1231),\n",
       " ('ha', 1214),\n",
       " ('hai', 804),\n",
       " ('ho', 793),\n",
       " ('ka', 726),\n",
       " ('me', 640),\n",
       " ('?', 615),\n",
       " ('b', 604),\n",
       " ('kr', 568),\n",
       " ('ga', 559),\n",
       " ('ni', 553),\n",
       " ('ko', 543),\n",
       " ('ki', 532),\n",
       " ('tha', 528),\n",
       " (',', 518),\n",
       " ('...', 502),\n",
       " ('na', 497),\n",
       " ('hn', 473),\n",
       " ('hy', 464),\n",
       " ('wo', 461),\n",
       " ('ma', 453),\n",
       " ('nai', 450),\n",
       " ('..', 450),\n",
       " ('a', 446),\n",
       " ('se', 415),\n",
       " ('p', 409),\n",
       " ('yar', 401),\n",
       " ('or', 392),\n",
       " ('yr', 389),\n",
       " ('h', 388),\n",
       " ('i', 385),\n",
       " ('han', 385),\n",
       " ('tu', 371),\n",
       " ('e', 331),\n",
       " (':', 327),\n",
       " ('ne', 324),\n",
       " ('kia', 321),\n",
       " ('he', 287),\n",
       " ('hain', 284),\n",
       " ('main', 281),\n",
       " ('ab', 254),\n",
       " ('koi', 252),\n",
       " ('us', 251),\n",
       " ('nae', 250),\n",
       " ('ap', 250),\n",
       " ('sir', 250),\n",
       " ('sy', 248),\n",
       " ('tm', 237),\n",
       " ('is', 223),\n",
       " ('nahi', 223),\n",
       " ('hi', 222),\n",
       " ('raha', 220),\n",
       " ('kal', 218),\n",
       " ('rha', 214),\n",
       " ('ja', 202),\n",
       " ('ny', 200),\n",
       " ('aj', 199),\n",
       " ('g', 199),\n",
       " ('m', 198),\n",
       " ('phr', 195),\n",
       " (':-', 193),\n",
       " ('aur', 192),\n",
       " ('mai', 192),\n",
       " ('....', 187),\n",
       " ('gya', 184),\n",
       " ('d', 183),\n",
       " ('bht', 181),\n",
       " ('u', 173),\n",
       " ('pta', 172),\n",
       " ('kar', 171),\n",
       " ('the', 169),\n",
       " ('mein', 168),\n",
       " ('jana', 168),\n",
       " ('ya', 167),\n",
       " ('time', 166),\n",
       " ('ok', 165),\n",
       " ('ye', 163),\n",
       " ('nhi', 161),\n",
       " ('kya', 161),\n",
       " ('jo', 161),\n",
       " ('in', 157),\n",
       " ('pe', 157),\n",
       " ('n', 154),\n",
       " ('0', 153),\n",
       " ('hu', 152),\n",
       " ('hun', 151),\n",
       " ('kam', 150),\n",
       " ('aa', 149),\n",
       " ('bhi', 147),\n",
       " ('abi', 142),\n",
       " (\"'\", 140),\n",
       " ('do', 138),\n",
       " ('you', 135),\n",
       " ('mera', 135),\n",
       " ('kuch', 135),\n",
       " ('uni', 134),\n",
       " ('o', 131),\n",
       " ('msg', 127),\n",
       " ('ke', 125),\n",
       " ('aya', 124),\n",
       " ('sb', 123),\n",
       " ('!', 122),\n",
       " ('pata', 119),\n",
       " ('say', 119),\n",
       " ('bhai', 118),\n",
       " ('tk', 118),\n",
       " ('sath', 118),\n",
       " ('le', 117),\n",
       " ('gy', 117),\n",
       " ('send', 116),\n",
       " ('thi', 114),\n",
       " ('ana', 114),\n",
       " ('ge', 112),\n",
       " ('kaha', 111),\n",
       " ('bta', 110),\n",
       " ('2', 110),\n",
       " ('pas', 109),\n",
       " ('ly', 109),\n",
       " ('plz', 108),\n",
       " ('gi', 107),\n",
       " ('ghr', 106),\n",
       " ('bs', 105),\n",
       " ('??', 104),\n",
       " ('r', 103),\n",
       " ('bat', 103),\n",
       " ('no', 102),\n",
       " ('and', 101),\n",
       " ('dy', 100),\n",
       " ('meri', 98),\n",
       " ('keh', 97),\n",
       " ('mje', 96),\n",
       " ('din', 96),\n",
       " ('allah', 96),\n",
       " ('krna', 95),\n",
       " ('for', 95),\n",
       " ('kiya', 95),\n",
       " ('agr', 94),\n",
       " ('1', 92),\n",
       " ('mere', 92),\n",
       " ('bt', 91),\n",
       " ('acha', 90),\n",
       " ('lo', 89),\n",
       " ('ghar', 87),\n",
       " ('hon', 86),\n",
       " ('but', 85),\n",
       " ('thy', 85),\n",
       " ('baat', 85),\n",
       " ('de', 84),\n",
       " ('so', 84),\n",
       " ('par', 82),\n",
       " ('hota', 82),\n",
       " ('of', 82),\n",
       " ('my', 82),\n",
       " ('liye', 82),\n",
       " ('tak', 81),\n",
       " ('mjy', 81),\n",
       " ('be', 80),\n",
       " ('s', 80),\n",
       " ('yaar', 80),\n",
       " ('khud', 79),\n",
       " ('abhi', 79),\n",
       " ('class', 78),\n",
       " ('hua', 78),\n",
       " ('at', 77),\n",
       " ('aik', 77),\n",
       " ('py', 77),\n",
       " ('t', 76),\n",
       " ('kha', 76),\n",
       " ('jao', 76),\n",
       " ('kisi', 75),\n",
       " ('gai', 74),\n",
       " ('on', 73),\n",
       " ('wese', 73),\n",
       " ('mil', 73),\n",
       " ('may', 73),\n",
       " ('sa', 73),\n",
       " ('pr', 73),\n",
       " ('kro', 73),\n",
       " ('ra', 72),\n",
       " ('wala', 71),\n",
       " ('bi', 71),\n",
       " ('thk', 71),\n",
       " ('will', 71),\n",
       " ('have', 70),\n",
       " (':)', 70),\n",
       " ('c', 70),\n",
       " ('???', 70),\n",
       " ('it', 69),\n",
       " ('jani', 69),\n",
       " ('kb', 69),\n",
       " ('tum', 68),\n",
       " ('.....', 68),\n",
       " ('bus', 66),\n",
       " ('hum', 66),\n",
       " ('wali', 65),\n",
       " ('hay', 65),\n",
       " ('log', 64),\n",
       " ('call', 63),\n",
       " ('krta', 63),\n",
       " ('tw', 63),\n",
       " ('rhi', 63),\n",
       " ('we', 62),\n",
       " ('phir', 62),\n",
       " ('free', 61),\n",
       " ('kis', 61),\n",
       " ('hahaha', 61),\n",
       " ('3', 61),\n",
       " ('apni', 61),\n",
       " ('thek', 60),\n",
       " ('lab', 59),\n",
       " ('un', 59),\n",
       " ('subha', 59),\n",
       " ('th', 59),\n",
       " ('dia', 58),\n",
       " ('chal', 58),\n",
       " ('jb', 58),\n",
       " ('mama', 57),\n",
       " ('w', 57),\n",
       " ('q', 57),\n",
       " ('that', 56),\n",
       " ('aye', 56),\n",
       " (':-)', 55),\n",
       " ('ata', 54),\n",
       " ('teri', 54),\n",
       " ('v', 54),\n",
       " ('paper', 53),\n",
       " ('ek', 53),\n",
       " ('min', 53),\n",
       " ('bna', 53),\n",
       " ('jaye', 53),\n",
       " ('bjy', 52),\n",
       " ('papa', 52),\n",
       " ('sai', 52),\n",
       " ('pass', 52),\n",
       " ('mene', 52),\n",
       " ('gae', 52),\n",
       " ('dil', 52),\n",
       " ('yad', 51),\n",
       " ('nd', 51),\n",
       " ('haha', 51),\n",
       " ('mjhe', 51),\n",
       " ('oye', 50),\n",
       " ('dena', 50),\n",
       " ('8', 50),\n",
       " ('mery', 50),\n",
       " ('pa', 50),\n",
       " ('wahan', 50),\n",
       " ('ur', 49),\n",
       " ('dekh', 48),\n",
       " ('ao', 48),\n",
       " ('tera', 48),\n",
       " ('rhy', 48),\n",
       " ('laga', 48),\n",
       " ('hoti', 48),\n",
       " ('jaldi', 48),\n",
       " ('rahi', 48),\n",
       " ('karna', 48),\n",
       " ('mujy', 47),\n",
       " ('office', 47),\n",
       " ('am', 46),\n",
       " ('hm', 46),\n",
       " ('gay', 46),\n",
       " ('late', 46),\n",
       " ('kch', 46),\n",
       " ('eid', 46),\n",
       " ('khana', 45),\n",
       " ('not', 45),\n",
       " ('kahan', 45),\n",
       " ('da', 45),\n",
       " ('tou', 45),\n",
       " ('check', 45),\n",
       " ('yeh', 45),\n",
       " ('reply', 45),\n",
       " ('skta', 45),\n",
       " ('wapis', 44),\n",
       " ('bje', 44),\n",
       " ('if', 44),\n",
       " ('gaya', 44),\n",
       " ('jata', 44),\n",
       " ('use', 44),\n",
       " ('apna', 43),\n",
       " ('10', 43),\n",
       " ('ay', 43),\n",
       " ('bad', 43),\n",
       " ('kafi', 43),\n",
       " ('thora', 43),\n",
       " ('aaj', 43),\n",
       " ('krwa', 42),\n",
       " ('sab', 42),\n",
       " ('lye', 42),\n",
       " (':(', 42),\n",
       " ('good', 42),\n",
       " ('theek', 42),\n",
       " ('mn', 42),\n",
       " ('di', 42),\n",
       " ('nikal', 42),\n",
       " ('nay', 41),\n",
       " ('ae', 41),\n",
       " ('ku', 41),\n",
       " ('sorry', 41),\n",
       " ('day', 40),\n",
       " ('usy', 40),\n",
       " ('li', 40),\n",
       " ('4', 40),\n",
       " ('muje', 40),\n",
       " ('5', 40),\n",
       " ('ai', 39),\n",
       " ('lahore', 39),\n",
       " ('plan', 39),\n",
       " ('dua', 39),\n",
       " ('start', 38),\n",
       " (',,,', 38),\n",
       " ('7', 38),\n",
       " ('hui', 38),\n",
       " ('fb', 38),\n",
       " ('agar', 38),\n",
       " ('test', 38),\n",
       " ('bike', 38),\n",
       " ('nh', 38),\n",
       " ('chala', 37),\n",
       " ('krny', 37),\n",
       " ('jae', 37),\n",
       " ('lia', 37),\n",
       " ('kbi', 37),\n",
       " ('lga', 37),\n",
       " ('your', 37),\n",
       " ('dost', 37),\n",
       " ('mila', 37),\n",
       " ('net', 37),\n",
       " ('masla', 36),\n",
       " ('last', 36),\n",
       " ('try', 36),\n",
       " ('rat', 36),\n",
       " ('kaam', 36),\n",
       " ('lena', 36),\n",
       " ('sakta', 36),\n",
       " ('per', 36),\n",
       " ('are', 35),\n",
       " ('la', 35),\n",
       " ('hoon', 35),\n",
       " ('assignment', 35),\n",
       " ('haan', 35),\n",
       " ('rahy', 35),\n",
       " ('change', 35),\n",
       " ('lag', 35),\n",
       " ('krni', 35),\n",
       " ('ta', 35),\n",
       " ('pay', 35)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is additional text cleaning necessary? I don't see why\n",
    "from collections import Counter\n",
    "train_words = ' '.join( df_train['text_clean'].tolist() ).lower().split()\n",
    "c = Counter( train_words )\n",
    "c.most_common(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afcaeedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "102 318 129\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/owaisraza009/roman-urdu-sentiment-analysis/notebook\n",
    "stopwords1 = [ 'ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
    "               'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
    "               'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se',\n",
    "               'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski',\n",
    "               'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya',\n",
    "               'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi',\n",
    "               'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain',\n",
    "               'krny', 'tou', ]\n",
    "\n",
    "# https://github.com/haseebelahi/roman-urdu-stopwords.git\n",
    "file = 'data/stopwords.txt'\n",
    "stopwords2 = open(file).read().split()\n",
    "print(stopwords2 == stopwords1)\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "stopwords_en  = _stop_words.ENGLISH_STOP_WORDS\n",
    "# selected from stopwords_en\n",
    "stopwords_en2 = [ 'a', 'about', 'also', 'am', 'an', 'and', 'are', 'as', 'at', 'be', \n",
    "                  'been', 'being', 'by', 'co', 'con', 'de', 'eg', 'eight', 'eleven', 'else', 'etc', \n",
    "                  'fifteen', 'fifty', 'five', 'for', 'forty', 'four', 'from', 'had',\n",
    "                  'has', 'hasnt', 'have', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', \n",
    "                  'his', 'how', 'i', 'ie', 'if', 'in', 'inc', 'into', 'is', 'it', 'its', 'itself',\n",
    "                  'ltd', 'me', 'mine', 'my', 'myself', 'nine', 'no', 'now', 'of', 'off', 'on',\n",
    "                  'once', 'one', 'onto', 'or', 'other', 'others', 'our', 'ours', 'ourselves',\n",
    "                  'out', 'part', 'per', 're', 'several', 'she', 'side', 'since', 'six', 'sixty',\n",
    "                  'so', 'ten', 'than', 'that', 'the', 'their', 'them',\n",
    "                  'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', \n",
    "                  'three', 'to', 'twelve', 'twenty', 'two', 'un','us', 'very',\n",
    "                  'via', 'was', 'we', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', \n",
    "                  'who', 'whom', 'whose', 'why', 'with', 'within', 'would', 'yet', 'you', 'your', 'yours',\n",
    "                   'yourself', 'yourselves', ]\n",
    "\n",
    "print( len(stopwords1), len(stopwords_en), len(stopwords_en2), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22878c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcf7f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train before upsampling:\n",
      "1    4070\n",
      "0    2097\n",
      "Name: target, dtype: int64\n",
      "Num neutral datapoints to add: 1973\n",
      "df_train after upsampling:\n",
      "0    4070\n",
      "1    4070\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# upsample neutral\n",
    "print('df_train before upsampling:\\n', df_train['target'].value_counts(), sep='')\n",
    "df_train_neutral = df_train[ df_train['target']==0 ]\n",
    "to_upsample      = df_train[ df_train['target']==1 ].shape[0] - df_train_neutral.shape[0]\n",
    "print('Num neutral datapoints to add:', to_upsample)\n",
    "\n",
    "df_additional = df_train_neutral.sample( n=to_upsample, random_state=random_state )\n",
    "df_train = pd.concat([ df_train.copy(), df_additional.copy() ])\\\n",
    "             .sample(frac=1, random_state=random_state)\\\n",
    "             .reset_index(drop=True)\n",
    "print('df_train after upsampling:\\n', df_train['target'].value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (8140,) (8140,) (1191,) (1191,) \n",
      "\n",
      "['Nahi yrr me ni jna . . jao tum log'\n",
      " 'Yar baba jan Lahore a rahy hain tu puch raha k ap k pass time hy tu kpry wagera leny jana hy q k main or hassan rehman tu ja rahy ap jao gy tu koi achy ly lain gy'\n",
      " 'Mughal Sahib .. Ghar ka kya bna ???'\n",
      " 'jan time wasting kaam hai kuch our kar le ,'\n",
      " 'Pagalwagal to nai ho es time bazar kun ja rhi ho ??'] [0 0 1 1 1] \n",
      "\n",
      "['Tension lene ki koi baat ni'\n",
      " 'Main ghar punch gya hun or ab spny laga hun'\n",
      " 'Nai mje nai mili mail .. mene check ki ti'\n",
      " 'Yr us din mai pura din bzy rahe vo mujy awne hi nai dy rahe the or kal b aisa hi raha koe naw koe aw raha tha aj to mn soba sy dekh rahe hn k tm aw jaao lkn tm to dada jee ke taraf ..'\n",
      " 'Lakin wo abhe dar dar ka chalata ha'] [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['text_clean'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "X_dev = df_dev['text_clean'].values\n",
    "y_dev = df_dev['target'].values\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle( X_train, y_train, random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, '\\n')\n",
    "print(X_train[:5], y_train[:5], '\\n')\n",
    "print(X_dev[:5], y_dev[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76dbc948",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c3541e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_nb = {\n",
    "    'alpha': 1.0,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "clf_params_lr = {\n",
    "    'C': 0.8,\n",
    "    'solver': 'liblinear',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter': 500,\n",
    "    'random_state': random_state,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "ca0e4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.3,                                 # eta\n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,                             # 0-1\n",
    "    'colsample_bynode': 1.0,                              # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 2,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "e92e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char_wb',\n",
    "    'ngram_range': (1,7),\n",
    "    'binary': False,\n",
    "    'stop_words': stopwords1 + stopwords_en2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-85 {color: black;background-color: white;}#sk-container-id-85 pre{padding: 0;}#sk-container-id-85 div.sk-toggleable {background-color: white;}#sk-container-id-85 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-85 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-85 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-85 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-85 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-85 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-85 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-85 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-85 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-85 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-85 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-85 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-85 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-85 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-85 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-85 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-85 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-85 div.sk-item {position: relative;z-index: 1;}#sk-container-id-85 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-85 div.sk-item::before, #sk-container-id-85 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-85 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-85 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-85 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-85 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-85 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-85 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-85 div.sk-label-container {text-align: center;}#sk-container-id-85 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-85 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-85\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char_wb&#x27;, ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;,\n",
       "                                             &#x27;ki&#x27;, &#x27;tha&#x27;, &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;,\n",
       "                                             &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                                             &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;,\n",
       "                                             &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;, &#x27;kar&#x27;, &#x27;lye&#x27;,\n",
       "                                             &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                                             &#x27;gaya&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsampl...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=5, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=2,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-253\" type=\"checkbox\" ><label for=\"sk-estimator-id-253\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char_wb&#x27;, ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;,\n",
       "                                             &#x27;ki&#x27;, &#x27;tha&#x27;, &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;,\n",
       "                                             &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                                             &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;,\n",
       "                                             &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;, &#x27;kar&#x27;, &#x27;lye&#x27;,\n",
       "                                             &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                                             &#x27;gaya&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsampl...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=5, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=2,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-254\" type=\"checkbox\" ><label for=\"sk-estimator-id-254\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char_wb&#x27;, ngram_range=(1, 7),\n",
       "                stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;, &#x27;ki&#x27;, &#x27;tha&#x27;,\n",
       "                            &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;, &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                            &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;, &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;,\n",
       "                            &#x27;kar&#x27;, &#x27;lye&#x27;, &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                            &#x27;gaya&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-255\" type=\"checkbox\" ><label for=\"sk-estimator-id-255\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;merror&#x27;, feature_types=None, gamma=0, gpu_id=None,\n",
       "              grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
       "              max_depth=5, max_leaves=None, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
       "              num_class=2, num_parallel_tree=None, objective=&#x27;multi:softmax&#x27;, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 7),\n",
       "                                 stop_words=['ai', 'ayi', 'hy', 'hai', 'main',\n",
       "                                             'ki', 'tha', 'koi', 'ko', 'sy',\n",
       "                                             'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
       "                                             'rha', 'hota', 'ho', 'ga', 'ka',\n",
       "                                             'le', 'lye', 'kr', 'kar', 'lye',\n",
       "                                             'liye', 'hotay', 'waisay', 'gya',\n",
       "                                             'gaya', ...])),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsampl...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=5, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=2,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective='multi:softmax', ...))])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer( **vect_params )\n",
    "#vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "#clf = MultinomialNB( **clf_params_nb )\n",
    "#clf = LogisticRegression( **clf_params_lr )\n",
    "#clf = SVC()\n",
    "#clf = RidgeClassifierCV()\n",
    "clf = XGBClassifier( **clf_params_xgb )\n",
    "\n",
    "\n",
    "model = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_dev   = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "cfd5f419",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 7),\n",
      "                stop_words=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha',\n",
      "                            'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
      "                            'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr',\n",
      "                            'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
      "                            'gaya', ...])\n",
      "\n",
      "Classifier:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric='merror', feature_types=None, gamma=0, gpu_id=None,\n",
      "              grow_policy=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
      "              max_depth=5, max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
      "              num_class=2, num_parallel_tree=None, objective='multi:softmax', ...)\n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9239    0.9968    0.9590      4070\n",
      "           1     0.9965    0.9179    0.9556      4070\n",
      "\n",
      "    accuracy                         0.9574      8140\n",
      "   macro avg     0.9602    0.9574    0.9573      8140\n",
      "weighted avg     0.9602    0.9574    0.9573      8140\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4947    0.6057    0.5446       388\n",
      "           1     0.7863    0.7011    0.7413       803\n",
      "\n",
      "    accuracy                         0.6700      1191\n",
      "   macro avg     0.6405    0.6534    0.6429      1191\n",
      "weighted avg     0.6913    0.6700    0.6772      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification reports\n",
    "print('Vectorizer:\\n', model['vect'], '\\n', sep='')\n",
    "print('Classifier:\\n', model['clf'], '\\n', sep='')\n",
    "\n",
    "print('\\nTRAINSET')\n",
    "print( classification_report( y_train, y_pred_train, digits=4 ) )\n",
    "\n",
    "print('DEVSET')\n",
    "print( classification_report( y_dev, y_pred_dev, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f32b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68504126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['clf_pred'] = y_pred_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2306e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af9ace6",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char', ngram_range=(1, 7),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "LogisticRegression(C=0.8, max_iter=500, random_state=47, solver='liblinear')\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9366    0.9690    0.9525      4070\n",
    "           1     0.9679    0.9344    0.9509      4070\n",
    "\n",
    "    accuracy                         0.9517      8140\n",
    "   macro avg     0.9523    0.9517    0.9517      8140\n",
    "weighted avg     0.9523    0.9517    0.9517      8140\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5607    0.4639    0.5078       388\n",
    "           1     0.7609    0.8244    0.7914       803\n",
    "\n",
    "    accuracy                         0.7070      1191\n",
    "   macro avg     0.6608    0.6442    0.6496      1191\n",
    "weighted avg     0.6957    0.7070    0.6990      1191\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char', min_df=5, ngram_range=(1, 5),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "MultinomialNB()\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.8755    0.8482    0.8616      4070\n",
    "           1     0.8528    0.8794    0.8659      4070\n",
    "\n",
    "    accuracy                         0.8638      8140\n",
    "   macro avg     0.8641    0.8638    0.8637      8140\n",
    "weighted avg     0.8641    0.8638    0.8637      8140\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5153    0.5206    0.5179       388\n",
    "           1     0.7672    0.7634    0.7653       803\n",
    "\n",
    "    accuracy                         0.6843      1191\n",
    "   macro avg     0.6413    0.6420    0.6416      1191\n",
    "weighted avg     0.6851    0.6843    0.6847      1191\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "\n",
    "Needs fine-tuning\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 7),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "XGBClassifier( max_depth=5, ...)\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9239    0.9968    0.9590      4070\n",
    "           1     0.9965    0.9179    0.9556      4070\n",
    "\n",
    "    accuracy                         0.9574      8140\n",
    "   macro avg     0.9602    0.9574    0.9573      8140\n",
    "weighted avg     0.9602    0.9574    0.9573      8140\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.4947    0.6057    0.5446       388\n",
    "           1     0.7863    0.7011    0.7413       803\n",
    "\n",
    "    accuracy                         0.6700      1191\n",
    "   macro avg     0.6405    0.6534    0.6429      1191\n",
    "weighted avg     0.6913    0.6700    0.6772      1191\n",
    "\n",
    "\n",
    "SVC() accu on training set = 0.99 with LR features\n",
    "RF and Ridge - same with even ngram(1,5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
