{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Text Classification (Three Sentiment Classes)\n",
    "## The Association for Computational Linguistics\n",
    "## WASSA 2023 Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/p2r9brhx2335cwnww04p9w180000gn/T/ipykernel_91463/3622026854.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import time\n",
    "import zipfile\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "import tiktoken\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91371be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    '''Return number of tokens used in a list of messages for ChatGPT'''\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        #print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        #print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        #print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56940aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new new version (Dec 2022)\n",
    "def upsample_all( df_, labels_col='target', random_state=47 ):\n",
    "    '''\n",
    "        Upsample each class in column labels_col of pandas dataframe df_\n",
    "        to the number of data points in majority class\n",
    "    '''\n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_[labels_col].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_[labels_col] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "\n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "\n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "\n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 4) (1191, 10) (1191, 1) (1191, 1)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/mcec_train_translated.pkl'\n",
    "df_train = pd.read_pickle(file1)\n",
    "\n",
    "file2    = 'data/mcec_dev_translated.pkl'\n",
    "df_dev   = pd.read_pickle(file2)\n",
    "\n",
    "file3    = 'data/mcec_test.csv'\n",
    "df_test  = pd.read_csv(file3)\n",
    "\n",
    "file4    = 'data/sample_submission/predictions_MCEC.csv'\n",
    "sample_submission = pd.read_csv(file4)\n",
    "\n",
    "print(df_train.shape, df_dev.shape, df_test.shape, sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8a2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pessimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotion\n",
       "0    neutral\n",
       "1    neutral\n",
       "2  pessimism\n",
       "3    disgust\n",
       "4       fear"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission format\n",
    "print( type(sample_submission) )\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7b97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2key = {\n",
    "    'neutral': 1,\n",
    "    'trust': 2,\n",
    "    'joy': 2,\n",
    "    'optimism': 2,\n",
    "    'anticipation': 2,\n",
    "    'disgust': 0,\n",
    "    'sadness': 0,\n",
    "    'fear': 0,\n",
    "    'anger': 0,\n",
    "    'surprise': 2,\n",
    "    'love': 2,\n",
    "    'pessimism': 0,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a0b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = df_train['emotion'].map( label2key )\n",
    "df_dev['target']   = df_dev['emotion'].map( label2key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         3262\n",
      "trust           1118\n",
      "joy             1022\n",
      "optimism         880\n",
      "anticipation     832\n",
      "disgust          687\n",
      "sadness          486\n",
      "fear             453\n",
      "anger            226\n",
      "surprise         199\n",
      "love             187\n",
      "pessimism        178\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "2    4238\n",
      "1    3262\n",
      "0    2030\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes.I am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yes.i am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>Y. Um in Fap Lab Cabin. Butt Fap Presentations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yar insan ka bcha bn chawliyn na mar :p</td>\n",
       "      <td>joy</td>\n",
       "      <td>Dude become a child of a human being, do not die.</td>\n",
       "      <td>Dude human beings do not die: P: P</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terai uncle nai kahna hai kai ham nai to bahr ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>Your Uncle Nai says that we had sent out money</td>\n",
       "      <td>Your Ankali says that we sent out money and wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr ajao I m cming in the club</td>\n",
       "      <td>neutral</td>\n",
       "      <td>YR AJAO I'M Coming in the Club</td>\n",
       "      <td>Yer organs were the club</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...</td>\n",
       "      <td>joy</td>\n",
       "      <td>Mje wes nimra ahmad ka qur'aan ki aayaat k bar...</td>\n",
       "      <td>Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion  \\\n",
       "0  Yes.I am in fyp lab cabin.but fyp presentation...  neutral   \n",
       "1           Yar insan ka bcha bn chawliyn na mar :p       joy   \n",
       "2  Terai uncle nai kahna hai kai ham nai to bahr ...  disgust   \n",
       "3                      Yr ajao I m cming in the club  neutral   \n",
       "4  Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...      joy   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0  Yes.i am in fyp lab cabin.but fyp presentation...   \n",
       "1  Dude become a child of a human being, do not die.   \n",
       "2     Your Uncle Nai says that we had sent out money   \n",
       "3                     YR AJAO I'M Coming in the Club   \n",
       "4  Mje wes nimra ahmad ka qur'aan ki aayaat k bar...   \n",
       "\n",
       "                                       translated_ur  target  \n",
       "0  Y. Um in Fap Lab Cabin. Butt Fap Presentations...       1  \n",
       "1                 Dude human beings do not die: P: P       2  \n",
       "2  Your Ankali says that we sent out money and wa...       0  \n",
       "3                           Yer organs were the club       1  \n",
       "4  Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...       2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['emotion'].value_counts(), '\\n')\n",
    "print(df_train['target'].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31a848d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         388\n",
      "joy             131\n",
      "trust           125\n",
      "disgust         113\n",
      "optimism        110\n",
      "anticipation     94\n",
      "sadness          62\n",
      "fear             52\n",
      "surprise         35\n",
      "anger            35\n",
      "pessimism        29\n",
      "love             17\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "2    512\n",
      "1    388\n",
      "0    291\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>gpt_pred_num</th>\n",
       "      <th>gpt_translated2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       1   \n",
       "2            Nai mje nai mili mail..mene check ki ti  pessimism       0   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       0   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \\\n",
       "0                         Any talk of taking tangoes   \n",
       "1              I have gone home punch and now dreams   \n",
       "2                                Ni Ni Ni Mille Mail   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                              But Wu runs the cedar   \n",
       "\n",
       "                                          text_clean  gpt_pred  gpt_pred_num  \\\n",
       "0                        Tension lene ki koi baat ni   neutral             1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   neutral             1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti   neutral             1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  negative             0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   neutral             1   \n",
       "\n",
       "                                     gpt_translated2  \n",
       "0                          There's no need to worry.  \n",
       "1   I have reached home and now I am going to sleep.  \n",
       "2      I didn't receive any new mail. I had checked.  \n",
       "3  I was busy the whole day on that day, they wer...  \n",
       "4       But he still walks with fear and hesitation.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_dev['emotion'].value_counts(), '\\n')\n",
    "print(df_dev['target'].value_counts())\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7fa44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# light text cleaning (should I use clean regex for better accuracy?)\n",
    "pad_punct    = re.compile('([^a-zA-Z ]+)')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "#clean        = re.compile('[^a-zA-Z0-9,.?!\\'\\s]+')\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = pad_punct.sub(r' \\1 ', s)\n",
    "    #s = clean.sub(' ', s)\n",
    "    s = multi_spaces.sub(' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply( clean_text )\n",
    "df_dev['text_clean']   = df_dev['text'].apply( clean_text )\n",
    "df_test['text_clean']  = df_test['Text'].apply( clean_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee2b6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 6)\n",
      "(4222, 6)\n",
      "(4221, 6)\n",
      "(4221, 6)\n"
     ]
    }
   ],
   "source": [
    "# 2K duplicates - these may affect claa imbalance during training! TO BE REDUCED\n",
    "print(df_train.shape)\n",
    "temp1 = df_train[ df_train.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_train[ df_train.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_train[ df_train.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20d752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 10)\n",
      "(82, 10)\n",
      "(82, 10)\n",
      "(76, 10)\n"
     ]
    }
   ],
   "source": [
    "# 82 duplicates ['clean_text', 'emotion'] - can't reduce because this is a dev set\n",
    "print(df_dev.shape)\n",
    "temp1 = df_dev[ df_dev.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_dev[ df_dev.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_dev[ df_dev.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f96271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 2)\n",
      "(93, 2)\n",
      "(93, 2)\n"
     ]
    }
   ],
   "source": [
    "# 93 complete duplicates - can't reduce because this is a test set\n",
    "print(df_test.shape)\n",
    "temp1 = df_test[ df_test.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp3 = df_test[ df_test.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7858da6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 554, 526, 526)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train vs. df_dev: half of the dev set is in train set\n",
    "overlap1 = [t for t in df_train['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "overlap2 = [t for t in df_dev['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap1), len(overlap2), len(set(overlap1)), len(set(overlap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5cb0077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 584, 557, 557)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. rest\n",
    "overlap3 = [ t for t in df_train['text_clean'].tolist() + df_dev['text_clean'].tolist()\\\n",
    "             if t in df_test['text_clean'].tolist() ]\n",
    "overlap4 = [ t for t in df_test['text_clean'].tolist() if t in\\\n",
    "             df_train['text_clean'].tolist() + df_dev['text_clean'].tolist() ]\n",
    "len(overlap3), len(overlap4), len(set(overlap3)), len(set(overlap4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8251ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 97, 88, 88)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_dev: half of the dev set is in train set\n",
    "overlap5 = [t for t in df_dev['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap6 = [t for t in df_test['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "len(overlap5), len(overlap6), len(set(overlap5)), len(set(overlap6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96454bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727, 540, 519, 519)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_train: half of the dev set is in train set\n",
    "overlap7 = [t for t in df_train['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap8 = [t for t in df_test['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap7), len(overlap8), len(set(overlap7)), len(set(overlap8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3c269",
   "metadata": {},
   "source": [
    "The reason why baseline ML models perform better than ChatGPT is because they get a lot of hints due to duplicates from the training set! ChatGPT doesn't have this knowledge because it's doing a zero-shot classification! The number of duplicates is such that they would not fit the context window of ChatGPT anyway.\n",
    "\n",
    "The only way to compare ML and ChatGPT correctly is to remove all the duplicates from the TRAINING SET, then train ML model and test it the dev set and compare with ChatGPT! (also, deduplicate the training set)\n",
    "\n",
    "Submission: use non-overfit ML or ChatGPT (whichever is better) on those samples from the test set that don't have duplicates in the training or dev set. Use training/dev set labels for the duploicates in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e50f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 2206\n",
      "(9530, 6)\n",
      "(8151, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove overlap with validation sets\n",
    "val_sets = df_dev['text_clean'].tolist() + df_test['text_clean'].tolist()\n",
    "print(len(val_sets), len(set(val_sets)))\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train = df_train[ ~df_train['text_clean'].isin(val_sets) ]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b2516bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates from train set\n",
    "df_train = df_train.drop_duplicates(subset=['text_clean', 'emotion'])\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05d318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40905d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 2127),\n",
       " ('k', 1244),\n",
       " ('to', 1231),\n",
       " ('ha', 1214),\n",
       " ('hai', 804),\n",
       " ('ho', 793),\n",
       " ('ka', 726),\n",
       " ('me', 640),\n",
       " ('?', 615),\n",
       " ('b', 604),\n",
       " ('kr', 568),\n",
       " ('ga', 559),\n",
       " ('ni', 553),\n",
       " ('ko', 543),\n",
       " ('ki', 532),\n",
       " ('tha', 528),\n",
       " (',', 518),\n",
       " ('...', 502),\n",
       " ('na', 497),\n",
       " ('hn', 473),\n",
       " ('hy', 464),\n",
       " ('wo', 461),\n",
       " ('ma', 453),\n",
       " ('nai', 450),\n",
       " ('..', 450),\n",
       " ('a', 446),\n",
       " ('se', 415),\n",
       " ('p', 409),\n",
       " ('yar', 401),\n",
       " ('or', 392),\n",
       " ('yr', 389),\n",
       " ('h', 388),\n",
       " ('i', 385),\n",
       " ('han', 385),\n",
       " ('tu', 371),\n",
       " ('e', 331),\n",
       " (':', 327),\n",
       " ('ne', 324),\n",
       " ('kia', 321),\n",
       " ('he', 287),\n",
       " ('hain', 284),\n",
       " ('main', 281),\n",
       " ('ab', 254),\n",
       " ('koi', 252),\n",
       " ('us', 251),\n",
       " ('nae', 250),\n",
       " ('ap', 250),\n",
       " ('sir', 250),\n",
       " ('sy', 248),\n",
       " ('tm', 237),\n",
       " ('is', 223),\n",
       " ('nahi', 223),\n",
       " ('hi', 222),\n",
       " ('raha', 220),\n",
       " ('kal', 218),\n",
       " ('rha', 214),\n",
       " ('ja', 202),\n",
       " ('ny', 200),\n",
       " ('aj', 199),\n",
       " ('g', 199),\n",
       " ('m', 198),\n",
       " ('phr', 195),\n",
       " (':-', 193),\n",
       " ('aur', 192),\n",
       " ('mai', 192),\n",
       " ('....', 187),\n",
       " ('gya', 184),\n",
       " ('d', 183),\n",
       " ('bht', 181),\n",
       " ('u', 173),\n",
       " ('pta', 172),\n",
       " ('kar', 171),\n",
       " ('the', 169),\n",
       " ('mein', 168),\n",
       " ('jana', 168),\n",
       " ('ya', 167),\n",
       " ('time', 166),\n",
       " ('ok', 165),\n",
       " ('ye', 163),\n",
       " ('nhi', 161),\n",
       " ('kya', 161),\n",
       " ('jo', 161),\n",
       " ('in', 157),\n",
       " ('pe', 157),\n",
       " ('n', 154),\n",
       " ('0', 153),\n",
       " ('hu', 152),\n",
       " ('hun', 151),\n",
       " ('kam', 150),\n",
       " ('aa', 149),\n",
       " ('bhi', 147),\n",
       " ('abi', 142),\n",
       " (\"'\", 140),\n",
       " ('do', 138),\n",
       " ('you', 135),\n",
       " ('mera', 135),\n",
       " ('kuch', 135),\n",
       " ('uni', 134),\n",
       " ('o', 131),\n",
       " ('msg', 127),\n",
       " ('ke', 125),\n",
       " ('aya', 124),\n",
       " ('sb', 123),\n",
       " ('!', 122),\n",
       " ('pata', 119),\n",
       " ('say', 119),\n",
       " ('bhai', 118),\n",
       " ('tk', 118),\n",
       " ('sath', 118),\n",
       " ('le', 117),\n",
       " ('gy', 117),\n",
       " ('send', 116),\n",
       " ('thi', 114),\n",
       " ('ana', 114),\n",
       " ('ge', 112),\n",
       " ('kaha', 111),\n",
       " ('bta', 110),\n",
       " ('2', 110),\n",
       " ('pas', 109),\n",
       " ('ly', 109),\n",
       " ('plz', 108),\n",
       " ('gi', 107),\n",
       " ('ghr', 106),\n",
       " ('bs', 105),\n",
       " ('??', 104),\n",
       " ('r', 103),\n",
       " ('bat', 103),\n",
       " ('no', 102),\n",
       " ('and', 101),\n",
       " ('dy', 100),\n",
       " ('meri', 98),\n",
       " ('keh', 97),\n",
       " ('mje', 96),\n",
       " ('din', 96),\n",
       " ('allah', 96),\n",
       " ('krna', 95),\n",
       " ('for', 95),\n",
       " ('kiya', 95),\n",
       " ('agr', 94),\n",
       " ('1', 92),\n",
       " ('mere', 92),\n",
       " ('bt', 91),\n",
       " ('acha', 90),\n",
       " ('lo', 89),\n",
       " ('ghar', 87),\n",
       " ('hon', 86),\n",
       " ('but', 85),\n",
       " ('thy', 85),\n",
       " ('baat', 85),\n",
       " ('de', 84),\n",
       " ('so', 84),\n",
       " ('par', 82),\n",
       " ('hota', 82),\n",
       " ('of', 82),\n",
       " ('my', 82),\n",
       " ('liye', 82),\n",
       " ('tak', 81),\n",
       " ('mjy', 81),\n",
       " ('be', 80),\n",
       " ('s', 80),\n",
       " ('yaar', 80),\n",
       " ('khud', 79),\n",
       " ('abhi', 79),\n",
       " ('class', 78),\n",
       " ('hua', 78),\n",
       " ('at', 77),\n",
       " ('aik', 77),\n",
       " ('py', 77),\n",
       " ('t', 76),\n",
       " ('kha', 76),\n",
       " ('jao', 76),\n",
       " ('kisi', 75),\n",
       " ('gai', 74),\n",
       " ('on', 73),\n",
       " ('wese', 73),\n",
       " ('mil', 73),\n",
       " ('may', 73),\n",
       " ('sa', 73),\n",
       " ('pr', 73),\n",
       " ('kro', 73),\n",
       " ('ra', 72),\n",
       " ('wala', 71),\n",
       " ('bi', 71),\n",
       " ('thk', 71),\n",
       " ('will', 71),\n",
       " ('have', 70),\n",
       " (':)', 70),\n",
       " ('c', 70),\n",
       " ('???', 70),\n",
       " ('it', 69),\n",
       " ('jani', 69),\n",
       " ('kb', 69),\n",
       " ('tum', 68),\n",
       " ('.....', 68),\n",
       " ('bus', 66),\n",
       " ('hum', 66),\n",
       " ('wali', 65),\n",
       " ('hay', 65),\n",
       " ('log', 64),\n",
       " ('call', 63),\n",
       " ('krta', 63),\n",
       " ('tw', 63),\n",
       " ('rhi', 63),\n",
       " ('we', 62),\n",
       " ('phir', 62),\n",
       " ('free', 61),\n",
       " ('kis', 61),\n",
       " ('hahaha', 61),\n",
       " ('3', 61),\n",
       " ('apni', 61),\n",
       " ('thek', 60),\n",
       " ('lab', 59),\n",
       " ('un', 59),\n",
       " ('subha', 59),\n",
       " ('th', 59),\n",
       " ('dia', 58),\n",
       " ('chal', 58),\n",
       " ('jb', 58),\n",
       " ('mama', 57),\n",
       " ('w', 57),\n",
       " ('q', 57),\n",
       " ('that', 56),\n",
       " ('aye', 56),\n",
       " (':-)', 55),\n",
       " ('ata', 54),\n",
       " ('teri', 54),\n",
       " ('v', 54),\n",
       " ('paper', 53),\n",
       " ('ek', 53),\n",
       " ('min', 53),\n",
       " ('bna', 53),\n",
       " ('jaye', 53),\n",
       " ('bjy', 52),\n",
       " ('papa', 52),\n",
       " ('sai', 52),\n",
       " ('pass', 52),\n",
       " ('mene', 52),\n",
       " ('gae', 52),\n",
       " ('dil', 52),\n",
       " ('yad', 51),\n",
       " ('nd', 51),\n",
       " ('haha', 51),\n",
       " ('mjhe', 51),\n",
       " ('oye', 50),\n",
       " ('dena', 50),\n",
       " ('8', 50),\n",
       " ('mery', 50),\n",
       " ('pa', 50),\n",
       " ('wahan', 50),\n",
       " ('ur', 49),\n",
       " ('dekh', 48),\n",
       " ('ao', 48),\n",
       " ('tera', 48),\n",
       " ('rhy', 48),\n",
       " ('laga', 48),\n",
       " ('hoti', 48),\n",
       " ('jaldi', 48),\n",
       " ('rahi', 48),\n",
       " ('karna', 48),\n",
       " ('mujy', 47),\n",
       " ('office', 47),\n",
       " ('am', 46),\n",
       " ('hm', 46),\n",
       " ('gay', 46),\n",
       " ('late', 46),\n",
       " ('kch', 46),\n",
       " ('eid', 46),\n",
       " ('khana', 45),\n",
       " ('not', 45),\n",
       " ('kahan', 45),\n",
       " ('da', 45),\n",
       " ('tou', 45),\n",
       " ('check', 45),\n",
       " ('yeh', 45),\n",
       " ('reply', 45),\n",
       " ('skta', 45),\n",
       " ('wapis', 44),\n",
       " ('bje', 44),\n",
       " ('if', 44),\n",
       " ('gaya', 44),\n",
       " ('jata', 44),\n",
       " ('use', 44),\n",
       " ('apna', 43),\n",
       " ('10', 43),\n",
       " ('ay', 43),\n",
       " ('bad', 43),\n",
       " ('kafi', 43),\n",
       " ('thora', 43),\n",
       " ('aaj', 43),\n",
       " ('krwa', 42),\n",
       " ('sab', 42),\n",
       " ('lye', 42),\n",
       " (':(', 42),\n",
       " ('good', 42),\n",
       " ('theek', 42),\n",
       " ('mn', 42),\n",
       " ('di', 42),\n",
       " ('nikal', 42),\n",
       " ('nay', 41),\n",
       " ('ae', 41),\n",
       " ('ku', 41),\n",
       " ('sorry', 41),\n",
       " ('day', 40),\n",
       " ('usy', 40),\n",
       " ('li', 40),\n",
       " ('4', 40),\n",
       " ('muje', 40),\n",
       " ('5', 40),\n",
       " ('ai', 39),\n",
       " ('lahore', 39),\n",
       " ('plan', 39),\n",
       " ('dua', 39),\n",
       " ('start', 38),\n",
       " (',,,', 38),\n",
       " ('7', 38),\n",
       " ('hui', 38),\n",
       " ('fb', 38),\n",
       " ('agar', 38),\n",
       " ('test', 38),\n",
       " ('bike', 38),\n",
       " ('nh', 38),\n",
       " ('chala', 37),\n",
       " ('krny', 37),\n",
       " ('jae', 37),\n",
       " ('lia', 37),\n",
       " ('kbi', 37),\n",
       " ('lga', 37),\n",
       " ('your', 37),\n",
       " ('dost', 37),\n",
       " ('mila', 37),\n",
       " ('net', 37),\n",
       " ('masla', 36),\n",
       " ('last', 36),\n",
       " ('try', 36),\n",
       " ('rat', 36),\n",
       " ('kaam', 36),\n",
       " ('lena', 36),\n",
       " ('sakta', 36),\n",
       " ('per', 36),\n",
       " ('are', 35),\n",
       " ('la', 35),\n",
       " ('hoon', 35),\n",
       " ('assignment', 35),\n",
       " ('haan', 35),\n",
       " ('rahy', 35),\n",
       " ('change', 35),\n",
       " ('lag', 35),\n",
       " ('krni', 35),\n",
       " ('ta', 35),\n",
       " ('pay', 35)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is additional text cleaning necessary? I don't see why\n",
    "from collections import Counter\n",
    "train_words = ' '.join( df_train['text_clean'].tolist() ).lower().split()\n",
    "c = Counter( train_words )\n",
    "c.most_common(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afcaeedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "102 318 129\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/owaisraza009/roman-urdu-sentiment-analysis/notebook\n",
    "stopwords1 = [ 'ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
    "               'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
    "               'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se',\n",
    "               'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski',\n",
    "               'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya',\n",
    "               'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi',\n",
    "               'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain',\n",
    "               'krny', 'tou', ]\n",
    "\n",
    "# https://github.com/haseebelahi/roman-urdu-stopwords.git\n",
    "file = 'data/stopwords.txt'\n",
    "stopwords2 = open(file).read().split()\n",
    "print(stopwords2 == stopwords1)\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "stopwords_en  = _stop_words.ENGLISH_STOP_WORDS\n",
    "# selected from stopwords_en\n",
    "stopwords_en2 = [ 'a', 'about', 'also', 'am', 'an', 'and', 'are', 'as', 'at', 'be', \n",
    "                  'been', 'being', 'by', 'co', 'con', 'de', 'eg', 'eight', 'eleven', 'else', 'etc', \n",
    "                  'fifteen', 'fifty', 'five', 'for', 'forty', 'four', 'from', 'had',\n",
    "                  'has', 'hasnt', 'have', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', \n",
    "                  'his', 'how', 'i', 'ie', 'if', 'in', 'inc', 'into', 'is', 'it', 'its', 'itself',\n",
    "                  'ltd', 'me', 'mine', 'my', 'myself', 'nine', 'no', 'now', 'of', 'off', 'on',\n",
    "                  'once', 'one', 'onto', 'or', 'other', 'others', 'our', 'ours', 'ourselves',\n",
    "                  'out', 'part', 'per', 're', 'several', 'she', 'side', 'since', 'six', 'sixty',\n",
    "                  'so', 'ten', 'than', 'that', 'the', 'their', 'them',\n",
    "                  'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', \n",
    "                  'three', 'to', 'twelve', 'twenty', 'two', 'un','us', 'very',\n",
    "                  'via', 'was', 'we', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', \n",
    "                  'who', 'whom', 'whose', 'why', 'with', 'within', 'would', 'yet', 'you', 'your', 'yours',\n",
    "                   'yourself', 'yourselves', ]\n",
    "\n",
    "print( len(stopwords1), len(stopwords_en), len(stopwords_en2), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "740c56fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train before upsampling:\n",
      "2    2805\n",
      "1    2097\n",
      "0    1265\n",
      "Name: target, dtype: int64\n",
      "df_train after upsampling:\n",
      "0    2805\n",
      "1    2805\n",
      "2    2805\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# upsample neutral\n",
    "print('df_train before upsampling:\\n', df_train['target'].value_counts(), sep='')\n",
    "\n",
    "df_train = upsample_all( df_train.copy(), random_state=random_state )\n",
    "\n",
    "print('df_train after upsampling:\\n', df_train['target'].value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (8415,) (8415,) (1191,) (1191,) \n",
      "\n",
      "['Mrra lapi ni kam kr ra sahi' 'mera intzar kr me a rha hon !!.'\n",
      " 'Yr shor bht aa rha he'\n",
      " 'Waqas kitni dair ma a rahe ho yar juldi a jao yar'\n",
      " 'kahty kitna prhna bs kr do'] [0 2 0 0 1] \n",
      "\n",
      "['Tension lene ki koi baat ni'\n",
      " 'Main ghar punch gya hun or ab spny laga hun'\n",
      " 'Nai mje nai mili mail .. mene check ki ti'\n",
      " 'Yr us din mai pura din bzy rahe vo mujy awne hi nai dy rahe the or kal b aisa hi raha koe naw koe aw raha tha aj to mn soba sy dekh rahe hn k tm aw jaao lkn tm to dada jee ke taraf ..'\n",
      " 'Lakin wo abhe dar dar ka chalata ha'] [1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['text_clean'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "X_dev = df_dev['text_clean'].values\n",
    "y_dev = df_dev['target'].values\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle( X_train, y_train, random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, '\\n')\n",
    "print(X_train[:5], y_train[:5], '\\n')\n",
    "print(X_dev[:5], y_dev[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76dbc948",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40880d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3541e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_nb = {\n",
    "    'alpha': 1.0,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "clf_params_lr = {\n",
    "    'C': 1.0,\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter': 500,\n",
    "    'random_state': random_state,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9792d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.3,                                 # eta\n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,                             # 0-1\n",
    "    'colsample_bynode': 1.0,                              # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 3,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e92e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,7),\n",
    "    'binary': True,\n",
    "    'stop_words': stopwords1 + stopwords_en2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char&#x27;, binary=True,\n",
       "                                 ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;,\n",
       "                                             &#x27;ki&#x27;, &#x27;tha&#x27;, &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;,\n",
       "                                             &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                                             &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;,\n",
       "                                             &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;, &#x27;kar&#x27;, &#x27;lye&#x27;,\n",
       "                                             &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                                             &#x27;gaya&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=47,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char&#x27;, binary=True,\n",
       "                                 ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;,\n",
       "                                             &#x27;ki&#x27;, &#x27;tha&#x27;, &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;,\n",
       "                                             &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                                             &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;,\n",
       "                                             &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;, &#x27;kar&#x27;, &#x27;lye&#x27;,\n",
       "                                             &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                                             &#x27;gaya&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=47,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, binary=True, ngram_range=(1, 7),\n",
       "                stop_words=[&#x27;ai&#x27;, &#x27;ayi&#x27;, &#x27;hy&#x27;, &#x27;hai&#x27;, &#x27;main&#x27;, &#x27;ki&#x27;, &#x27;tha&#x27;,\n",
       "                            &#x27;koi&#x27;, &#x27;ko&#x27;, &#x27;sy&#x27;, &#x27;woh&#x27;, &#x27;bhi&#x27;, &#x27;aur&#x27;, &#x27;wo&#x27;, &#x27;yeh&#x27;,\n",
       "                            &#x27;rha&#x27;, &#x27;hota&#x27;, &#x27;ho&#x27;, &#x27;ga&#x27;, &#x27;ka&#x27;, &#x27;le&#x27;, &#x27;lye&#x27;, &#x27;kr&#x27;,\n",
       "                            &#x27;kar&#x27;, &#x27;lye&#x27;, &#x27;liye&#x27;, &#x27;hotay&#x27;, &#x27;waisay&#x27;, &#x27;gya&#x27;,\n",
       "                            &#x27;gaya&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, random_state=47, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='char', binary=True,\n",
       "                                 ngram_range=(1, 7),\n",
       "                                 stop_words=['ai', 'ayi', 'hy', 'hai', 'main',\n",
       "                                             'ki', 'tha', 'koi', 'ko', 'sy',\n",
       "                                             'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
       "                                             'rha', 'hota', 'ho', 'ga', 'ka',\n",
       "                                             'le', 'lye', 'kr', 'kar', 'lye',\n",
       "                                             'liye', 'hotay', 'waisay', 'gya',\n",
       "                                             'gaya', ...])),\n",
       "                ('clf',\n",
       "                 LogisticRegression(max_iter=500, random_state=47,\n",
       "                                    solver='saga'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer( **vect_params )\n",
    "#vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "#clf = MultinomialNB( **clf_params_nb )\n",
    "clf = LogisticRegression( **clf_params_lr )\n",
    "#clf = XGBClassifier( **clf_params_xgb )\n",
    "\n",
    "\n",
    "model = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_dev   = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfd5f419",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "TfidfVectorizer(analyzer='char', binary=True, ngram_range=(1, 7),\n",
      "                stop_words=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha',\n",
      "                            'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
      "                            'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr',\n",
      "                            'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
      "                            'gaya', ...])\n",
      "\n",
      "Classifier:\n",
      "LogisticRegression(max_iter=500, random_state=47, solver='saga')\n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9865    0.9922    0.9893      2805\n",
      "           1     0.9868    0.9825    0.9846      2805\n",
      "           2     0.9839    0.9825    0.9832      2805\n",
      "\n",
      "    accuracy                         0.9857      8415\n",
      "   macro avg     0.9857    0.9857    0.9857      8415\n",
      "weighted avg     0.9857    0.9857    0.9857      8415\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5225    0.5189    0.5207       291\n",
      "           1     0.5235    0.4871    0.5047       388\n",
      "           2     0.6451    0.6816    0.6629       512\n",
      "\n",
      "    accuracy                         0.5785      1191\n",
      "   macro avg     0.5637    0.5626    0.5627      1191\n",
      "weighted avg     0.5755    0.5785    0.5766      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification reports\n",
    "print('Vectorizer:\\n', model['vect'], '\\n', sep='')\n",
    "print('Classifier:\\n', model['clf'], '\\n', sep='')\n",
    "\n",
    "print('\\nTRAINSET')\n",
    "print( classification_report( y_train, y_pred_train, digits=4 ) )\n",
    "\n",
    "print('DEVSET')\n",
    "print( classification_report( y_dev, y_pred_dev, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ea416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e15f6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['clf_pred'] = y_pred_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73183cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf2ee67",
   "metadata": {},
   "source": [
    "```\n",
    "LogisticRegression is less overfit\n",
    "\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char', min_df=2, ngram_range=(1, 7),\n",
    "                stop_words=stopwords + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "LogisticRegression(max_iter=500, random_state=47, solver='liblinear')\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9520    0.9687    0.9603      6268\n",
    "           1     0.9682    0.9512    0.9596      6268\n",
    "\n",
    "    accuracy                         0.9600     12536\n",
    "   macro avg     0.9601    0.9600    0.9600     12536\n",
    "weighted avg     0.9601    0.9600    0.9600     12536\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.7619    0.6598    0.7072       388\n",
    "           1     0.8456    0.9004    0.8721       803\n",
    "\n",
    "    accuracy                         0.8220      1191\n",
    "   macro avg     0.8038    0.7801    0.7897      1191\n",
    "weighted avg     0.8183    0.8220    0.8184      1191\n",
    "\n",
    "\n",
    "\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 5),\n",
    "                stop_words=stopwords + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "XGBClassifier( n_estimators=200 OR max_depth=7, ...)\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9894    0.9995    0.9944      6268\n",
    "           1     0.9995    0.9893    0.9944      6268\n",
    "\n",
    "    accuracy                         0.9944     12536\n",
    "   macro avg     0.9945    0.9944    0.9944     12536\n",
    "weighted avg     0.9945    0.9944    0.9944     12536\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.7136    0.7191    0.7163       388\n",
    "           1     0.8638    0.8605    0.8621       803\n",
    "\n",
    "    accuracy                         0.8144      1191\n",
    "   macro avg     0.7887    0.7898    0.7892      1191\n",
    "weighted avg     0.8148    0.8144    0.8146      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7a589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396d0fce",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ad9e7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English. Then classify the translated text into one most relevant category from the following list of categories: positive, neutral, or negative. The category must depend on the sentiment or emotional content of the text. Classify the text below and output only one most relevant category from the above list of categories. Text: \"This is a text sample\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_one   = '''The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English. Then classify the translated text into one most relevant category from the following list of categories: positive, neutral, or negative. The category must depend on the sentiment or emotional content of the text. Classify the text below and output only one most relevant category from the above list of categories. Text: \"{}\"'''\n",
    "s = 'This is a text sample'\n",
    "print(prompt_one.format(s), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dc99c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using followup Q1 can improve the reponse. If the reponse has multiple words, first parse it and try to find\n",
    "# the category in it. Only if this doesn't work, send followup Q2. ChatGPT can offer the second category in reponse\n",
    "# to Q1, but can change its mind again and offer a third category if asked Q2\n",
    "followup1 = 'Are you sure about that?'\n",
    "followup2 = 'Output only the category and nothing else'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7583ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral', 'negative', 'positive'}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model          = 'gpt-3.5-turbo'\n",
    "labels_set     = {'positive', 'neutral', 'negative'}\n",
    "clean = re.compile(r'[^a-zA-Z ]+')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ee57217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_label(label_):\n",
    "    '''\n",
    "       Verify if label_ contains any of the categories\n",
    "       from the predefined set of labels\n",
    "    '''\n",
    "    label_ = clean.sub(' ', label_)\n",
    "    label_ = multi_spaces.sub(' ', label_).lower().split()\n",
    "    res    = [i for i in label_ if i in labels_set]\n",
    "    res    = list(set(res))\n",
    "    return '/'.join(res) if res else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "96c4ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_num_tokens(model, messages):\n",
    "    '''Check that there is enough tokens available for a ChatGPT repsonse'''\n",
    "    num_tokens_tiktoken = num_tokens_from_messages(messages, model)\n",
    "    if num_tokens_tiktoken > 3950:\n",
    "        print(f'Number of tokens is {num_tokens_tiktoken} which exceeds 3950')\n",
    "        print(f'TEXT: {text_}\\n')\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_response(model, messages, temperature=0, max_tokens=None):\n",
    "    '''Send request, return reponse'''\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = temperature,        # range(0,2), the more the less deterministic / focused\n",
    "        top_p = 1,                        # top probability mass, e.g. 0.1 = only tokens from top 10% proba mass\n",
    "        n = 1,                            # number of chat completions\n",
    "        #max_tokens = max_tokens,          # tokens to return\n",
    "        stream = False,        \n",
    "        stop=None,                        # sequence to stop generation (new line, end of text, etc.)\n",
    "        )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    #num_tokens_api = response['usage']['prompt_tokens']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0eb629b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire text below or parts of it can be written in Roman Urdu. Act as a smart Roman Urdu to English translator, and do your best to translate the text below completely into English. Pay special attention to and carefully convey the correct English meaning of any words or phrases that describe sentiment or emotions. Based on the above instructions carefully translate the text below into English. Output only the English translation. Text: This is a text sample \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_text(text_, prompt_):\n",
    "    '''Translate text_ using prompt_ and ChatGPT API'''    \n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [            \n",
    "            { \"role\": \"system\", \"content\": \"You are a smart translator from Roman Urdu into English.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    return get_response(model, messages)\n",
    "\n",
    "prompt_translate = '''The entire text below or parts of it can be written in Roman Urdu. Act as a smart Roman Urdu to English translator, and do your best to translate the text below completely into English. Pay special attention to and carefully convey the correct English meaning of any words or phrases that describe sentiment or emotions. Based on the above instructions carefully translate the text below into English. Output only the English translation. Text: {}'''\n",
    "s = 'This is a text sample'\n",
    "print(prompt_translate.format(s), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1f9fa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text_, prompt_):\n",
    "    '''Classify text_ using prompt_ and ChatGPT API'''\n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            #{ \"role\": \"system\", \"content\": \"You are a very accurate zero-shot text classifier.\", },            \n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    label_ = get_response(model, messages)\n",
    "        \n",
    "    # if label > 1 word long OR label has additional characters\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)\n",
    "        \n",
    "    # if label not found in response text - second, extended chat\n",
    "    if label_ is None:\n",
    "        messages += [\n",
    "            { \"role\": \"assistant\", \"content\": old_label, },\n",
    "            { \"role\": \"user\", \"content\": followup1, }\n",
    "            ]        \n",
    "        label_    = get_response(model, messages)        \n",
    "        old_label = label_\n",
    "        label_    = verify_label(label_)\n",
    "            \n",
    "    return label_ if label_ is not None else old_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0773f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text2(text_, prompt_):\n",
    "    '''Classify text_ using prompt_ and ChatGPT API and two questions'''\n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            #{ \"role\": \"system\", \"content\": \"You are a very accurate zero-shot text classifier.\", },            \n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    label_ = get_response(model, messages)\n",
    "    \n",
    "        \n",
    "    # if label > 1 word long OR label has additional characters\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)\n",
    "        \n",
    "    # if label not found in response text - second, extended chat\n",
    "    if label_ is None:\n",
    "        messages += [\n",
    "            { \"role\": \"assistant\", \"content\": old_label, },\n",
    "            { \"role\": \"user\", \"content\": followup1, }\n",
    "            ]\n",
    "        label_    = get_response(model, messages)        \n",
    "        old_label = label_\n",
    "        label_    = verify_label(label_)\n",
    "            \n",
    "    return label_ if label_ is not None else old_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8bb18d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English. Then classify the translated text into one most relevant category from the following list of categories: positive, neutral, or negative. The category must depend on the sentiment or emotional content of the text. Classify the text below and output only one most relevant category from the above list of categories. Text: \"Yar phely kab mana kia ha tm lOgo kO yr ajaO wsy b abi free hO\"\n",
      "\n",
      "GROUNDTRUTH LABEL:\n",
      "n/e/u/t/r/a/l\n",
      "\n",
      "PREDICTED LABEL:\n",
      "\"Yaar pehle kab mana kia hai tum logon ko yaar ajaao. Waise bhi abhi free ho.\"\n",
      "\n",
      "Translation: \"Friend, when did I ever refuse you guys to come over? Come on over, I'm free right now anyway.\"\n"
     ]
    }
   ],
   "source": [
    "# test as single prompt\n",
    "idx = 11\n",
    "text, groundtruth_labels = df_dev[['text_clean', 'emotion']].values[idx]\n",
    "#label = classify_text(text, prompt_one)\n",
    "translated = translate_text(text, prompt_translate)\n",
    "\n",
    "print(prompt_one.format( text ))\n",
    "print(f\"\\nGROUNDTRUTH LABEL:\\n{'/'.join( groundtruth_labels )}\")\n",
    "print(f\"\\nPREDICTED LABEL:\\n{translated}\")\n",
    "#print(f'\\nTOTAL TOKENS: {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e957b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc30a36d695f41178159683f16e04454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt 1 tqdm results - 1191/1191 [25:38<00:00, 1.18s/it]\n",
    "df_dev['gpt_translated2'] = df_dev['text_clean'].progress_apply( lambda x: translate_text(x, prompt_translate) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "46cd9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_apostr(s):\n",
    "    if s.startswith('\"'):\n",
    "        s = s[1:]\n",
    "    if s.endswith('\"'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "\n",
    "df_dev['gpt_translated2'] = df_dev['gpt_translated2'].apply( strip_apostr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e5be0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = 'data/mcec_dev2.xlsx'\n",
    "#df_dev.to_excel(file, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0143ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file2 = 'data/mcec_dev_translated.pkl'\n",
    "#df_dev.to_pickle( file2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "92811b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1019\n",
       "negative      88\n",
       "positive      84\n",
       "Name: gpt_pred, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['gpt_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ca209010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              0\n",
       "emotion           0\n",
       "target            0\n",
       "gtp_translated    0\n",
       "translated_hi     0\n",
       "translated_ur     0\n",
       "text_clean        0\n",
       "gpt_pred          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7dedb2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text              0\n",
      "emotion           0\n",
      "target            0\n",
      "gtp_translated    0\n",
      "translated_hi     0\n",
      "translated_ur     0\n",
      "text_clean        0\n",
      "gpt_pred          0\n",
      "gpt_pred_num      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    1019\n",
       "0      88\n",
       "2      84\n",
       "Name: gpt_pred_num, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_pred(pred):\n",
    "    '''Convert text prediction into number'''\n",
    "    if pred=='negative':\n",
    "        return 0\n",
    "    elif pred=='neutral':\n",
    "        return 1\n",
    "    elif pred=='positive':\n",
    "        return 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_dev['gpt_pred_num'] = df_dev['gpt_pred'].apply( convert_pred )\n",
    "print(df_dev.isna().sum())\n",
    "df_dev['gpt_pred_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5961083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ChatGPT made no prediction, choose the prediction coming from the classifier\n",
    "def improve_predictions(row):\n",
    "    if row['gpt_pred_num'] is None:\n",
    "        row['gpt_pred_binary'] = row['clf_pred']\n",
    "    return row\n",
    "\n",
    "#df_dev = df_dev.apply( improve_predictions, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f659274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6818    0.2062    0.3166       291\n",
      "           1     0.3582    0.9407    0.5188       388\n",
      "           2     0.7738    0.1270    0.2181       512\n",
      "\n",
      "    accuracy                         0.4114      1191\n",
      "   macro avg     0.6046    0.4246    0.3512      1191\n",
      "weighted avg     0.6159    0.4114    0.3402      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev      = df_dev['target'].values\n",
    "y_dev_pred = df_dev['gpt_pred_num'].values\n",
    "print( classification_report( y_dev, y_dev_pred, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfd118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3c932f",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693df45",
   "metadata": {},
   "source": [
    "### Prompts and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09d1c8",
   "metadata": {},
   "source": [
    "_Prompt_: The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English. Then classify the translated text into one most relevant category from the following list of categories: positive, neutral, or negative. The category must depend on the sentiment or emotional content of the text. Classify the text below and output only one most relevant category from the above list of categories. Text: \"This is a text sample\"\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.6818    0.2062    0.3166       291\n",
    "           1     0.3582    0.9407    0.5188       388\n",
    "           2     0.7738    0.1270    0.2181       512\n",
    "\n",
    "    accuracy                         0.4114      1191\n",
    "   macro avg     0.6046    0.4246    0.3512      1191\n",
    "weighted avg     0.6159    0.4114    0.3402      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52962add",
   "metadata": {},
   "source": [
    "_Prompt used for gpt_translated2 column translation_: The entire text below or parts of it can be written in Roman Urdu. Act as a smart Roman Urdu to English translator, and do your best to translate the text below completely into English. Pay special attention to and carefully convey the correct English meaning of any words or phrases that describe sentiment or emotions. Based on the above instructions carefully translate the text below into English. Output only the English translation. Text: Yar phely kab mana kia ha tm lOgo kO yr ajaO wsy b abi free hO\n",
    "\n",
    "GROUNDTRUTH LABEL:\n",
    "n/e/u/t/r/a/l\n",
    "\n",
    "PREDICTED LABEL:\n",
    "\"Yaar pehle kab mana kia hai tum logon ko yaar ajaao. Waise bhi abhi free ho.\"\n",
    "\n",
    "Translation: \"Friend, when did I ever refuse you guys to come over? Come on over, I'm free right now anyway.\"\n",
    "\n",
    "_Conclusion about the prompt_: this is not an efficient prompt. Sometimes it outputs extraneous text (see above) that needs additional cleaning. Sometimes, ChatGPT says that it can't translate because it doesn't know (because of \"smart\" or \"do your best\" - perfectionism?) while the first translation prompt did produce a translation. Also, ChatGPT is not always clear about the direction of translation (because of \"Roman Urdu to English translator\"). ChatGPT also is making comments like: \"Sorry, I cannot translate because this is a form of slang that is not appropriate for professional or polite conversation\" - seriously? When the text is already in English, it may say: Sorry, I cannot translate Roman Urdu into English without the Roman Urdu text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542fb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8cfa35",
   "metadata": {},
   "source": [
    "### Best HPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42043735",
   "metadata": {},
   "source": [
    "```\n",
    "clf_params_lr = {\n",
    "    'C': 1.0,\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter': 500,\n",
    "    'random_state': random_state,\n",
    "}\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,7),\n",
    "    'binary': True,\n",
    "    'stop_words': stopwords1 + stopwords_en2,\n",
    "}\n",
    "\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char', binary=True, ngram_range=(1, 7),\n",
    "                stop_words=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha',\n",
    "                            'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
    "                            'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr',\n",
    "                            'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
    "                            'gaya', ...])\n",
    "\n",
    "Classifier:\n",
    "LogisticRegression(max_iter=500, random_state=47, solver='saga')\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9865    0.9922    0.9893      2805\n",
    "           1     0.9868    0.9825    0.9846      2805\n",
    "           2     0.9839    0.9825    0.9832      2805\n",
    "\n",
    "    accuracy                         0.9857      8415\n",
    "   macro avg     0.9857    0.9857    0.9857      8415\n",
    "weighted avg     0.9857    0.9857    0.9857      8415\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5225    0.5189    0.5207       291\n",
    "           1     0.5235    0.4871    0.5047       388\n",
    "           2     0.6451    0.6816    0.6629       512\n",
    "\n",
    "    accuracy                         0.5785      1191\n",
    "   macro avg     0.5637    0.5626    0.5627      1191\n",
    "weighted avg     0.5755    0.5785    0.5766      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b806d0c",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "```\n",
    "Vectorizer:\n",
    "TfidfVectorizer(analyzer='char_wb', binary=True, min_df=2, ngram_range=(1, 5),\n",
    "                stop_words=stopwords1 + stopwords_en2)\n",
    "\n",
    "Classifier:\n",
    "XGBClassifier(learning_rate=0.3, n_estimators=100, max_depth=6, ...)\n",
    "\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9818    0.9811    0.9815      2805\n",
    "           1     0.9386    0.9857    0.9616      2805\n",
    "           2     0.9861    0.9373    0.9611      2805\n",
    "\n",
    "    accuracy                         0.9680      8415\n",
    "   macro avg     0.9688    0.9680    0.9680      8415\n",
    "weighted avg     0.9688    0.9680    0.9680      8415\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5205    0.4364    0.4748       291\n",
    "           1     0.5116    0.6263    0.5632       388\n",
    "           2     0.6716    0.6191    0.6443       512\n",
    "\n",
    "    accuracy                         0.5768      1191\n",
    "   macro avg     0.5679    0.5606    0.5607      1191\n",
    "weighted avg     0.5826    0.5768    0.5764      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
