{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. ReviewText + length of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\n",
      "Scikit-learn 0.20.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "#nltk.download('stopwords')    # this is done just once\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from platform import python_version\n",
    "print('Python {}'.format(python_version()))\n",
    "print('Scikit-learn {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in json, add length of review as additional feature to reviewText, get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 982619\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path=''\n",
    "file='kindle_reviews.json'\n",
    "df = pd.read_json(path_or_buf=path+file, lines=True, encoding='utf-8')    #, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, chunksize=None, compression='infer')\n",
    "print('Length of text: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1F6404F1VG29J</td>\n",
       "      <td>Avidreader</td>\n",
       "      <td>Nice vintage story</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "      <td>01 6, 2014</td>\n",
       "      <td>AN0N05A9LIJEQ</td>\n",
       "      <td>critters</td>\n",
       "      <td>Different...</td>\n",
       "      <td>1388966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "      <td>04 4, 2014</td>\n",
       "      <td>A795DMNCJILA6</td>\n",
       "      <td>dot</td>\n",
       "      <td>Oldie</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>A1FV0SX13TWVXQ</td>\n",
       "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
       "      <td>I really liked it.</td>\n",
       "      <td>1392768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "      <td>03 19, 2014</td>\n",
       "      <td>A3SPTOKDG7WBLN</td>\n",
       "      <td>Father Dowling Fan</td>\n",
       "      <td>Period Mystery</td>\n",
       "      <td>1395187200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  B000F83SZQ  [0, 0]        5   \n",
       "1  B000F83SZQ  [2, 2]        4   \n",
       "2  B000F83SZQ  [2, 2]        4   \n",
       "3  B000F83SZQ  [1, 1]        5   \n",
       "4  B000F83SZQ  [0, 1]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I enjoy vintage books and movies so I enjoyed ...   05 5, 2014   \n",
       "1  This book is a reissue of an old one; the auth...   01 6, 2014   \n",
       "2  This was a fairly interesting read.  It had ol...   04 4, 2014   \n",
       "3  I'd never read any of the Amy Brewster mysteri...  02 19, 2014   \n",
       "4  If you like period pieces - clothing, lingo, y...  03 19, 2014   \n",
       "\n",
       "       reviewerID                         reviewerName             summary  \\\n",
       "0  A1F6404F1VG29J                           Avidreader  Nice vintage story   \n",
       "1   AN0N05A9LIJEQ                             critters        Different...   \n",
       "2   A795DMNCJILA6                                  dot               Oldie   \n",
       "3  A1FV0SX13TWVXQ  Elaine H. Turley \"Montana Songbird\"  I really liked it.   \n",
       "4  A3SPTOKDG7WBLN                   Father Dowling Fan      Period Mystery   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1399248000  \n",
       "1      1388966400  \n",
       "2      1396569600  \n",
       "3      1392768000  \n",
       "4      1395187200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the \"length\" column to reviewText\n",
    "df['length'] = df['reviewText'].apply(lambda x: len(x))\n",
    "df['reviewText'] = df['length'].map(str) + ' ' + df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['reviewText', 'overall']].copy()        # copy only certain columns to another df\n",
    "df = None                                             # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294 I enjoy vintage books and movies so I enjo...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>455 This book is a reissue of an old one; the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375 This was a fairly interesting read.  It ha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101 I'd never read any of the Amy Brewster mys...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130 If you like period pieces - clothing, ling...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  294 I enjoy vintage books and movies so I enjo...        5\n",
       "1  455 This book is a reissue of an old one; the ...        4\n",
       "2  375 This was a fairly interesting read.  It ha...        4\n",
       "3  101 I'd never read any of the Amy Brewster mys...        5\n",
       "4  130 If you like period pieces - clothing, ling...        4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert 5 ratings to 3 ('neg', 'mixed', and 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 5 ratings to 3 ('neg', 'mixed', and 'pos')\n",
    "df_text['overall'] = df_text['overall'].apply(lambda x: 'pos' if x > 3 else 'neg' if x < 3 else 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294 I enjoy vintage books and movies so I enjo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>455 This book is a reissue of an old one; the ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375 This was a fairly interesting read.  It ha...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101 I'd never read any of the Amy Brewster mys...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130 If you like period pieces - clothing, ling...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText overall\n",
       "0  294 I enjoy vintage books and movies so I enjo...     pos\n",
       "1  455 This book is a reissue of an old one; the ...     pos\n",
       "2  375 This was a fairly interesting read.  It ha...     pos\n",
       "3  101 I'd never read any of the Amy Brewster mys...     pos\n",
       "4  130 If you like period pieces - clothing, ling...     pos"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the dataset is 982619\n"
     ]
    }
   ],
   "source": [
    "print('Total length of the dataset is {}'.format(len(df_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reduce size of data, get reduced-size dataset and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sample is 42722\n"
     ]
    }
   ],
   "source": [
    "# get a random sample from the dataframe whose size is manageable for cross-validation and grid search\n",
    "# with more computing resources and/or time, this can be done on a larger data set\n",
    "length = len(df_text)\n",
    "df_text = df_text.sample(n=length)\n",
    "df_text_short = df_text.sample(n=int(length/23))\n",
    "print('Length of the sample is {}'.format(len(df_text_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos      35977\n",
       "mixed     4274\n",
       "neg       2471\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique labels\n",
    "df_text_short.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our train set and labels\n",
    "data = df_text_short['reviewText'].values\n",
    "labels = df_text_short['overall'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Experimenting with different lists of stopwords and selecting the most efficient one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['around', 'any', 'everything', 'anywhere', 'alone', 'two', 'of', 'done', 'much', 'been', 'ever', 'would', 'anything', 'was', 'an', 'mostly', 'describe', 'from', 'along', 'down', 'wherein', 'ten', 'herein', 'were', 'here', 'show', 'nine', 'beforehand', 'onto', 'once', 'hers', 'my', 'neither', 'myself', 'nowhere', 'perhaps', 'cannot', 'other', 'him', 'when', 'made', 'our', 'whatever', 'which', 'am', 'latter', 'only', 'but', 'call', 'give', 'hasnt', 'somewhere', 'found', 'part', 'con', 'seeming', 'be', 'else', 'its', 'therein', 'anyone', 'became', 'nevertheless', 'there', 'whom', 'same', 'between', 'fifteen', 'hereby', 'keep', 'behind', 'name', 'thin', 'throughout', 'get', 'us', 'further', 'hereupon', 'yet', 'we', 'fifty', 'sometimes', 'hence', 'mill', 'whereby', 'to', 'this', 'already', 'who', 'several', 'ie', 'afterwards', 'they', 'become', 'above', 'them', 'four', 'your', 'whole', 'eleven', 'noone', 'find', 'while', 'those', 'sixty', 'forty', 'by', 'still', 'formerly', 'couldnt', 'herself', 'at', 'wherever', 'former', 'front', 'ltd', 'over', 'mine', 'all', 'anyhow', 'are', 'about', 'even', 'inc', 'un', 'often', 'cant', 'almost', 'becomes', 'after', 'since', 'and', 'six', 'both', 'these', 'ours', 'is', 'always', 'too', 'again', 'latterly', 'serious', 'because', 'hundred', 'should', 'each', 'becoming', 'co', 'take', 'than', 'themselves', 'bottom', 'put', 'yourself', 'very', 'some', 'most', 'thereupon', 'per', 'himself', 'amount', 'if', 'upon', 'nobody', 'not', 'before', 'go', 'now', 'namely', 'within', 'during', 'ourselves', 'that', 'own', 'beyond', 'de', 'thence', 'hereafter', 'somehow', 'for', 'seem', 'well', 'either', 'their', 'against', 're', 'through', 'seems', 'whither', 'could', 'or', 'so', 'last', 'why', 'besides', 'may', 'something', 'no', 'empty', 'less', 'twelve', 'then', 'out', 'without', 'others', 'otherwise', 'has', 'via', 'thru', 'although', 'towards', 'her', 'third', 'also', 'across', 'beside', 'on', 'top', 'fill', 'below', 'everywhere', 'do', 'except', 'seemed', 'yourselves', 'among', 'with', 'side', 'thereafter', 'as', 'into', 'until', 'whence', 'had', 'she', 'yours', 'another', 'whereafter', 'under', 'sometime', 'can', 'it', 'might', 'few', 'a', 'whenever', 'due', 'together', 'have', 'detail', 'anyway', 'sincere', 'three', 'meanwhile', 'whoever', 'whereupon', 'none', 'up', 'one', 'me', 'whereas', 'move', 'nor', 'least', 'itself', 'nothing', 'more', 'back', 'full', 'moreover', 'see', 'indeed', 'five', 'such', 'cry', 'every', 'bill', 'thereby', 'fire', 'enough', 'whether', 'where', 'will', 'in', 'therefore', 'thick', 'i', 'his', 'thus', 'twenty', 'amoungst', 'he', 'being', 'everyone', 'amongst', 'many', 'please', 'off', 'toward', 'the', 'what', 'system', 'eight', 'you', 'however', 'eg', 'next', 'though', 'rather', 'interest', 'whose', 'elsewhere', 'must', 'someone', 'etc', 'first', 'never', 'how']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "153\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['around', 'any', 'everything', 'anywhere', 'does', 'alone', 'quite', 'two', 'thereabout', 'of', 'lest', 'ff', 'done', 'much', 'been', \"i'm\", 'thou', 'ever', 'isn', 'would', 'anything', 'was', 'an', 'week', 'mostly', 'describe', 'from', 'ought', 'along', 'cos', 'down', 'hast', 'unable', 'wherein', 'ten', 'were', 'herein', 'here', 'staves', 'show', 'nine', 'beforehand', 'onto', 'stave', \"we'd\", 'supposing', 'once', 'hers', 'my', 'neither', 'myself', 'contrariwise', 'save', 'nowhere', 'perhaps', 'cannot', 'other', 'him', 'when', \"they'll\", \"won't\", 'made', 'ms', 'our', 'll', 'whatever', 'which', 'kind', 'am', 'latter', 'only', \"aren't\", 'but', 'call', \"you'd\", 'thereto', 'give', 't', 'hasnt', 'hadn', 'somewhere', 'found', 'part', 'con', 'seeming', 'whereof', 'be', 'year', 'hasn', 'else', 'including', 'its', 'therein', 'anyone', 'became', 'nevertheless', 'underneath', 'there', 'wasn', 'spake', 'whomever', 'whom', 'seen', 'same', 'between', 'fifteen', 'hereby', 'keep', 'dost', 'dual', 'sent', 'behind', 'name', 'thin', 'throughout', 'get', \"what's\", 'us', 'wilt', 'hindmost', 'maybe', 'further', 'hereupon', 'exclusive', 'yet', 'sake', 'aren', 'we', 'need', \"here's\", 'fifty', 'provide', 'include', 'saw', 'really', 'sometimes', 'hence', 'mill', 'whereby', \"who's\", 'to', 'this', 'already', 'thy', 'who', 'cf', 've', 'several', 'seldom', 'ie', 'afterwards', 'they', 'henceforth', 'slept', 'become', 'seeing', 'above', \"shouldn't\", 'double', 'them', '*', 'four', '\"', 'your', 'whole', 'whilst', 'furthermore', 'eleven', 'noone', 'find', 'hitherto', 'while', 'whensoever', 'those', \"we're\", 'nowadays', 'mustn', 'sixty', 'forty', 'said', 'outside', 'yippee', \"needn't\", 'by', 'still', 'halves', 'formerly', 'couldnt', 'let', 'ma', 's', 'herself', 'at', 'wherever', 'used', 'spat', 'former', 'like', 'front', 'inside', 'ltd', 'over', 'slunk', 'shouldn', 'excepting', 'day', 'little', 'just', 'mine', 'all', 'want', 'vs', 'anyhow', 'are', 'about', 'even', 'shan', 'inc', 'un', 'choose', 'unless', 'don', 'thereof', 'spoke', 'whereto', 'often', 'thrice', 'cant', 'almost', 'becomes', 'after', 'whereabouts', 'thee', 'since', 'sprang', 'and', 'notwithstanding', \"he's\", 'six', 'both', 'these', 'ours', 'is', 'canst', 'always', 'furthest', 'too', 'again', 'thereon', 'latterly', 'serious', 'because', 'somebody', \"hadn't\", 'meantime', 'hundred', 'should', 'hereabouts', \"there's\", 'each', 'becoming', 'co', '/', 'excluding', 'howsoever', 'use', 'take', 'than', 'themselves', \"we'll\", 'mightn', 'bottom', 'put', 'yourself', 'inwards', 'sideways', 'very', 'some', 'most', \"they've\", 'thereupon', 'per', 'himself', 'upwards', 'round', 'amount', 'whatsoever', 'wow', 'whew', 'if', 'upon', 'ugh', \"'\", \"didn't\", 'exception', 'nobody', 'not', 'before', 'far', \"we've\", 'go', \"i'll\", 'now', 'namely', 'within', 'during', 'whichever', 'ourselves', 'o', 'that', '``', 'anybody', 'own', 'beyond', 'whomsoever', 'de', 'thence', 'hereafter', 'somehow', '+', \"couldn't\", 'for', 'seem', 'well', 'either', 'having', \"you'll\", 'their', 'albeit', 'against', 're', 'through', 'seems', 'plenty', 'whither', \"mustn't\", 'according', 'could', 'weren', 'selves', 'or', 'wherefore', 'so', 'last', 'why', 'besides', 'included', 'may', 'something', \"i've\", 'no', 'haven', 'empty', \"don't\", \"how's\", \"you're\", 'less', 'twelve', 'doing', 'm', \"hasn't\", 'hither', \"where's\", 'couldn', 'everybody', \"he'll\", '?', 'then', 'forth', 'out', 'without', 'others', 'av', 'otherwise', '.', \"she's\", \"she'll\", 'has', 'kg', 'via', 'thru', 'although', 'slew', 'towards', 'whichsoever', 'et', 'her', 'third', '-', 'shown', 'inasmuch', \"wouldn't\", 'also', \"let's\", 'across', 'beside', 'on', 'top', 'wherefrom', 'fill', 'smote', 'spoken', \"i'd\", 'thenceforth', 'below', \"you've\", \"''\", 'everywhere', 'do', 'theirs', 'wouldn', 'except', 'seemed', \"haven't\", 'yourselves', \"she'd\", 'among', 'excepted', 'with', 'side', 'mr', 'thereafter', 'as', 'into', 'until', 'worse', 'whence', 'whereon', 'had', 'she', 'yours', 'didn', \"isn't\", 'hath', 'whoa', 'another', 'whereafter', 'under', 'sometime', 'can', 'it', 'doesn', \"they'd\", 'might', 'cu', \"shan't\", 'few', '`', 'needn', 'farthest', 'a', 'whenever', 'due', 'ye', 'whereat', 'nonetheless', 'wherewith', 'together', ',', \"when's\", \"they're\", 'have', 'detail', 'anyway', 'using', 'sincere', 'three', 'farther', 'meanwhile', 'somewhat', 'ok', 'whosoever', 'whoever', 'thereabouts', \"that's\", 'whereupon', 'nope', \"that'll\", 'none', 'up', 'slung', 'one', '!', 'me', 'sprung', 'whereas', 'move', 'nor', \"doesn't\", 'least', 'unlike', 'itself', 'nothing', 'forward', 'worst', 'more', 'shalt', 'hardly', 'back', 'full', 'moreover', 'see', 'indeed', 'five', 'such', 'cry', 'instead', 'every', 'bill', 'thereby', 'fire', 'enough', 'whether', \"should've\", \"why's\", 'where', 'thyself', 'will', 'in', 'therefore', 'thick', 'i', 'his', 'hereto', 'thus', 'till', 'won', 'wheresoever', 'insomuch', 'twenty', 'amoungst', 'he', \"it's\", 'being', 'everyone', 'amongst', 'certain', 'many', 'please', 'doth', \"weren't\", 'off', 'toward', 'sang', 'y', 'did', 'the', \"wasn't\", 'km', 'what', 'system', 'eight', 'you', 'however', 'eg', 'next', 'though', 'upward', \"mightn't\", 'rather', 'interest', 'whereunto', 'whose', 'd', 'inward', 'apart', 'elsewhere', 'must', 'whereinto', 'exclude', 'ain', 'someone', 'etc', 'first', 'never', 'indoors', 'how', 'mrs', \"he'd\"]\n"
     ]
    }
   ],
   "source": [
    "# GENERATING A LIST OF STOPWORDS\n",
    "# these various stopword lists and the combined joint list were tested on the same classifier (MultinomialNB) with the same\n",
    "# parameters, and it was found that the lemur list and the combined list, the latter includes the former, were the most\n",
    "# efficient ones\n",
    "\n",
    "from sklearn.feature_extraction import stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Straightforward Implementation of a text classifier (as a benchmark)\n",
    "Using the same two classifiers - Naive Bayes and SVM. The classification functions are generic, so you can use any other classifiers by just making minor modifications of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple straightforward\n",
    "def clf_simple(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; use TfidfVectorizer\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))\n",
    "    matrix_train = vectorizer.fit_transform(trainX)    # lowercase=True by default, initially min_df=15, max_df=0.23\n",
    "    matrix_test = vectorizer.transform(testX)\n",
    "                   \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = classifier\n",
    "    clf = clf.fit(matrix_train, trainY)  \n",
    "        \n",
    "    # predict and compute metrics    \n",
    "    predictions=clf.predict(matrix_test)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text Classifier with Pipeline (as a benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "def clf_pipe(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "       \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    predictions=clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text Classifier Using 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cross_val_score\n",
    "def clf_cv(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "        \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(clf, trainX, trainY, cv=kfold, scoring='f1_micro')\n",
    "    print('Cross-validated Accuracy of {}: {:0.4f} +/- {:0.4f}'.format(classifier_name, scores.mean(), scores.std() * 2))\n",
    "    predictions = clf.predict(testX)\n",
    "    cm          = metrics.confusion_matrix(testY, predictions)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Text Classifier with Cross-Validated Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "def clf_GridSearchCV(classifier, data, labels, param_grid):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "            \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # do 3-fold cross validation for each of the possible combinations of the parameter values above\n",
    "    grid = GridSearchCV(clf, cv=3, param_grid=param_grid, scoring='f1_micro')\n",
    "    grid.fit(trainX, trainY)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid.best_score_, \n",
    "        grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    # train and predict on test instances using the best configs found in the CV step\n",
    "        \n",
    "    #predictions = grid.best_estimator_.predict(testX)                   # this is how to find the best estimator \n",
    "    #testX = grid.best_estimator_.named_steps['tfidf'].transform(testX)  # this is how to find indiv. components (same for pipeline)\n",
    "    predictions=grid.predict(testX)                                      # called on the best estimator by default\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} cross-validated F-1 score with grid search: {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    # return the best classifier to run it on the full dataset    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Running 7 options on the limited data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8339\n",
      "Confusion matrix:\n",
      "[[   0    0  922]\n",
      " [   0    0  497]\n",
      " [   0    0 7126]]\n",
      "\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultinomialNB()\n",
    "nb = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "        'vect__max_df':[0.25, 0.5,0.75],\n",
    "        'vect__min_df':[5,15,25,50,100],\n",
    "        'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "        'clf__alpha':[0.1,0.25,0.5,0.75,1.0]\n",
    "    }\n",
    "clf_simple(nb, data, labels)    # straightforward NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8339\n",
      "Confusion matrix:\n",
      "[[   0    0  922]\n",
      " [   0    0  497]\n",
      " [   0    0 7126]]\n",
      "\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_pipe(nb, data, labels)    # pipeline NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of MultinomialNB: 0.8443 +/- 0.0052\n",
      "Confusion matrix:\n",
      "[[   0    0  922]\n",
      " [   0    0  497]\n",
      " [   0    0 7126]]\n",
      "\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(nb, data, labels)    # cross_val_score NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.856629 using {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850396 (0.000679) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.855429 (0.000804) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856044 (0.000656) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.850894 (0.000760) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.856248 (0.000323) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.856629 (0.000215) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850338 (0.000573) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854346 (0.001198) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854610 (0.001054) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849899 (0.000687) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851801 (0.000392) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851830 (0.000433) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848143 (0.000687) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849489 (0.000341) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849548 (0.000330) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.850367 (0.000668) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.855341 (0.000805) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.855868 (0.000469) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.851011 (0.000753) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.856365 (0.000406) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.856599 (0.000247) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850279 (0.000647) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854610 (0.001054) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854551 (0.000732) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849870 (0.000949) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851684 (0.000475) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851889 (0.000576) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848085 (0.000474) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849343 (0.000558) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849402 (0.000619) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.850221 (0.000768) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.855254 (0.000805) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.855751 (0.000664) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.850835 (0.000651) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.856102 (0.000328) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.856629 (0.000285) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850133 (0.000538) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854405 (0.000777) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854551 (0.000732) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849753 (0.000970) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851567 (0.000446) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851772 (0.000663) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847968 (0.000565) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849285 (0.000542) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849343 (0.000510) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847500 (0.000403) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.849168 (0.000894) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.849431 (0.000661) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849197 (0.000446) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854288 (0.000875) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.854259 (0.000904) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849314 (0.000650) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852854 (0.000541) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852942 (0.000558) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849343 (0.000700) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850982 (0.000744) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851099 (0.000784) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847909 (0.000728) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849197 (0.000449) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849197 (0.000449) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847558 (0.000363) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.849051 (0.000865) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.849343 (0.000461) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849197 (0.000573) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854200 (0.000808) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.854200 (0.000808) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849431 (0.000538) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852913 (0.000508) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852913 (0.000397) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849226 (0.000865) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851069 (0.000765) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851157 (0.000836) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847939 (0.000576) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849138 (0.000538) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849197 (0.000540) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847500 (0.000356) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.848992 (0.000838) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.849285 (0.000568) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849109 (0.000540) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854025 (0.000678) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.854112 (0.000802) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849343 (0.000467) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852766 (0.000604) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852825 (0.000582) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849109 (0.000635) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850982 (0.000787) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850952 (0.000825) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847909 (0.000659) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848816 (0.000595) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849021 (0.000612) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845393 (0.000573) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845188 (0.000484) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845364 (0.000433) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847032 (0.000540) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.849782 (0.000930) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.849782 (0.000856) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847353 (0.000536) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849987 (0.000192) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850133 (0.000185) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.847851 (0.000476) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849489 (0.000598) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849577 (0.000598) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847471 (0.000475) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848407 (0.000446) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848465 (0.000476) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845422 (0.000495) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845188 (0.000539) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845276 (0.000416) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846973 (0.000526) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.849606 (0.000787) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.849606 (0.000755) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847236 (0.000427) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.850045 (0.000424) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850192 (0.000366) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.847939 (0.000337) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849577 (0.000673) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849606 (0.000639) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847558 (0.000607) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848319 (0.000499) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848290 (0.000526) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845276 (0.000454) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845101 (0.000416) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845218 (0.000391) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846915 (0.000515) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.849665 (0.000800) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.849548 (0.000678) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847119 (0.000363) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849753 (0.000274) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.849811 (0.000188) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.847763 (0.000324) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849489 (0.000783) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849519 (0.000763) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847412 (0.000360) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848290 (0.000516) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848290 (0.000516) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844398 (0.000213) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844428 (0.000204) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844369 (0.000185) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845627 (0.000389) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847295 (0.000701) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847236 (0.000595) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.846242 (0.000325) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.847997 (0.000824) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.847997 (0.000824) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.846885 (0.000543) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.848524 (0.000504) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.848582 (0.000536) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847178 (0.000377) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847880 (0.000493) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847880 (0.000493) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844369 (0.000252) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844398 (0.000226) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844398 (0.000226) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845686 (0.000369) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847207 (0.000701) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847236 (0.000660) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.846242 (0.000213) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.847822 (0.000760) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.847851 (0.000780) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.846856 (0.000605) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.848553 (0.000524) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.848582 (0.000633) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847090 (0.000377) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847734 (0.000437) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847734 (0.000437) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844369 (0.000242) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844398 (0.000226) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844398 (0.000226) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845744 (0.000337) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847149 (0.000671) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847149 (0.000620) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.846212 (0.000186) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.847792 (0.000801) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.847822 (0.000821) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.846885 (0.000524) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.848495 (0.000701) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.848524 (0.000673) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847002 (0.000430) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847617 (0.000369) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847675 (0.000392) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844252 (0.000174) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844193 (0.000066) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844193 (0.000066) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845101 (0.000226) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.845803 (0.000433) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.845773 (0.000324) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.845861 (0.000353) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.846681 (0.000644) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.846739 (0.000651) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.846300 (0.000353) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.847822 (0.000659) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.847851 (0.000640) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.846827 (0.000377) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847207 (0.000351) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847207 (0.000351) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844223 (0.000143) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844193 (0.000066) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844193 (0.000066) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845013 (0.000228) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.845891 (0.000433) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.845861 (0.000324) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.845861 (0.000322) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.846563 (0.000495) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.846505 (0.000449) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.846388 (0.000536) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.847792 (0.000701) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.847880 (0.000652) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.846681 (0.000394) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847090 (0.000375) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847119 (0.000350) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844223 (0.000143) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844193 (0.000066) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844193 (0.000066) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.844954 (0.000267) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.845803 (0.000318) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.845803 (0.000246) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.845803 (0.000401) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.846388 (0.000685) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.846388 (0.000618) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.846271 (0.000573) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.847734 (0.000691) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.847734 (0.000691) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.846651 (0.000358) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847061 (0.000328) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847090 (0.000369) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB cross-validated F-1 score with grid search: 0.8490\n",
      "Confusion matrix:\n",
      "[[  79   19  824]\n",
      " [  40   90  367]\n",
      " [  34    6 7086]]\n",
      "\n",
      "Wall time: 2h 16min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_NB = clf_GridSearchCV(nb, data, labels, nb_param_grid)    # parameter grid search NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC F-1 score:   0.8596\n",
      "Confusion matrix:\n",
      "[[ 209   77  636]\n",
      " [  92  200  205]\n",
      " [ 156   34 6936]]\n",
      "\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC()\n",
    "svc_param_grid = {\n",
    "    'vect__max_df':[0.25,0.5,0.75],\n",
    "    'vect__min_df':[5,15,25,50,100],\n",
    "    'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "    'clf__C':[0.1,0.25,0.5,0.75,1.0]\n",
    "}\n",
    "clf_pipe(svc, data, labels)            # pipeline SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of LinearSVC: 0.8668 +/- 0.0062\n",
      "Confusion matrix:\n",
      "[[ 209   77  636]\n",
      " [  92  200  205]\n",
      " [ 156   34 6936]]\n",
      "\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(svc, data, labels)             # cross_val_score SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.869474 using {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.858765 (0.001001) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857126 (0.000341) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856804 (0.000574) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.860140 (0.000981) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.859994 (0.000751) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.859818 (0.000874) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.860286 (0.000693) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.860813 (0.001169) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.860754 (0.001182) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.861339 (0.000744) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862100 (0.000919) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862217 (0.000844) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.859877 (0.000722) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.860725 (0.000295) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.860754 (0.000337) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.859613 (0.001035) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857009 (0.000195) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856834 (0.000143) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.860988 (0.000651) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.860081 (0.000830) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.860169 (0.000695) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.861398 (0.000781) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.860959 (0.001565) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.860871 (0.001632) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.861691 (0.001081) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862510 (0.001224) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862510 (0.001338) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.861047 (0.000941) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.860930 (0.000772) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.860901 (0.000855) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.859642 (0.000886) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.856834 (0.000067) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856804 (0.000148) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.861018 (0.000623) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.859935 (0.000998) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.859964 (0.000803) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.861574 (0.000760) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.861135 (0.001453) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.860959 (0.001453) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.861691 (0.001117) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862685 (0.001196) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862627 (0.001296) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.861047 (0.000639) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.860930 (0.000694) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.860988 (0.000585) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.865202 (0.000896) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.865290 (0.000425) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.865085 (0.000264) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866021 (0.001206) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866518 (0.001165) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.866752 (0.000812) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.866021 (0.001520) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867250 (0.000467) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867484 (0.000398) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866021 (0.001039) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867572 (0.001828) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867513 (0.001756) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863651 (0.001579) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864178 (0.001403) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864236 (0.001323) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866401 (0.001063) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.865319 (0.000614) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.865377 (0.000543) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867162 (0.000973) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866782 (0.001301) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.866987 (0.001192) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867074 (0.001535) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867455 (0.000849) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867308 (0.000764) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867221 (0.001393) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867952 (0.001477) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867835 (0.001463) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864207 (0.001329) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864646 (0.001482) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864675 (0.001478) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866460 (0.001288) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.865348 (0.000903) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.865524 (0.000764) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867308 (0.001094) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866782 (0.001292) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867045 (0.001219) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867367 (0.001555) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867221 (0.000722) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867250 (0.000513) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867016 (0.001182) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.868128 (0.001472) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867894 (0.001503) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864500 (0.001390) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864470 (0.001655) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864324 (0.001494) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866197 (0.000859) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868859 (0.001335) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868713 (0.000911) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866431 (0.001535) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.867484 (0.001848) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867425 (0.001719) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.865494 (0.001260) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867338 (0.000912) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867074 (0.001060) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866021 (0.000869) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867162 (0.001203) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867133 (0.001381) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864031 (0.001012) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864880 (0.001087) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864851 (0.001094) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866782 (0.000608) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869474 (0.000565) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869357 (0.000452) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867513 (0.000920) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.867367 (0.000963) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867630 (0.001272) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.866489 (0.000689) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867542 (0.001132) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867104 (0.000944) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866723 (0.000604) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867835 (0.000721) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867747 (0.000744) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864178 (0.001105) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864441 (0.001939) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864558 (0.001776) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867045 (0.000251) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869444 (0.000499) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869386 (0.000354) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867455 (0.000805) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.867308 (0.001191) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867660 (0.001333) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.866460 (0.000782) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867455 (0.001080) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867104 (0.000944) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867221 (0.000185) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867747 (0.001123) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867689 (0.000825) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863973 (0.001289) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864704 (0.001475) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864704 (0.001482) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864061 (0.001281) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868713 (0.000641) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868918 (0.001068) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.863563 (0.000851) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.864412 (0.001709) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.864558 (0.001759) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863358 (0.001666) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.864412 (0.000471) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864587 (0.000601) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.865172 (0.001262) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.864880 (0.001677) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.864792 (0.001437) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864207 (0.001225) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864353 (0.001237) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864353 (0.001237) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.865582 (0.000289) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868830 (0.000850) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868947 (0.000712) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.865026 (0.000864) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.864938 (0.001223) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.865260 (0.001595) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.864207 (0.001067) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.864324 (0.000866) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864558 (0.001123) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866109 (0.000679) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.864938 (0.001072) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.864675 (0.001003) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864617 (0.001028) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863914 (0.002176) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863827 (0.002129) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.865641 (0.000498) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868888 (0.000835) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868859 (0.000652) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.864412 (0.000367) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.865348 (0.001114) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.865524 (0.001542) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863739 (0.001076) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.864119 (0.000722) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864529 (0.001046) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866167 (0.000228) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.865114 (0.001191) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.864909 (0.001296) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864236 (0.001109) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863827 (0.001905) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863651 (0.001905) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.862334 (0.001418) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.866987 (0.000285) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867250 (0.000596) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.860637 (0.000533) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.861808 (0.000734) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.861925 (0.000844) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.861661 (0.001609) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.861252 (0.001092) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.861310 (0.001117) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864148 (0.001343) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863212 (0.001028) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863037 (0.001095) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863534 (0.001023) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863417 (0.001672) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863271 (0.001675) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.863651 (0.000204) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867045 (0.000389) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867338 (0.000088) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.861603 (0.000575) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.862773 (0.000804) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862364 (0.000805) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.862568 (0.001364) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.861983 (0.001147) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862217 (0.001203) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864880 (0.000467) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863271 (0.001223) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863154 (0.001365) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864353 (0.001391) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863329 (0.002481) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863329 (0.002567) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.863710 (0.000591) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867279 (0.000323) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867425 (0.000256) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.861632 (0.000515) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.862656 (0.000730) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862539 (0.000811) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.862568 (0.001288) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862012 (0.001178) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862364 (0.001431) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864734 (0.000805) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863680 (0.001068) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863300 (0.001475) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864236 (0.001591) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863007 (0.002168) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863007 (0.002215) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC cross-validated F-1 score with grid search: 0.8628\n",
      "Confusion matrix:\n",
      "[[ 183   61  678]\n",
      " [  81  181  235]\n",
      " [  92   25 7009]]\n",
      "\n",
      "Wall time: 3h 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_SVM = clf_GridSearchCV(svc, data, labels, svc_param_grid)    # parameter grid search SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Running Naive Bayes and SVM with the Best Parameters from Grid Search on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full dataset and labels\n",
    "full_data = df_text['reviewText'].values\n",
    "full_labels = df_text['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Naive Bayes F-1 score on full dataset: 0.8742\n",
      "Confusion matrix:\n",
      "[[  4420   1372  13506]\n",
      " [  1713   4814   5074]\n",
      " [  2626    425 162574]]\n",
      "\n",
      "The best SVM F-1 score on full dataset: 0.8833\n",
      "Confusion matrix:\n",
      "[[  4897   1909  12492]\n",
      " [  1941   6101   3559]\n",
      " [  2395    643 162587]]\n",
      "\n",
      "Wall time: 23min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the two best classifier on it\n",
    "for best_clf in [best_NB, best_SVM]:\n",
    "        \n",
    "    # split into train and test sets\n",
    "    trainX, testX, trainY, testY = train_test_split(full_data, full_labels, test_size = 0.2, random_state = 43)\n",
    "    clf = best_clf.fit(trainX, trainY)\n",
    "    \n",
    "    # predict and compute metrics\n",
    "    predictions = clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('The best {} F-1 score on full dataset: {:0.4f}'.format('Naive Bayes' if best_clf==best_NB else 'SVM', score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
