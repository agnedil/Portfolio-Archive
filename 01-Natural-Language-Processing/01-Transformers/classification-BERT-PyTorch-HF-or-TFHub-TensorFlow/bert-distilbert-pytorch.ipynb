{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH HUGGINGFACE TRANSFORMERS BERT\n",
    "Additional reference: https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "!pip install -U nltk\n",
    "!pip install ftfy\n",
    "!pip install pycld2\n",
    "!pip install emoji\n",
    "!pip install tqdm\n",
    "!pip install openpyxl\n",
    "!pip3 install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESTART KERNEL AFTER INSTALLING `IPYWIDGETS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 103\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0,500)\n",
    "#seed = 195\n",
    "random.seed( seed )\n",
    "np.random.seed( seed )\n",
    "torch.manual_seed( seed )\n",
    "torch.cuda.manual_seed_all( seed )\n",
    "print('Seed:', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,                          \n",
    "                          title='CONFUSION MATRIX',\n",
    "                          cmap=plt.cm.PuBu):               # originally plt.cm.Blues; also good: BuPu,RdPu,PuRd,OrRd,Oranges\n",
    "    '''\n",
    "    Plot the confusion matrix    \n",
    "    '''\n",
    "    plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "    plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "            \n",
    "    plt.figure(figsize=(5,5))\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.05)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True labels')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe( df1, df2, col_ ):\n",
    "    '''\n",
    "        df2 should not contain annything from df1 in column col_\n",
    "        (e.g. df2 = smaller test set OR smaller set for 1 category)\n",
    "        to preserve smaller df2, duplicates are deleted from df1\n",
    "    '''\n",
    "    original_length = df1.shape[0]\n",
    "    df2_sents = df2[col_].values\n",
    "    df1 = df1[ ~df1[col_].isin(df2_sents) ]\n",
    "    print( f'\\tDropping {original_length - df1.shape[0]} duplicates')\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def upsample( df_, to_oversample_ ):\n",
    "    '''\n",
    "        Upsample df_ by to_oversample_ more samples excluding re-evaluation data\n",
    "    '''    \n",
    "    # EXCLUDE RE-EVALUATED DATA FROM OVERSMPLING IF IT'S PRESENT    \n",
    "    if 'source' in df_.columns:\n",
    "        df_to_oversample = df_[ df_['source'] != 'reeval_2021' ]\n",
    "        print( '\\tData shape for this category without re-eval:', df_to_oversample.shape )\n",
    "    else:\n",
    "        df_to_oversample = df_\n",
    "        \n",
    "    # OVERSAMPLE AND CONCAT W/ORIGINAL DF_\n",
    "    replace = False\n",
    "    if len(df_to_oversample) < to_oversample_:\n",
    "        replace = True        \n",
    "\n",
    "    df_upsampled = df_to_oversample.sample( n=to_oversample_, replace=replace )\n",
    "    df_          = pd.concat([ df_, df_upsampled ])\n",
    "        \n",
    "    return df_.sample( frac=1 )\n",
    "\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return ( round( f1_score(labels_flat, preds_flat, average='micro'), 4 ),\n",
    "             round( f1_score(labels_flat, preds_flat, average='macro'), 4 ),\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ''\n",
    "df   = pd.read_csv( file , sep='\\t', encoding='utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_categories = [ ]\n",
    "#df = df[ df['is_subtle'] == 0 ]\n",
    "df = df[ df['label'].isin( ml_categories ) ]\n",
    "df['target'] = df['label'].apply( lambda x: 0 if x == 'unk' else 1 )\n",
    "df = df.drop(['award_reason', 'is_group_award', 'group_award_id',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10843, 12)\n",
      "\tDropping 6 duplicates\n",
      "(10837, 12)\n"
     ]
    }
   ],
   "source": [
    "# DEDUPE BETWEEN CATEGORIES. FAVOR CATEGORY 1\n",
    "print(df.shape)\n",
    "df1 = df[ df['target'] == 1 ].copy()\n",
    "df0 = df[ df['target'] == 0 ].copy()\n",
    "df0, df1 = dedupe( df0, df1, 'sentence' )\n",
    "df = pd.concat([ df0, df1 ]).copy().sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEDUPE TRAIN / VAL / TEST SETS. FAVOR TEST, THEN VAL SET - better results if not doing it, but it's not right\n",
    "print(df.shape)\n",
    "df_train = df[ df['subset'] == 'train' ].copy()\n",
    "df_val   = df[ df['subset'] == 'val' ].copy()\n",
    "df_test  = df[ df['subset'] == 'test' ].copy()\n",
    "\n",
    "df_train, df_test = dedupe( df_train, df_test, 'sentence' )\n",
    "df_val, df_test   = dedupe( df_val, df_test, 'sentence' )\n",
    "df_train, df_val  = dedupe( df_train, df_val, 'sentence' )\n",
    "df = pd.concat([ df_train, df_val, df_test ]).copy().sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INCORRECT WAY TO ESTIMATE MAXLEN - USE BERT TOKENIZER\n",
    "df['length'] = df['sentence'].apply( lambda x: len(x.split()) )\n",
    "maxlen = df['length'].max()\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IF TRAINING ON FULL DATASET\n",
    "X_train = df['sentence'].values\n",
    "y_train = df['target'].values\n",
    "X_val   = df[ df['subset'] == 'val' ]['sentence'].values\n",
    "y_val   = df[ df['subset'] == 'val' ]['target'].values\n",
    "X_test  = df[ df['subset'] == 'test' ]['sentence'].values\n",
    "y_test  = df[ df['subset'] == 'test' ]['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[ df['subset'] == 'train' ]['sentence'].values\n",
    "y_train = df[ df['subset'] == 'train' ]['target'].values\n",
    "X_val   = df[ df['subset'] == 'val' ]['sentence'].values\n",
    "y_val   = df[ df['subset'] == 'val' ]['target'].values\n",
    "X_test  = df[ df['subset'] == 'test' ]['sentence'].values\n",
    "y_test  = df[ df['subset'] == 'test' ]['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24476,) (24476,) (2794,) (2794,) (918,) (918,)\n"
     ]
    }
   ],
   "source": [
    "print( X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader( X, y, tokenizer, batch_size, maxlen ):\n",
    "    \n",
    "    # `batch_encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_data = tokenizer.batch_encode_plus( X, \n",
    "                                                add_special_tokens    = True, \n",
    "                                                return_attention_mask = True, \n",
    "                                                pad_to_max_length     = True, \n",
    "                                                max_length            = maxlen,\n",
    "                                                return_tensors        = 'pt',\n",
    "                                              )\n",
    "    \n",
    "    input_ids       = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels          = torch.tensor( y )\n",
    "    \n",
    "    dataset         = TensorDataset( input_ids, attention_masks, labels )\n",
    "    dataloader      = DataLoader(    dataset, \n",
    "                                     sampler    = RandomSampler( dataset ), \n",
    "                                     batch_size = batch_size,\n",
    "                                 )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate( dataloader ):\n",
    "\n",
    "    # put model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    preds, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # add batch to device (GPU)\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # unpack inputs from dataloader\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        # tell the model not to compute gradients => save memory, speed up prediction\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        loss_val_total += loss.item()\n",
    "        logits = outputs[1]  \n",
    "\n",
    "        # move logits, labels to CPU (logits = raw classifier output)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "                \n",
    "        preds.append( logits )\n",
    "        true_vals.append( label_ids )\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader) \n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, preds, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT HAS A LOT MORE TOKENS IN A SENTENCE THAN ACCORDING TO PYTHON'S SENTENCES.SPLIT()!!!!\n",
    "* maxlen per split() - 99\n",
    "* maxlen per Bert    - 152!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAXLEN BASED ON BERT TOKENIZATION\n",
    "#data_tokenized = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(i)) for i in X_train]\n",
    "#mlen = max([len(i) for i in data_tokenized])\n",
    "#mlen\n",
    "# OUTPUT FOR SUMMER 2021 DATASET - 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp:20211010T0608\n",
      "LR=5e-05, batch_size=16, classifier_dropout=0.0\n",
      "epochs=5, maxlen=200, seed=103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e219633e4e2e4803b9f7c3e69b54fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1acaef9613a406fa9c0eb8aa1f19e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e754a781b9ae4ca7b21e8e6376068952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a2d8740325492794aff87afe4984b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=483.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384a8a5feab84a08bac1ee80b982e03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267967963.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137f922a51454c2e8fc11a9a9d2d7e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=1530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-10-10 06:09:26.095 pytorch-1-6-gpu-py3-ml-g4dn-xlarge-cfec521e9f0eef638bc93c1751d2:2073 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-10-10 06:09:26.129 pytorch-1-6-gpu-py3-ml-g4dn-xlarge-cfec521e9f0eef638bc93c1751d2:2073 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\n",
      "Epoch 1\n",
      "Training loss: 0.3389\n",
      "Validation loss: 0.6347\n",
      "F1 Score (micro): 0.7856\n",
      "F1 Score (macro): 0.7763\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=1530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.1217\n",
      "Validation loss: 0.8273\n",
      "F1 Score (micro): 0.8017\n",
      "F1 Score (macro): 0.7954\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=1530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.0473\n",
      "Validation loss: 1.1179\n",
      "F1 Score (micro): 0.8128\n",
      "F1 Score (macro): 0.8063\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 4'), FloatProgress(value=0.0, max=1530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.0239\n",
      "Validation loss: 1.3356\n",
      "F1 Score (micro): 0.7989\n",
      "F1 Score (macro): 0.7952\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 5'), FloatProgress(value=0.0, max=1530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.007\n",
      "Validation loss: 1.5414\n",
      "F1 Score (micro): 0.7931\n",
      "F1 Score (macro): 0.7872\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs         = 5\n",
    "maxlen         = maxlen\n",
    "learning_rates = [5e-5]\n",
    "batch_sizes    = [16]                      # [ 32, 16, 8, ]\n",
    "douts          = [0.0]                     # 0.4, 0.5    Done: 0.1, 0.25, \n",
    "\n",
    "all_combinations = list(itertools.product( *[learning_rates, batch_sizes, douts] ))\n",
    "\n",
    "time_stamp1 = time.strftime(\"%Y%m%dT%H%M\") \n",
    "file_name   = f'logs/log_{time_stamp1}.txt'\n",
    "wdir        = 'ckpts/current/'\n",
    "\n",
    "with open( file_name, 'w', encoding='utf-8' ) as f:\n",
    "    experiment_name = 'BERT PYTORCH\\n'\n",
    "    f.write( experiment_name )\n",
    "    for LR, batch_size, dout in all_combinations:\n",
    "\n",
    "        time_stamp = time.strftime(\"%Y%m%dT%H%M\") \n",
    "        params1    = f'LR={LR}, batch_size={batch_size}, classifier_dropout={dout}'\n",
    "        params2    = f'epochs={epochs}, maxlen={maxlen}, seed={seed}'\n",
    "        print( 'Timestamp:', time_stamp, '\\n', params1, '\\n', params2, sep='')\n",
    "        f.write( '\\nTimestamp: ' + time_stamp + '\\n' + params1 + '\\n' + params2 + '\\n' )\n",
    "        \n",
    "        tokenizer  = DistilBertTokenizer.from_pretrained( 'distilbert-base-uncased', \n",
    "                                                           do_lower_case=True,\n",
    "                                                           padding_side = 'right',\n",
    "                                                        )\n",
    "        dataloader_train = get_dataloader( X_train, y_train, tokenizer, batch_size, maxlen )\n",
    "        dataloader_val   = get_dataloader( X_val, y_val, tokenizer, batch_size, maxlen )\n",
    "        dataloader_test  = get_dataloader( X_test, y_test, tokenizer, batch_size, maxlen )\n",
    "\n",
    "        model = DistilBertForSequenceClassification.from_pretrained( 'distilbert-base-uncased',\n",
    "                                                                      num_labels=2,\n",
    "                                                                      output_attentions=False,\n",
    "                                                                      output_hidden_states=False,\n",
    "                                                                      #classifier_dropout = dout,\n",
    "                                                                      #attention_probs_dropout_prob=dout,\n",
    "                                                                      #hidden_dropout_prob=dout,\n",
    "                                                                   )\n",
    "        optimizer = AdamW( model.parameters(),\n",
    "                           lr=LR,                 # 1e-5\n",
    "                           eps=1e-8,              # very small number to avoid division by 0\n",
    "                         )             \n",
    "        \n",
    "        # Note: len(dataloader_train) = len(X_train) / batch_size\n",
    "        # in case of augmented / oversampled data, len(X_train) == 24459, steps = 1529\n",
    "        scheduler = get_linear_schedule_with_warmup( optimizer, \n",
    "                                                     num_warmup_steps=500,\n",
    "                                                     num_training_steps=len(dataloader_train)*epochs,\n",
    "                                                   )\n",
    "        model.to(device)\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "            model.train()\n",
    "            loss_train_total = 0\n",
    "            progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "                        \n",
    "            for batch in progress_bar:\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch = tuple(b.to(device) for b in batch)\n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'labels':         batch[2],\n",
    "                         }       \n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[0]\n",
    "                loss_train_total += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "            tqdm.write( f'\\nEpoch {epoch}' )\n",
    "            f.write( f'\\nEpoch {epoch}' + '\\n' )\n",
    "\n",
    "            loss_train_avg = round( loss_train_total/len(dataloader_train), 4 )\n",
    "            val_loss, preds, y_val = evaluate( dataloader_val )\n",
    "            val_loss = round( val_loss, 4 )\n",
    "            val_f1 = f1_score_func( preds, y_val )\n",
    "            \n",
    "            metrics = f'Training loss: {loss_train_avg}\\n' + f'Validation loss: {val_loss}\\n' +\\\n",
    "                      f'F1 Score (micro): {val_f1[0]}\\n' + f'F1 Score (macro): {val_f1[1]}\\n'\n",
    "            \n",
    "            tqdm.write( metrics )\n",
    "            f.write( metrics + '\\n')\n",
    "            \n",
    "            filepath = wdir + time_stamp + f'-epoch_{epoch}-val_loss_{val_loss}-f1micro_{val_f1[0]}-f1macro{val_f1[1]}.model'\n",
    "            torch.save(model.state_dict(), filepath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: saved models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen     = maxlen\n",
    "batch_size = 16\n",
    "tokenizer  = DistilBertTokenizer.from_pretrained( 'distilbert-base-uncased', \n",
    "                                                   do_lower_case=True,\n",
    "                                                   padding_side = 'right',\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211010T0608-epoch_1-val_loss_0.6347-f1micro_0.7856-f1macro0.7763.model \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7827    0.8289    0.8051       491\n",
      "           1     0.7889    0.7354    0.7612       427\n",
      "\n",
      "    accuracy                         0.7854       918\n",
      "   macro avg     0.7858    0.7821    0.7832       918\n",
      "weighted avg     0.7856    0.7854    0.7847       918\n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211010T0608-epoch_2-val_loss_0.8273-f1micro_0.8017-f1macro0.7954.model \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8012    0.7882    0.7947       491\n",
      "           1     0.7609    0.7752    0.7680       427\n",
      "\n",
      "    accuracy                         0.7821       918\n",
      "   macro avg     0.7811    0.7817    0.7813       918\n",
      "weighted avg     0.7825    0.7821    0.7823       918\n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211010T0608-epoch_3-val_loss_1.1179-f1micro_0.8128-f1macro0.8063.model \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7947    0.7963    0.7955       491\n",
      "           1     0.7653    0.7635    0.7644       427\n",
      "\n",
      "    accuracy                         0.7810       918\n",
      "   macro avg     0.7800    0.7799    0.7799       918\n",
      "weighted avg     0.7810    0.7810    0.7810       918\n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211010T0608-epoch_4-val_loss_1.3356-f1micro_0.7989-f1macro0.7952.model \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8188    0.7821    0.8000       491\n",
      "           1     0.7617    0.8009    0.7808       427\n",
      "\n",
      "    accuracy                         0.7908       918\n",
      "   macro avg     0.7902    0.7915    0.7904       918\n",
      "weighted avg     0.7922    0.7908    0.7911       918\n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211010T0608-epoch_5-val_loss_1.5414-f1micro_0.7931-f1macro0.7872.model \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8151    0.7902    0.8025       491\n",
      "           1     0.7670    0.7939    0.7802       427\n",
      "\n",
      "    accuracy                         0.7919       918\n",
      "   macro avg     0.7910    0.7921    0.7913       918\n",
      "weighted avg     0.7927    0.7919    0.7921       918\n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdir = 'ckpts/current/'\n",
    "res  = []\n",
    "\n",
    "dataloader_test = get_dataloader( X_test, y_test, tokenizer, batch_size, maxlen )\n",
    "for path, directories, files in os.walk( wdir ):\n",
    "    for file in sorted(files):\n",
    "\n",
    "        try:\n",
    "            if not file.endswith( '.model' ):\n",
    "                continue\n",
    "\n",
    "            model = DistilBertForSequenceClassification.from_pretrained( 'distilbert-base-uncased',\n",
    "                                                                         num_labels=2,\n",
    "                                                                         output_attentions=False,\n",
    "                                                                         output_hidden_states=False,\n",
    "                                                                         #classifier_dropout = dout,\n",
    "                                                                         #attention_probs_dropout_prob=dout,\n",
    "                                                                         #hidden_dropout_prob=dout,\n",
    "                                                                       )\n",
    "\n",
    "            model.to(device)\n",
    "            model.load_state_dict( torch.load( wdir + file, map_location=torch.device('cpu') ) )\n",
    "            _, preds, y_test = evaluate( dataloader_test )\n",
    "\n",
    "            preds_flat  = np.argmax( preds, axis=1 ).flatten()\n",
    "            y_test_flat = y_test.flatten()\n",
    "            clf_report = classification_report( y_test_flat, preds_flat, digits=4 )\n",
    "            print( file, '\\n', clf_report )            \n",
    "            res.append((file, clf_report))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                        \n",
    "        print( '='*50, '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
