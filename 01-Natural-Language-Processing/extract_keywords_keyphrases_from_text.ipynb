{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT KEYWORDS AND KEYPHRASES FROM TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import spacy   \n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import digits, punctuation\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyphrases(nlp, text_, add_keywords = False):\n",
    "    \n",
    "    '''\n",
    "    Get keyphrases as noun & verb phrases based on a limited set of POS tags\n",
    "        Arguments:\n",
    "            nlp          - loaded Spacy model\n",
    "            text_        - text as one string to have keywords extracted from\n",
    "            add_keywords - if single keywords need to be added\n",
    "        Returns:\n",
    "            result - list of all keywords / keyphrases with repetitions\n",
    "    '''    \n",
    "    \n",
    "    # POS tags allowed in keywords / keyphrases    \n",
    "    tag_set = ['PROPN','NOUN','ADJ', 'ADV']\n",
    "\n",
    "    # create a spacy doc object\n",
    "    doc = nlp(text_.lower())\n",
    "        \n",
    "    result = []\n",
    "        \n",
    "    # get noun chunks and remove irrelevant POS tags (articles etc.)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        final_chunk = ''\n",
    "        for token in chunk:\n",
    "            if token.pos_ in tag_set and not token.text in nlp.Defaults.stop_words and not token.text in punctuation:\n",
    "                final_chunk =  final_chunk + token.text + \" \"\n",
    "        if final_chunk:\n",
    "            result.append(final_chunk.strip())    \n",
    "    count = len(result)\n",
    "    print('* * * * Discovered {} noun phrases * * * *'.format(count))\n",
    "\n",
    "    # get verb chunks    \n",
    "    # instantiate a Matcher instance\n",
    "    pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "               {'POS': 'ADV', 'OP': '*'},                   \n",
    "               {'POS': 'VERB', 'OP': '+'}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('Verb phrases', None, pattern)\n",
    "    \n",
    "    # find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "    spans = filter_spans(spans)\n",
    "    for item in spans:\n",
    "        final_token = ''\n",
    "        for token in item:\n",
    "            if not token.text in nlp.Defaults.stop_words and not token.text in punctuation:\n",
    "                final_token = final_token + token.text + ' '\n",
    "        if final_token:\n",
    "            result.append(final_token.strip())\n",
    "    count = len(result) - count\n",
    "    print('* * * * Discovered {} verb phrases * * * *'.format(count))\n",
    "\n",
    "    # get keywords if needed\n",
    "    if add_keywords:\n",
    "        for token in doc:\n",
    "            if (token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "                continue\n",
    "            if (token.pos_ in tag_set):\n",
    "                result.append(token.text)\n",
    "        count = len(result) - count\n",
    "        print('* * * * Discovered {} keywords * * * *'.format(count))\n",
    "        print('* * * * Total number of keyphrases and keywords - {} * * * *'.format(len(result)))\n",
    "        print('* * * * Total number of unique keyphrases and keywords - {} * * * *'.format(len(set(result))))\n",
    "        \n",
    "    if not add_keywords:\n",
    "        print('* * * * Total number of keyphrases - {} * * * *'.format(len(result)))\n",
    "        print('* * * * Total number of unique keyphrases - {} * * * *'.format(len(set(result))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TEXT\n",
    "top_dir = 'take_or_pay'\n",
    "top_file = 'top_clauses.txt'\n",
    "full_path = os.path.join(top_dir, top_file)\n",
    "with open(full_path) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * Discovered 1969 noun phrases * * * *\n",
      "* * * * Discovered 878 verb phrases * * * *\n",
      "* * * * Total number of keyphrases - 2847 * * * *\n",
      "* * * * Total number of unique keyphrases - 1316 * * * *\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT KEYWORDS AND KEYPHRASES\n",
    "all_words = extract_keyphrases(nlp, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries - 1316\n",
      "('seller', 105)\n",
      "('buyer', 104)\n",
      "('pay', 77)\n",
      "('quantity', 34)\n",
      "('contract', 32)\n",
      "('delivery', 28)\n",
      "('agreement', 22)\n",
      "('supplier', 22)\n",
      "('parties', 21)\n",
      "('pay clauses', 21)\n",
      "('commodity', 19)\n",
      "('taken', 19)\n",
      "('obligation', 16)\n",
      "('clauses', 15)\n",
      "('gas', 14)\n",
      "('payment', 14)\n",
      "('pay clause', 14)\n",
      "('breach', 14)\n",
      "('buyers', 14)\n",
      "('clause', 14)\n",
      "('required', 14)\n",
      "('damages', 12)\n",
      "('agreed', 12)\n",
      "('price', 11)\n",
      "('quantities', 11)\n",
      "('year', 10)\n",
      "('default', 10)\n",
      "('cases', 10)\n",
      "('article', 10)\n",
      "('sellers', 10)\n",
      "('depa', 10)\n",
      "('courts', 9)\n",
      "('failure', 9)\n",
      "('losses', 9)\n",
      "('suppliers', 9)\n",
      "('terms', 9)\n",
      "('claim', 9)\n",
      "('deliver', 9)\n",
      "('fails', 9)\n",
      "('right', 8)\n",
      "('contracts', 8)\n",
      "('pay contract', 8)\n",
      "('obligations', 8)\n",
      "('order', 8)\n",
      "('loss', 8)\n",
      "('profit', 8)\n",
      "('customers', 8)\n",
      "('provide', 8)\n",
      "('provided', 8)\n",
      "('lng', 7)\n",
      "('event', 7)\n",
      "('account', 7)\n",
      "('section', 7)\n",
      "('party', 7)\n",
      "('natural gas', 7)\n",
      "('offtake', 7)\n",
      "('customer', 7)\n",
      "('shall', 7)\n",
      "('given', 7)\n",
      "('reduced', 7)\n",
      "('receive', 7)\n",
      "('sale', 6)\n",
      "('provisions', 6)\n",
      "('banks', 6)\n",
      "('fact', 6)\n",
      "('type', 6)\n",
      "('deficiency quantity', 6)\n",
      "('force majeure', 6)\n",
      "('tender', 6)\n",
      "('natural gas quantities', 6)\n",
      "('case', 6)\n",
      "('lng cargo', 6)\n",
      "('supply', 5)\n",
      "('goods', 5)\n",
      "('borrowers', 5)\n",
      "('difference', 5)\n",
      "('contract price', 5)\n",
      "('calendar year', 5)\n",
      "('use', 5)\n",
      "('pay contracts', 5)\n",
      "('performance', 5)\n",
      "('nature', 5)\n",
      "('demand', 5)\n",
      "('producers', 5)\n",
      "('claims', 5)\n",
      "('injured party', 5)\n",
      "('reasonable endeavours', 5)\n",
      "('contracted', 5)\n",
      "('entitled', 5)\n",
      "('making', 5)\n",
      "('called', 5)\n",
      "('volume', 4)\n",
      "('prices', 4)\n",
      "('form', 4)\n",
      "('amounts', 4)\n",
      "('minimum contract quantity', 4)\n",
      "('deliveries', 4)\n",
      "('penalty', 4)\n",
      "('start', 4)\n",
      "('practice', 4)\n"
     ]
    }
   ],
   "source": [
    "# GET FREQUENCIES\n",
    "c = Counter(all_words)\n",
    "res = c.most_common()\n",
    "print('Total number of entries -', len(res))\n",
    "for item in res[:100]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE TO FILE IN THE ORDER OF DECREASING FREQUENCY\n",
    "to_save = [item[0] for item in res]\n",
    "with open('keyphrases_extracted_take_or_pay.txt', 'w', encoding='utf8') as f:\n",
    "    for item in to_save:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    s = s.replace('-', ' ')\n",
    "    return s.strip().lower().translate(str.maketrans('', '', punctuation)).translate(str.maketrans('', '', digits)).split()\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    #tokens = [t for t in tokens if t not in stop_words]\n",
    "    #tokens = [spanish_stemmer.stem(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if len(t) > 1]    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
