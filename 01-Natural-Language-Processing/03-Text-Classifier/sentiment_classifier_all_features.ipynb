{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7. ReviewText + ALL meaningful features\n",
    "It was established during several previous runs of \"reveiwText + x\" (where x is each single column from dataset) that most of the additional features improve the score except for the length of the review. Now, let us now add ALL of the contributing features to the classifier and see what the score is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\n",
      "Scikit-learn 0.20.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "#nltk.download('stopwords')    # this is done just once\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from platform import python_version\n",
    "print('Python {}'.format(python_version()))\n",
    "print('Scikit-learn {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in json, add ALL meaningful features to reviewText, get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 982619\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path=''\n",
    "file='kindle_reviews.json'\n",
    "df = pd.read_json(path_or_buf=path+file, lines=True, encoding='utf-8')    #, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, chunksize=None, compression='infer')\n",
    "print('Length of text: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1F6404F1VG29J</td>\n",
       "      <td>Avidreader</td>\n",
       "      <td>Nice vintage story</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "      <td>01 6, 2014</td>\n",
       "      <td>AN0N05A9LIJEQ</td>\n",
       "      <td>critters</td>\n",
       "      <td>Different...</td>\n",
       "      <td>1388966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "      <td>04 4, 2014</td>\n",
       "      <td>A795DMNCJILA6</td>\n",
       "      <td>dot</td>\n",
       "      <td>Oldie</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>A1FV0SX13TWVXQ</td>\n",
       "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
       "      <td>I really liked it.</td>\n",
       "      <td>1392768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "      <td>03 19, 2014</td>\n",
       "      <td>A3SPTOKDG7WBLN</td>\n",
       "      <td>Father Dowling Fan</td>\n",
       "      <td>Period Mystery</td>\n",
       "      <td>1395187200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  B000F83SZQ  [0, 0]        5   \n",
       "1  B000F83SZQ  [2, 2]        4   \n",
       "2  B000F83SZQ  [2, 2]        4   \n",
       "3  B000F83SZQ  [1, 1]        5   \n",
       "4  B000F83SZQ  [0, 1]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I enjoy vintage books and movies so I enjoyed ...   05 5, 2014   \n",
       "1  This book is a reissue of an old one; the auth...   01 6, 2014   \n",
       "2  This was a fairly interesting read.  It had ol...   04 4, 2014   \n",
       "3  I'd never read any of the Amy Brewster mysteri...  02 19, 2014   \n",
       "4  If you like period pieces - clothing, lingo, y...  03 19, 2014   \n",
       "\n",
       "       reviewerID                         reviewerName             summary  \\\n",
       "0  A1F6404F1VG29J                           Avidreader  Nice vintage story   \n",
       "1   AN0N05A9LIJEQ                             critters        Different...   \n",
       "2   A795DMNCJILA6                                  dot               Oldie   \n",
       "3  A1FV0SX13TWVXQ  Elaine H. Turley \"Montana Songbird\"  I really liked it.   \n",
       "4  A3SPTOKDG7WBLN                   Father Dowling Fan      Period Mystery   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1399248000  \n",
       "1      1388966400  \n",
       "2      1396569600  \n",
       "3      1392768000  \n",
       "4      1395187200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the ALL meaningful features to reviewText\n",
    "df['reviewText'] = df['asin'].map(str) + ' ' + df['reviewerID'].map(str) + ' ' + df['reviewerName'].map(str) + ' ' + df['summary'].map(str) + ' ' + df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['reviewText', 'overall']].copy()        # copy only certain columns to another df\n",
    "df = None                                             # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ A1F6404F1VG29J Avidreader Nice vint...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ AN0N05A9LIJEQ critters Different......</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ A795DMNCJILA6 dot Oldie This was a ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ A1FV0SX13TWVXQ Elaine H. Turley \"Mo...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ A3SPTOKDG7WBLN Father Dowling Fan P...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  B000F83SZQ A1F6404F1VG29J Avidreader Nice vint...        5\n",
       "1  B000F83SZQ AN0N05A9LIJEQ critters Different......        4\n",
       "2  B000F83SZQ A795DMNCJILA6 dot Oldie This was a ...        4\n",
       "3  B000F83SZQ A1FV0SX13TWVXQ Elaine H. Turley \"Mo...        5\n",
       "4  B000F83SZQ A3SPTOKDG7WBLN Father Dowling Fan P...        4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"B000F83SZQ A1F6404F1VG29J Avidreader Nice vintage story I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone in the jaw to knock them out would wash today.Still it was a good read for me.\",\n",
       "        5],\n",
       "       [\"B000F83SZQ AN0N05A9LIJEQ critters Different... This book is a reissue of an old one; the author was born in 1910. It's of the era of, say, Nero Wolfe. The introduction was quite interesting, explaining who the author was and why he's been forgotten; I'd never heard of him.The language is a little dated at times, like calling a gun a &#34;heater.&#34;  I also made good use of my Fire's dictionary to look up words like &#34;deshabille&#34; and &#34;Canarsie.&#34; Still, it was well worth a look-see.\",\n",
       "        4],\n",
       "       [\"B000F83SZQ A795DMNCJILA6 dot Oldie This was a fairly interesting read.  It had old- style terminology.I was glad to get  to read a story that doesn't have coarse, crasslanguage.  I read for fun and relaxation......I like the free ebooksbecause I can check out a writer and decide if they are intriguing,innovative, and have enough of the command of Englishthat they can convey the story without crude language.\",\n",
       "        4],\n",
       "       ['B000F83SZQ A1FV0SX13TWVXQ Elaine H. Turley \"Montana Songbird\" I really liked it. I\\'d never read any of the Amy Brewster mysteries until this one..  So I am really hooked on them now.',\n",
       "        5],\n",
       "       ['B000F83SZQ A3SPTOKDG7WBLN Father Dowling Fan Period Mystery If you like period pieces - clothing, lingo, you will enjoy this mystery.  Author had me guessing at least 2/3 of the way through.',\n",
       "        4]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert 5 ratings to 3 ('neg', 'mixed', and 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 5 ratings to 3 ('neg', 'mixed', and 'pos')\n",
    "df_text['overall'] = df_text['overall'].apply(lambda x: 'pos' if x > 3 else 'neg' if x < 3 else 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ A1F6404F1VG29J Avidreader Nice vint...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ AN0N05A9LIJEQ critters Different......</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ A795DMNCJILA6 dot Oldie This was a ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ A1FV0SX13TWVXQ Elaine H. Turley \"Mo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ A3SPTOKDG7WBLN Father Dowling Fan P...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText overall\n",
       "0  B000F83SZQ A1F6404F1VG29J Avidreader Nice vint...     pos\n",
       "1  B000F83SZQ AN0N05A9LIJEQ critters Different......     pos\n",
       "2  B000F83SZQ A795DMNCJILA6 dot Oldie This was a ...     pos\n",
       "3  B000F83SZQ A1FV0SX13TWVXQ Elaine H. Turley \"Mo...     pos\n",
       "4  B000F83SZQ A3SPTOKDG7WBLN Father Dowling Fan P...     pos"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the dataset is 982619\n"
     ]
    }
   ],
   "source": [
    "print('Total length of the dataset is {}'.format(len(df_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reduce size of data, get reduced-size dataset and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sample is 42722\n"
     ]
    }
   ],
   "source": [
    "# get a random sample from the dataframe whose size is manageable for cross-validation and grid search\n",
    "# with more computing resources and/or time, this can be done on a larger data set\n",
    "length = len(df_text)\n",
    "df_text = df_text.sample(n=length)\n",
    "df_text_short = df_text.sample(n=int(length/23))\n",
    "print('Length of the sample is {}'.format(len(df_text_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos      35989\n",
       "mixed     4230\n",
       "neg       2503\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique labels\n",
    "df_text_short.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our train set and labels\n",
    "data = df_text_short['reviewText'].values\n",
    "labels = df_text_short['overall'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Experimenting with different lists of stopwords and selecting the most efficient one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['noone', 'their', 'hereupon', 'thus', 'nowhere', 'her', 'whole', 'mine', 'etc', 'per', 'down', 'eleven', 'move', 'almost', 'same', 'might', 'former', 'above', 'amount', 'cannot', 'often', 'sometimes', 'ltd', 'twenty', 'onto', 'again', 'indeed', 'been', 'whence', 'therein', 'thick', 'up', 'most', 'side', 'whom', 'can', 'whereafter', 'done', 'hasnt', 'thereby', 'afterwards', 'with', 'take', 'in', 'due', 'as', 'enough', 'hereafter', 'yet', 'whereby', 'yourself', 'interest', 'being', 'somewhere', 'he', 'mill', 'but', 'thence', 'other', 'fire', 'few', 'me', 'get', 'give', 'by', 'others', 'more', 'nor', 'five', 'together', 'amongst', 'namely', 'last', 'during', 'while', 'itself', 'anything', 'empty', 'ie', 'found', 'moreover', 'were', 'everything', 'him', 'still', 'cant', 'thereupon', 'must', 'for', 'too', 'anyhow', 'only', 'was', 'although', 'wherever', 'seemed', 'third', 'un', 'whether', 'those', 'your', 'everyone', 'serious', 'to', 'under', 'whereupon', 'on', 'you', 'show', 'six', 'one', 'across', 'always', 'none', 'off', 'already', 'she', 'against', 'sixty', 'own', 'why', 'seeming', 'co', 'or', 'another', 'herein', 'except', 'himself', 'name', 'forty', 'at', 'myself', 'becoming', 'else', 'nobody', 'something', 'after', 'out', 'should', 'toward', 'much', 'part', 'i', 'several', 'whenever', 'mostly', 'because', 'until', 'if', 'thin', 'ours', 'us', 'which', 'could', 'also', 'here', 'fifteen', 'perhaps', 'and', 'become', 'bottom', 'of', 'meanwhile', 'herself', 'cry', 'would', 'beside', 'besides', 'someone', 'fifty', 'whatever', 'anyone', 'go', 'though', 'whither', 'formerly', 'a', 're', 'amoungst', 'are', 'anyway', 'his', 'had', 'hundred', 'within', 'sometime', 'without', 'thru', 'these', 'however', 'bill', 'everywhere', 'will', 'seems', 'what', 'below', 'became', 'never', 'them', 'hereby', 'many', 'see', 'eg', 'nothing', 'top', 'wherein', 'further', 'inc', 'put', 'whoever', 'call', 'some', 'detail', 'next', 'it', 'please', 'somehow', 'once', 'beforehand', 'even', 'first', 'latterly', 'themselves', 'my', 'we', 'hers', 'how', 'there', 'well', 'twelve', 'do', 'both', 'our', 'latter', 'am', 'full', 'between', 'so', 'alone', 'least', 'every', 'towards', 'be', 'via', 'anywhere', 'couldnt', 'very', 'made', 'de', 'is', 'elsewhere', 'about', 'no', 'four', 'any', 'each', 'may', 'not', 'into', 'among', 'an', 'around', 'hence', 'over', 'they', 'beyond', 'three', 'throughout', 'upon', 'since', 'whereas', 'con', 'seem', 'now', 'describe', 'whose', 'two', 'that', 'front', 'this', 'ourselves', 'through', 'than', 'has', 'rather', 'when', 'then', 'keep', 'eight', 'yourselves', 'sincere', 'from', 'otherwise', 'fill', 'less', 'have', 'therefore', 'who', 'nevertheless', 'ever', 'back', 'where', 'either', 'before', 'its', 'such', 'system', 'all', 'becomes', 'the', 'find', 'along', 'nine', 'neither', 'thereafter', 'ten', 'yours', 'behind']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "153\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['noone', 'mrs', 'thus', 'their', 'hereupon', 'sake', 'contrariwise', 'nowhere', 'her', 'whole', 'farther', 'whichever', 'hither', 'mine', 'etc', 'ain', 'per', 'wilt', 'down', 'eleven', 'move', 'almost', 'instead', 'same', 'might', 'former', 'above', 'anybody', 'amount', 'cannot', 'often', 'sometimes', 'quite', 'ltd', 'twenty', 'onto', 'again', 'indeed', \"what's\", 'selves', 'whoa', 'been', 'whence', 'thereof', \"don't\", 'hindmost', 'av', 'o', 'exclusive', 'dual', 'therein', 'vs', 'thick', \"he'll\", 'choose', 'kg', 'inside', \"hadn't\", \"couldn't\", 'up', 'most', 'ma', 'using', 'inasmuch', 'side', 'whom', 'can', 'hardly', 'whereafter', 'done', 'hasnt', 'thereby', 'stave', 'wherefrom', 'afterwards', 'whereat', 'with', 'take', 'slew', 'in', 'due', 'henceforth', '!', 'as', 'enough', 'nowadays', 'hereafter', 'cu', 'yet', \"where's\", 'whereby', \"mightn't\", 'yourself', 'whilst', 'interest', 'thenceforth', \"weren't\", 'day', 'thou', 'being', 'ok', 'somewhere', 'he', 'mill', 'thereto', 'but', 'mustn', 'thence', 'farthest', 'underneath', 'other', \"he's\", 'fire', 'few', 'exclude', 'hadn', 'me', 'get', 'give', 'by', '?', 'others', 'more', 'nor', 'five', 'together', 'whichsoever', 'amongst', 'namely', 'last', 'haven', 'howsoever', 'canst', \"shan't\", '+', 'during', 'sang', 'while', 'itself', 'spake', 'hereabouts', 'theirs', 'shouldn', 'anything', 'empty', 'ie', 'found', 'notwithstanding', \"who's\", 'moreover', 'till', 'mightn', 'were', 'everything', 'him', 'still', 'y', 'cant', \"she'll\", 'thereupon', 'ff', 'sideways', \"won't\", 'wouldn', 'must', 'for', \"why's\", 'couldn', 'whereon', 'too', 'anyhow', 'only', 'was', 'included', 'although', 'upwards', 'did', 'wherever', 'somebody', \"wouldn't\", 'seemed', 'third', 'un', 'thee', 'whether', '-', \"they'll\", 'those', 'your', 'inwards', 'everyone', 'ye', 'serious', 'to', ',', 'under', 'whereupon', \"she's\", 'on', 'you', \"we've\", \"mustn't\", 'outside', 'isn', \"haven't\", \"you'd\", 'thereon', 'doesn', \"you've\", 'dost', \"how's\", 'show', 'six', 'one', 'd', 'across', \"i'll\", 'always', 'none', 'off', 'aren', 'already', 'whosoever', 'worse', 'she', \"it's\", 'against', 'furthest', 'sixty', 'everybody', 'own', 'excepting', 'yippee', 'why', 'seeming', 'whereabouts', \"doesn't\", 'co', 'far', 'or', 'wow', 'another', 'herein', 'except', 'himself', \"shouldn't\", 'name', 'forty', 'hath', 'weren', \"they've\", 'at', 'myself', 'including', 'becoming', 'else', \"you're\", 'spat', 'nobody', 'ms', 'unless', 'something', 'after', 'out', 'should', 'toward', 'et', 'much', 'shalt', 'part', 'thereabout', \"i've\", \"that's\", \"we'll\", 'cos', 'thereabouts', 'slunk', 'i', '``', 'whensoever', 'several', 'whenever', 'hitherto', \"let's\", 'mostly', 'because', \"when's\", 'until', 'if', 'sent', 'indoors', 'thin', 'ours', 'shown', 'us', 'which', 'provide', 'mr', 'used', 'could', 'also', 'here', 'somewhat', 'fifteen', 'nope', 'perhaps', 'and', 'round', 'become', 'bottom', 'save', 'inward', \"we'd\", 'ought', 'of', 'meanwhile', 'herself', 'halves', 'lest', 'cry', 'would', 'beside', 'besides', 'unlike', 'someone', 'fifty', 'whatever', 'anyone', 't', '*', 'go', 'though', 'slung', \"he'd\", 'wheresoever', 'whereof', 'whither', 'little', 'sprang', 'worst', 'formerly', 'wasn', 'a', '/', 're', 'kind', \"should've\", 'supposing', 'amoungst', 'are', \"'\", \"you'll\", 'hereto', 'anyway', 'his', 'had', 'hundred', 'within', 'sometime', 'wherewith', 'said', 'seen', 'forth', \"aren't\", 'without', 'thru', 'these', '`', 'however', 'bill', 'everywhere', 'will', 'seems', 'what', \"she'd\", 'below', 'spoke', 'year', 'km', \"they're\", 'became', 'never', 'albeit', 'm', 'want', 'them', 'hereby', 'many', 'let', 'does', 'see', 'eg', 'nothing', 'll', 'top', 'according', 'wherein', 'further', 'inc', 'put', 'whoever', 's', \"wasn't\", 'call', 'some', \"there's\", 'upward', 'detail', 'next', 'it', 'please', 'somehow', 'whereinto', 'once', 'week', 'beforehand', 'double', 'unable', 'even', \"needn't\", 'first', 'latterly', 'themselves', 'my', 'hers', 'we', 'whomever', 'how', \"i'm\", 'there', 'well', 'having', 'twelve', 'do', 'whereunto', 'both', \"we're\", 'our', 'latter', 'am', 'excepted', 'doing', 'plenty', 'whereto', 'forward', 'use', 'full', 'maybe', 'between', 'sprung', \"i'd\", 'whomsoever', 'so', 'alone', 'least', 'every', 'towards', 'be', 'via', 'didn', 'anywhere', 'couldnt', '\"', 'very', 'cf', 'insomuch', 'made', 'de', 'is', 'elsewhere', 'about', 'no', 'exception', 'four', 'saw', 'any', 'each', 'nonetheless', 'need', 'may', 'not', 'into', 'among', 'hasn', 'an', 'around', 'hence', 'over', 'they', 'smote', 'beyond', 'three', 'throughout', 'upon', 'since', 'whereas', 'needn', 'excluding', 'whatsoever', 'con', 'seem', 'now', 'describe', 've', 'furthermore', 'shan', 'whose', 'certain', 'two', \"isn't\", 'seldom', \"here's\", 'that', 'front', \"hasn't\", 'this', \"they'd\", 'ourselves', 'through', 'than', 'seeing', 'meantime', 'has', \"that'll\", 'rather', 'spoken', 'when', 'then', 'apart', 'thy', 'keep', 'eight', 'wherefore', 'yourselves', 'sincere', 'from', 'otherwise', '.', 'fill', 'thrice', 'less', 'won', 'have', 'therefore', 'who', 'nevertheless', 'hast', 'ever', 'just', 'ugh', 'where', 'back', \"didn't\", 'include', 'either', 'like', 'before', 'its', 'such', 'system', 'doth', 'all', 'becomes', 'the', 'find', 'along', 'nine', 'staves', \"''\", 'neither', 'slept', 'thereafter', 'ten', 'really', 'thyself', 'don', 'whew', 'yours', 'behind']\n"
     ]
    }
   ],
   "source": [
    "# GENERATING A LIST OF STOPWORDS\n",
    "# these various stopword lists and the combined joint list were tested on the same classifier (MultinomialNB) with the same\n",
    "# parameters, and it was found that the lemur list and the combined list, the latter includes the former, were the most\n",
    "# efficient ones\n",
    "\n",
    "from sklearn.feature_extraction import stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Straightforward Implementation of a text classifier (as a benchmark)\n",
    "Using the same two classifiers - Naive Bayes and SVM. The classification functions are generic, so you can use any other classifiers by just making minor modifications of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple straightforward\n",
    "def clf_simple(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; use TfidfVectorizer\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))\n",
    "    matrix_train = vectorizer.fit_transform(trainX)    # lowercase=True by default, initially min_df=15, max_df=0.23\n",
    "    matrix_test = vectorizer.transform(testX)\n",
    "                   \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = classifier\n",
    "    clf = clf.fit(matrix_train, trainY)  \n",
    "        \n",
    "    # predict and compute metrics    \n",
    "    predictions=clf.predict(matrix_test)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text Classifier with Pipeline (as a benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "def clf_pipe(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "       \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    predictions=clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text Classifier Using 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cross_val_score\n",
    "def clf_cv(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "        \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(clf, trainX, trainY, cv=kfold, scoring='f1_micro')\n",
    "    print('Cross-validated Accuracy of {}: {:0.4f} +/- {:0.4f}'.format(classifier_name, scores.mean(), scores.std() * 2))\n",
    "    predictions = clf.predict(testX)\n",
    "    cm          = metrics.confusion_matrix(testY, predictions)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Text Classifier with Cross-Validated Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "def clf_GridSearchCV(classifier, data, labels, param_grid):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "            \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # do 3-fold cross validation for each of the possible combinations of the parameter values above\n",
    "    grid = GridSearchCV(clf, cv=3, param_grid=param_grid, scoring='f1_micro')\n",
    "    grid.fit(trainX, trainY)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid.best_score_, \n",
    "        grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    # train and predict on test instances using the best configs found in the CV step\n",
    "        \n",
    "    #predictions = grid.best_estimator_.predict(testX)                   # this is how to find the best estimator \n",
    "    #testX = grid.best_estimator_.named_steps['tfidf'].transform(testX)  # this is how to find indiv. components (same for pipeline)\n",
    "    predictions=grid.predict(testX)                                      # called on the best estimator by default\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} cross-validated F-1 score with grid search: {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    # return the best classifier to run it on the full dataset    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Running 7 options on the limited data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8386\n",
      "Confusion matrix:\n",
      "[[   1    0  879]\n",
      " [   0    0  500]\n",
      " [   0    0 7165]]\n",
      "\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultinomialNB()\n",
    "nb = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "        'vect__max_df':[0.25, 0.5,0.75],\n",
    "        'vect__min_df':[5,15,25,50,100],\n",
    "        'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "        'clf__alpha':[0.1,0.25,0.5,0.75,1.0]\n",
    "    }\n",
    "clf_simple(nb, data, labels)    # straightforward NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8386\n",
      "Confusion matrix:\n",
      "[[   1    0  879]\n",
      " [   0    0  500]\n",
      " [   0    0 7165]]\n",
      "\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_pipe(nb, data, labels)    # pipeline NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of MultinomialNB: 0.8436 +/- 0.0040\n",
      "Confusion matrix:\n",
      "[[   1    0  879]\n",
      " [   0    0  500]\n",
      " [   0    0 7165]]\n",
      "\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(nb, data, labels)    # cross_val_score NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.862510 using {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853527 (0.000559) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.861047 (0.000571) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.861076 (0.000629) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.854727 (0.000848) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.861954 (0.001561) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862451 (0.001346) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853410 (0.000899) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.859379 (0.001380) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.860081 (0.001309) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.852035 (0.001052) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.855283 (0.001013) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.855605 (0.000870) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.849665 (0.001027) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.851538 (0.001272) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.851684 (0.001296) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.853674 (0.000369) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.861135 (0.000695) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.861252 (0.000654) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.854961 (0.001024) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.862012 (0.001504) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862510 (0.001206) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853878 (0.000980) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.859584 (0.001487) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.860286 (0.001290) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.852562 (0.001109) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.855195 (0.001033) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.855312 (0.000970) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.850162 (0.001185) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.851713 (0.001393) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.851801 (0.001268) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.853820 (0.000242) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.860901 (0.000514) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.860959 (0.000622) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.854844 (0.001060) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.861866 (0.001629) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862217 (0.001128) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853849 (0.000937) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.859379 (0.001403) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.859906 (0.001126) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.852298 (0.001044) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.855195 (0.001201) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.855195 (0.000988) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.849928 (0.001122) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.851421 (0.001228) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.851421 (0.001166) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.849899 (0.000584) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.852006 (0.000551) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.852415 (0.000591) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853118 (0.000719) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858823 (0.000979) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.859262 (0.000836) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852503 (0.000907) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.858179 (0.001427) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.858531 (0.001055) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851186 (0.000848) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.854493 (0.001316) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.854902 (0.001112) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.849197 (0.001155) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.850952 (0.001062) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.851157 (0.001172) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.850016 (0.000525) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.852211 (0.000483) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.852445 (0.000584) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853410 (0.000443) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.859174 (0.000772) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.859584 (0.000731) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852796 (0.000718) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.858297 (0.001121) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.858677 (0.000951) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851625 (0.000984) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.854522 (0.001122) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.854698 (0.000916) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.849753 (0.001079) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.851421 (0.001107) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.851567 (0.001008) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.849899 (0.000615) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.851830 (0.000359) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.852240 (0.000557) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853205 (0.000411) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858882 (0.000916) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.859262 (0.000837) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852620 (0.000766) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.858092 (0.001055) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.858531 (0.000700) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851508 (0.000686) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.854405 (0.001292) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.854581 (0.001104) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.849665 (0.001146) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.851216 (0.000952) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.851274 (0.000927) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845686 (0.000387) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846154 (0.000153) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846095 (0.000207) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849723 (0.000565) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854112 (0.001157) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.854610 (0.001122) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850455 (0.000729) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854932 (0.001486) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855224 (0.001567) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850309 (0.000795) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853293 (0.001483) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853556 (0.001516) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848758 (0.001185) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.850192 (0.000699) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.850221 (0.000719) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845715 (0.000351) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845949 (0.000132) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846008 (0.000194) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849841 (0.000668) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854229 (0.001201) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.854493 (0.001104) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850572 (0.000525) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.855254 (0.001449) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855400 (0.001310) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850455 (0.000586) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853439 (0.001408) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853556 (0.001304) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.849197 (0.001142) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.850484 (0.000790) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.850514 (0.000848) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845686 (0.000316) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845832 (0.000194) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845949 (0.000204) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849753 (0.000732) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854054 (0.001088) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.854288 (0.000938) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850367 (0.000705) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854873 (0.001425) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854961 (0.001380) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850133 (0.000662) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853381 (0.001301) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853498 (0.001338) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848992 (0.001117) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.850133 (0.000655) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.850192 (0.000629) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844223 (0.000146) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844223 (0.000169) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844398 (0.000209) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847617 (0.000709) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.850455 (0.001013) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.850396 (0.000907) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848407 (0.000551) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.851918 (0.001123) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852064 (0.001174) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849402 (0.000917) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851801 (0.000885) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851918 (0.000908) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848085 (0.000839) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849255 (0.000719) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849314 (0.000717) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844223 (0.000201) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844164 (0.000241) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844340 (0.000241) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847617 (0.000848) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.850338 (0.000904) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.850279 (0.000946) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848524 (0.000618) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852035 (0.001104) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852006 (0.001126) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849519 (0.000628) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851684 (0.000875) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851830 (0.000784) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848670 (0.001210) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849606 (0.000722) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849665 (0.000671) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844106 (0.000073) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844047 (0.000201) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844252 (0.000223) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847500 (0.000896) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.850133 (0.000815) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.850133 (0.000912) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848407 (0.000526) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.851947 (0.001231) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852006 (0.001123) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849197 (0.000656) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851421 (0.000797) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851596 (0.000740) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848114 (0.001048) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849168 (0.000655) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849314 (0.000681) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.843608 (0.000120) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.843608 (0.000049) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.843813 (0.000037) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846125 (0.000262) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847968 (0.000510) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847792 (0.000441) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847149 (0.000586) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849577 (0.001103) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.849577 (0.001044) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848319 (0.000737) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850221 (0.001101) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850192 (0.000993) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847471 (0.000919) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848495 (0.000860) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848524 (0.000825) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.843608 (0.000120) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.843608 (0.000049) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.843784 (0.000049) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846183 (0.000394) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847705 (0.000441) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847763 (0.000514) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847178 (0.000717) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849723 (0.001197) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.849636 (0.001123) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848582 (0.000715) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850338 (0.001037) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850396 (0.000979) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847734 (0.001041) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848816 (0.000878) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848933 (0.000918) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.843579 (0.000131) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.843579 (0.000013) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.843725 (0.000090) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846066 (0.000302) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847558 (0.000483) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847529 (0.000480) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847032 (0.000687) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849606 (0.001148) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.849519 (0.001082) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848348 (0.000755) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850162 (0.001020) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850279 (0.000993) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847324 (0.001093) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848582 (0.000899) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848670 (0.000942) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB cross-validated F-1 score with grid search: 0.8578\n",
      "Confusion matrix:\n",
      "[[  93   24  763]\n",
      " [  44  118  338]\n",
      " [  44    2 7119]]\n",
      "\n",
      "Wall time: 2h 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_NB = clf_GridSearchCV(nb, data, labels, nb_param_grid)    # parameter grid search NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC F-1 score:   0.8728\n",
      "Confusion matrix:\n",
      "[[ 209   71  600]\n",
      " [ 101  209  190]\n",
      " [ 109   16 7040]]\n",
      "\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC()\n",
    "svc_param_grid = {\n",
    "    'vect__max_df':[0.25,0.5,0.75],\n",
    "    'vect__min_df':[5,15,25,50,100],\n",
    "    'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "    'clf__C':[0.1,0.25,0.5,0.75,1.0]\n",
    "}\n",
    "clf_pipe(svc, data, labels)            # pipeline SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of LinearSVC: 0.8738 +/- 0.0057\n",
      "Confusion matrix:\n",
      "[[ 209   71  600]\n",
      " [ 101  209  190]\n",
      " [ 109   16 7040]]\n",
      "\n",
      "Wall time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(svc, data, labels)             # cross_val_score SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.875384 using {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.862656 (0.000531) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.861544 (0.000654) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.861018 (0.000411) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.864529 (0.000641) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.864412 (0.000423) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.864529 (0.000493) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.865055 (0.000485) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.865611 (0.000113) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.865582 (0.000172) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.865494 (0.001204) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867133 (0.001174) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867221 (0.000874) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864090 (0.000984) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865699 (0.000950) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865728 (0.000957) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864792 (0.000196) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.862422 (0.000772) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.862247 (0.000647) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866460 (0.000538) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.865816 (0.000466) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.865728 (0.000343) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867396 (0.000637) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.866665 (0.000308) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.866548 (0.000037) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.868186 (0.000488) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867718 (0.000186) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867630 (0.000119) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.866782 (0.000335) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.866606 (0.000806) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.866694 (0.000855) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864851 (0.000267) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.862305 (0.000606) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.862334 (0.000649) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866606 (0.000574) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.865670 (0.000196) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.865582 (0.000037) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867367 (0.000555) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.866372 (0.000394) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.866460 (0.000298) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.868333 (0.000062) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867689 (0.000072) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867630 (0.000130) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.866343 (0.000673) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.866752 (0.000755) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.866723 (0.000903) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.870000 (0.000970) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.870732 (0.000510) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.870556 (0.000663) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.869883 (0.001424) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.873511 (0.000765) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.873687 (0.000891) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.870820 (0.001838) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.872955 (0.001424) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.872985 (0.001251) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.870117 (0.002332) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.871990 (0.002090) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.871844 (0.002098) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.866460 (0.001394) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.869386 (0.001135) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.869386 (0.001347) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.872838 (0.000736) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.871902 (0.000296) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.871902 (0.000320) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.872838 (0.000360) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.873950 (0.000507) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.874155 (0.000471) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873863 (0.000378) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.873277 (0.000870) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873365 (0.000918) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.873014 (0.001086) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.872780 (0.001124) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.872780 (0.001300) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.869503 (0.000796) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.870468 (0.001430) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.870644 (0.001486) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.872809 (0.000695) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.871873 (0.000467) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.871844 (0.000369) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.873219 (0.000624) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.873950 (0.000480) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.874009 (0.000488) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873628 (0.000727) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.872868 (0.000789) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873102 (0.000859) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.873336 (0.000667) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.872897 (0.001193) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.872721 (0.001234) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.869854 (0.000825) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.870351 (0.001445) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.870498 (0.001338) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.871171 (0.001221) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.874331 (0.000957) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.874301 (0.000640) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.871083 (0.001411) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.873775 (0.001012) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.873863 (0.000215) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.870937 (0.001841) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.872868 (0.001116) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873160 (0.001000) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.869591 (0.002558) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.872517 (0.002227) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.872195 (0.001828) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.866577 (0.002020) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.869269 (0.001638) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.869269 (0.001552) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.873511 (0.000797) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.874565 (0.000573) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875121 (0.000638) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.873804 (0.000559) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.874331 (0.000574) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.874448 (0.000697) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873453 (0.001227) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.873043 (0.001168) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873336 (0.000800) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.872575 (0.000965) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.872224 (0.001239) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.872517 (0.000965) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.870264 (0.000967) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.869942 (0.001221) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.869971 (0.001142) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.873716 (0.000483) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.875062 (0.000728) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875267 (0.000802) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.873511 (0.000243) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.874301 (0.000376) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.874155 (0.000584) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873453 (0.000992) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.873248 (0.000970) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873511 (0.001106) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.873014 (0.000769) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.872429 (0.001105) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.872751 (0.001107) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.870615 (0.000750) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.869942 (0.001042) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.870030 (0.001108) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.870059 (0.002044) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.875121 (0.000906) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875121 (0.000441) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868625 (0.001215) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.871902 (0.000996) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.872224 (0.000731) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867747 (0.002347) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.870351 (0.001228) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.870937 (0.001283) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.868128 (0.002591) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.869766 (0.002398) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.869825 (0.002354) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.866226 (0.001591) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.868859 (0.001646) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.868713 (0.001610) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.871375 (0.001483) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.875384 (0.000424) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875179 (0.000367) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.871054 (0.001402) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.872165 (0.000790) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.872312 (0.000667) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.871610 (0.001482) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.870790 (0.001350) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.870966 (0.001418) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.871141 (0.000766) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.869854 (0.001525) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.870234 (0.001113) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.869737 (0.000511) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.869005 (0.001122) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.869181 (0.000757) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.871551 (0.001208) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.875384 (0.000559) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875121 (0.000375) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.871112 (0.001019) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.872224 (0.000758) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.872400 (0.000510) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.871902 (0.001410) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.870761 (0.001400) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.871141 (0.001443) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.870761 (0.000588) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.870234 (0.001360) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.870468 (0.000986) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.869913 (0.000339) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.869005 (0.000620) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.869152 (0.000488) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867367 (0.002032) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.873599 (0.000597) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.874331 (0.000513) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.865641 (0.001414) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.869620 (0.000539) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.869474 (0.001002) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.865962 (0.002804) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.868128 (0.001940) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867806 (0.001850) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866899 (0.002560) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867630 (0.002898) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867835 (0.002781) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.865494 (0.001312) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.867952 (0.001241) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.867894 (0.001580) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.869093 (0.001331) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.874067 (0.001361) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875004 (0.001019) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868567 (0.001810) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.869181 (0.000649) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.869444 (0.000761) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.868567 (0.002237) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.868128 (0.000803) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.868128 (0.000814) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.869444 (0.001228) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867894 (0.001563) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867806 (0.001811) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.868830 (0.000448) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.867777 (0.000612) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.867777 (0.000866) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.868947 (0.001224) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.874214 (0.001369) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875091 (0.001156) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868508 (0.001322) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.869298 (0.000826) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.869503 (0.000682) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.869181 (0.001742) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.868245 (0.001156) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.868157 (0.001023) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.869181 (0.000799) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.868011 (0.001459) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867894 (0.001566) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.869123 (0.000339) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.868040 (0.000688) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.868011 (0.000607) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC cross-validated F-1 score with grid search: 0.8734\n",
      "Confusion matrix:\n",
      "[[ 196   68  616]\n",
      " [  96  207  197]\n",
      " [  92   13 7060]]\n",
      "\n",
      "Wall time: 2h 10min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_SVM = clf_GridSearchCV(svc, data, labels, svc_param_grid)    # parameter grid search SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Running Naive Bayes and SVM with the Best Parameters from Grid Search on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full dataset and labels\n",
    "full_data = df_text['reviewText'].values\n",
    "full_labels = df_text['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Naive Bayes F-1 score on full dataset: 0.8832\n",
      "Confusion matrix:\n",
      "[[  6189   1657  11385]\n",
      " [  2036   5718   3598]\n",
      " [  3742    543 161656]]\n",
      "\n",
      "The best SVM F-1 score on full dataset: 0.8971\n",
      "Confusion matrix:\n",
      "[[  6591   1997  10643]\n",
      " [  2089   6884   2379]\n",
      " [  2577    532 162832]]\n",
      "\n",
      "Wall time: 16min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the two best classifier on it\n",
    "for best_clf in [best_NB, best_SVM]:\n",
    "        \n",
    "    # split into train and test sets\n",
    "    trainX, testX, trainY, testY = train_test_split(full_data, full_labels, test_size = 0.2, random_state = 43)\n",
    "    clf = best_clf.fit(trainX, trainY)\n",
    "    \n",
    "    # predict and compute metrics\n",
    "    predictions = clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('The best {} F-1 score on full dataset: {:0.4f}'.format('Naive Bayes' if best_clf==best_NB else 'SVM', score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
