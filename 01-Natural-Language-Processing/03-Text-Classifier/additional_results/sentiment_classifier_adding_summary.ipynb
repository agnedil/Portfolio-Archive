{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. ReviewText + Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\n",
      "Scikit-learn 0.20.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "#nltk.download('stopwords')    # this is done just once\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from platform import python_version\n",
    "print('Python {}'.format(python_version()))\n",
    "print('Scikit-learn {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in json, combine reviewText with summary columns, get labels cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 982619\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path=''\n",
    "file='kindle_reviews.json'\n",
    "df = pd.read_json(path_or_buf=path+file, lines=True, encoding='utf-8')    #, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, chunksize=None, compression='infer')\n",
    "print('Length of text: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1F6404F1VG29J</td>\n",
       "      <td>Avidreader</td>\n",
       "      <td>Nice vintage story</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "      <td>01 6, 2014</td>\n",
       "      <td>AN0N05A9LIJEQ</td>\n",
       "      <td>critters</td>\n",
       "      <td>Different...</td>\n",
       "      <td>1388966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "      <td>04 4, 2014</td>\n",
       "      <td>A795DMNCJILA6</td>\n",
       "      <td>dot</td>\n",
       "      <td>Oldie</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>A1FV0SX13TWVXQ</td>\n",
       "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
       "      <td>I really liked it.</td>\n",
       "      <td>1392768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "      <td>03 19, 2014</td>\n",
       "      <td>A3SPTOKDG7WBLN</td>\n",
       "      <td>Father Dowling Fan</td>\n",
       "      <td>Period Mystery</td>\n",
       "      <td>1395187200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  B000F83SZQ  [0, 0]        5   \n",
       "1  B000F83SZQ  [2, 2]        4   \n",
       "2  B000F83SZQ  [2, 2]        4   \n",
       "3  B000F83SZQ  [1, 1]        5   \n",
       "4  B000F83SZQ  [0, 1]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I enjoy vintage books and movies so I enjoyed ...   05 5, 2014   \n",
       "1  This book is a reissue of an old one; the auth...   01 6, 2014   \n",
       "2  This was a fairly interesting read.  It had ol...   04 4, 2014   \n",
       "3  I'd never read any of the Amy Brewster mysteri...  02 19, 2014   \n",
       "4  If you like period pieces - clothing, lingo, y...  03 19, 2014   \n",
       "\n",
       "       reviewerID                         reviewerName             summary  \\\n",
       "0  A1F6404F1VG29J                           Avidreader  Nice vintage story   \n",
       "1   AN0N05A9LIJEQ                             critters        Different...   \n",
       "2   A795DMNCJILA6                                  dot               Oldie   \n",
       "3  A1FV0SX13TWVXQ  Elaine H. Turley \"Montana Songbird\"  I really liked it.   \n",
       "4  A3SPTOKDG7WBLN                   Father Dowling Fan      Period Mystery   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1399248000  \n",
       "1      1388966400  \n",
       "2      1396569600  \n",
       "3      1392768000  \n",
       "4      1395187200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the \"summary\" column to reviewText\n",
    "df['reviewText'] = df['summary'].map(str) + ' ' + df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['reviewText', 'overall']].copy()        # copy only certain columns to another df\n",
    "df = None                                             # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice vintage story I enjoy vintage books and m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Different... This book is a reissue of an old ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oldie This was a fairly interesting read.  It ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I really liked it. I'd never read any of the A...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Period Mystery If you like period pieces - clo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  Nice vintage story I enjoy vintage books and m...        5\n",
       "1  Different... This book is a reissue of an old ...        4\n",
       "2  Oldie This was a fairly interesting read.  It ...        4\n",
       "3  I really liked it. I'd never read any of the A...        5\n",
       "4  Period Mystery If you like period pieces - clo...        4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert 5 ratings to 3 ('neg', 'mixed', and 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 5 ratings to 3 ('neg', 'mixed', and 'pos')\n",
    "df_text['overall'] = df_text['overall'].apply(lambda x: 'pos' if x > 3 else 'neg' if x < 3 else 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice vintage story I enjoy vintage books and m...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Different... This book is a reissue of an old ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oldie This was a fairly interesting read.  It ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I really liked it. I'd never read any of the A...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Period Mystery If you like period pieces - clo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText overall\n",
       "0  Nice vintage story I enjoy vintage books and m...     pos\n",
       "1  Different... This book is a reissue of an old ...     pos\n",
       "2  Oldie This was a fairly interesting read.  It ...     pos\n",
       "3  I really liked it. I'd never read any of the A...     pos\n",
       "4  Period Mystery If you like period pieces - clo...     pos"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the dataset is 982619\n"
     ]
    }
   ],
   "source": [
    "print('Total length of the dataset is {}'.format(len(df_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reduce size of data, get reduced-size dataset and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sample is 42722\n"
     ]
    }
   ],
   "source": [
    "# get a random sample from the dataframe whose size is manageable for cross-validation and grid search\n",
    "# with more computing resources and/or time, this can be done on a larger data set\n",
    "length = len(df_text)\n",
    "df_text = df_text.sample(n=length)\n",
    "df_text_short = df_text.sample(n=int(length/23))\n",
    "print('Length of the sample is {}'.format(len(df_text_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos      36117\n",
       "mixed     4094\n",
       "neg       2511\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique labels\n",
    "df_text_short.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our train set and labels\n",
    "data = df_text_short['reviewText'].values\n",
    "labels = df_text_short['overall'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Experimenting with different lists of stopwords and selecting the most efficient one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['he', 'nowhere', 'put', 'between', 'once', 'within', 'him', 'under', 'many', 'every', 'hereby', 'towards', 'thereby', 'herself', 'again', 'amoungst', 'besides', 'been', 'bill', 'very', 'mine', 'give', 'see', 'am', 'name', 'how', 'must', 'before', 'three', 'get', 'seems', 'much', 'further', 'down', 'to', 'without', 'nevertheless', 'thin', 'move', 'enough', 'con', 'even', 'keep', 'most', 'ours', 'yourselves', 'whereby', 'do', 'yourself', 'eight', 'thick', 'detail', 'by', 'such', 'thru', 'ie', 'may', 'should', 'their', 'those', 'done', 'full', 'across', 'are', 'the', 'bottom', 'has', 'yours', 'thereafter', 'hundred', 'behind', 'mill', 'being', 'we', 'had', 'same', 'because', 'nobody', 'more', 'thus', 'upon', 'whole', 'as', 're', 'all', 'in', 'sometime', 'found', 'fifty', 'seemed', 'beside', 'onto', 'itself', 'these', 'a', 'perhaps', 'about', 'everyone', 'ourselves', 'sometimes', 'herein', 'formerly', 'part', 'amongst', 'i', 'for', 'therein', 'forty', 'or', 'find', 'anyway', 'go', 'which', 'whenever', 'everything', 'sixty', 'anyhow', 'several', 'this', 'whose', 'own', 'beyond', 'becomes', 'somewhere', 'former', 'namely', 'among', 'below', 'an', 'after', 'front', 'also', 'while', 'his', 'sincere', 'along', 'our', 'though', 'others', 'therefore', 'still', 'into', 'wherever', 'latter', 'here', 'would', 'she', 'please', 'un', 'could', 'nor', 'last', 'cannot', 'eg', 'anywhere', 'at', 'couldnt', 'wherein', 'not', 'call', 'thence', 'alone', 'one', 'up', 'anything', 'is', 'well', 'describe', 'de', 'next', 'something', 'through', 'mostly', 'was', 'ltd', 'himself', 'from', 'hasnt', 'less', 'together', 'already', 'when', 'amount', 'during', 'fill', 'out', 'us', 'hers', 'inc', 'empty', 'any', 'hereupon', 'another', 'whereas', 'whoever', 'serious', 'least', 'toward', 'due', 'became', 'top', 'cry', 'them', 'whether', 'everywhere', 'its', 'meanwhile', 'now', 'ever', 'other', 'and', 'that', 'else', 'co', 'whereupon', 'your', 'myself', 'might', 'someone', 'take', 'have', 'first', 'four', 'eleven', 'of', 'indeed', 'whom', 'with', 'third', 'on', 'never', 'be', 'off', 'some', 'rather', 'back', 'except', 'nine', 'fifteen', 'it', 'per', 'around', 'none', 'hence', 'etc', 'where', 'were', 'cant', 'over', 'otherwise', 'almost', 'moreover', 'each', 'me', 'can', 'two', 'you', 'yet', 'but', 'five', 'against', 'will', 'twenty', 'seeming', 'if', 'via', 'beforehand', 'there', 'throughout', 'anyone', 'show', 'made', 'ten', 'afterwards', 'elsewhere', 'thereupon', 'than', 'often', 'fire', 'themselves', 'too', 'both', 'noone', 'system', 'hereafter', 'her', 'few', 'interest', 'no', 'why', 'only', 'my', 'until', 'whereafter', 'six', 'seem', 'latterly', 'what', 'whither', 'side', 'somehow', 'who', 'then', 'although', 'however', 'neither', 'becoming', 'nothing', 'since', 'above', 'either', 'twelve', 'whatever', 'become', 'always', 'whence', 'so', 'they']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "153\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['furthermore', 'put', 'he', 'nowhere', \"'\", 'between', 'once', \"don't\", 'within', \"that'll\", 'underneath', 'upward', 'him', 'under', 'hereabouts', 'worst', 'many', 'every', 'wherefore', 'halves', \"should've\", 'hadn', 'hereby', 'towards', 'thereby', 'spake', \"she'd\", 'herself', 'contrariwise', 'till', 'again', 'selves', 'amoungst', 'besides', 'apart', 'nonetheless', 'been', 'week', 'bill', 'very', 'thou', 'sprung', 'mine', 'give', 'mightn', 'slept', 'see', 'forward', 'am', 'name', 'somewhat', 'let', 'hath', 'how', 'thereof', 'must', 'before', 'three', 'farther', 'get', 'really', 'seems', 'double', 'much', 'further', 'down', 'whereof', '-', '?', 'doth', 'to', 'shalt', 'vs', 'without', 'nevertheless', 'wow', \"''\", 'thin', 'move', 'enough', 'worse', 'meantime', 'con', 'even', 'keep', 'most', 'whereat', 'ours', 'anybody', 'included', 'yourselves', 'whereby', 'do', 'nowadays', 'yourself', 'said', 'eight', 'thick', 'detail', 'by', 'did', 'such', 'thru', 'spoke', 'ok', 'ie', 'mr', 'sang', 'may', 'whereinto', 'should', 'their', \"it's\", 'those', 'cf', 'hither', 'stave', 'done', 'full', 's', \"they'll\", 'exclusive', \"let's\", 'across', 'are', \"who's\", 'the', 'bottom', 'has', 'yours', 'thereafter', 'hundred', 'whatsoever', 'behind', 'mill', \"why's\", 'thereon', 'save', 'saw', 'being', 'notwithstanding', 'we', 'had', 'same', 'because', 'nobody', 'more', 'thus', 'o', 'upon', 'whole', 'as', 're', 'all', \"you're\", 'in', 'sometime', 'found', 'fifty', 'farthest', 'cu', 'supposing', 'seemed', 'beside', 'onto', 'itself', 'wasn', 'hast', 'these', 'a', 'ye', 'needn', \"he's\", '!', 'inasmuch', 'et', 'used', 'perhaps', 'wilt', 'does', \"we'll\", 'about', \"how's\", \"mightn't\", 'everyone', 'ourselves', 'sometimes', 'plenty', 'using', 'herein', 'thrice', \"aren't\", 'formerly', 'part', 'aren', 'amongst', 'i', 'ain', 'round', 'for', 'therein', 'henceforth', 'hindmost', 'need', 'forty', 'or', 'find', 'slunk', 'anyway', 'go', 'which', 'whenever', 'everything', 'sixty', 'anyhow', 'several', 'this', 'whose', 'own', \"they've\", 'beyond', 'quite', 'becomes', \"couldn't\", 'somewhere', 'former', 'kg', 'namely', \"isn't\", 'among', 'whereon', 'below', 'an', 'after', 'front', \"hadn't\", \"won't\", \"we'd\", 't', \"i'm\", 'mrs', 'thee', 'also', 'while', 'his', 'sincere', \"shouldn't\", 'along', 'canst', 'our', 'unable', 'though', 'others', 'whosoever', 'therefore', 'staves', 'inwards', 'still', 'don', 'y', 'ought', 'wherewith', 'choose', 'into', 'forth', 'wherever', 'exclude', 'latter', 'wouldn', 'here', 'including', 'would', 'km', 'she', 'seldom', 'please', 'un', 'doing', 'want', 'could', \"where's\", 'nor', 'last', 'cannot', 'unlike', \"doesn't\", \"hasn't\", 'eg', 'anywhere', 'at', 'thereto', 'couldnt', \"haven't\", 'wherein', 'not', \"wasn't\", 'indoors', 'call', 'thence', 'alone', 'one', \"they're\", \"we've\", 'wherefrom', 'up', 'anything', 'yippee', 'is', 'well', 'describe', 'having', '``', 'smote', 'maybe', 'de', 'next', 'something', 'through', 'mostly', 'was', \"he'd\", 'hardly', 'ltd', 'himself', \"mustn't\", 'slew', 'from', 'hasnt', 'less', 'together', 'year', 'thyself', 'already', 'when', \"needn't\", 'amount', \"we're\", 've', 'during', 'ugh', \"didn't\", 'fill', 'out', 'couldn', 'us', 'sideways', 'hers', 'inc', 'empty', 'any', 'dost', \"there's\", 'hereupon', 'another', 'whereas', 'whoever', 'serious', '*', 'somebody', 'least', 'toward', 'due', 'sent', 'became', 'top', 'sprang', 'whensoever', 'av', 'dual', 'howsoever', 'provide', 'cry', \"you'll\", 'nope', 'them', '`', 'whether', 'doesn', 'everywhere', 'its', 'little', 'ff', \"i'll\", 'meanwhile', 'now', 'kind', 'ever', 'other', 'furthest', 'and', \"he'll\", \"that's\", 'that', 'else', 'co', 'insomuch', 'whereupon', 'your', \"what's\", 'myself', 'hitherto', 'whereunto', 'might', 'd', 'someone', 'excepted', 'take', 'cos', 'have', 'first', 'thereabout', 'four', 'isn', \"weren't\", 'eleven', 'of', 'thereabouts', 'indeed', 'whom', 'with', 'third', 'hasn', 'whomsoever', 'on', 'never', 'll', 'just', 'be', 'off', 'some', 'use', 'ms', 'rather', 'didn', 'back', 'except', 'nine', 'far', 'fifteen', 'it', 'per', 'around', 'none', 'whereabouts', 'hence', \"when's\", 'weren', 'whereto', 'etc', 'where', 'were', 'cant', 'according', 'over', 'otherwise', 'excluding', 'almost', 'moreover', 'each', 'theirs', \"shan't\", 'me', 'inward', 'can', 'day', 'spat', 'two', 'you', 'seen', 'yet', 'but', 'whilst', 'five', 'against', 'will', 'twenty', 'seeming', 'whomever', \"they'd\", \"i've\", 'ma', \"here's\", 'hereto', 'if', 'via', 'beforehand', 'm', 'lest', 'there', 'throughout', 'anyone', 'show', 'made', 'ten', '.', 'afterwards', 'elsewhere', 'thereupon', 'than', 'whichever', 'often', 'fire', 'themselves', 'too', 'unless', 'inside', \"you'd\", 'both', 'noone', 'system', 'like', 'shown', 'hereafter', 'her', 'whichsoever', 'upwards', '+', 'few', 'instead', 'interest', 'thenceforth', 'no', 'why', \"wouldn't\", 'only', 'my', 'until', 'whereafter', 'six', 'whoa', 'seem', \"she's\", ',', 'latterly', 'what', 'whither', 'whew', 'sake', \"you've\", \"i'd\", 'side', 'mustn', 'somehow', 'who', 'everybody', 'exception', 'haven', 'certain', 'shouldn', 'excepting', 'then', 'although', 'outside', 'however', '\"', 'seeing', 'neither', 'becoming', 'wheresoever', 'nothing', \"she'll\", 'shan', 'spoken', 'slung', 'since', 'above', 'thy', 'either', 'twelve', '/', 'whatever', 'become', 'albeit', 'always', 'whence', 'so', 'include', 'won', 'they']\n"
     ]
    }
   ],
   "source": [
    "# GENERATING A LIST OF STOPWORDS\n",
    "# these various stopword lists and the combined joint list were tested on the same classifier (MultinomialNB) with the same\n",
    "# parameters, and it was found that the lemur list and the combined list, the latter includes the former, were the most\n",
    "# efficient ones\n",
    "\n",
    "from sklearn.feature_extraction import stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Straightforward Implementation of a text classifier (as a benchmark)\n",
    "Using the same two classifiers - Naive Bayes and SVM. The classification functions are generic, so you can use any other classifiers by just making minor modifications of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple straightforward\n",
    "def clf_simple(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; use TfidfVectorizer\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))\n",
    "    matrix_train = vectorizer.fit_transform(trainX)    # lowercase=True by default, initially min_df=15, max_df=0.23\n",
    "    matrix_test = vectorizer.transform(testX)\n",
    "                   \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = classifier\n",
    "    clf = clf.fit(matrix_train, trainY)  \n",
    "        \n",
    "    # predict and compute metrics    \n",
    "    predictions=clf.predict(matrix_test)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text Classifier with Pipeline (as a benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "def clf_pipe(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "       \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    predictions=clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text Classifier Using 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cross_val_score\n",
    "def clf_cv(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "        \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(clf, trainX, trainY, cv=kfold, scoring='f1_micro')\n",
    "    print('Cross-validated Accuracy of {}: {:0.4f} +/- {:0.4f}'.format(classifier_name, scores.mean(), scores.std() * 2))\n",
    "    predictions = clf.predict(testX)\n",
    "    cm          = metrics.confusion_matrix(testY, predictions)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Text Classifier with Cross-Validated Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "def clf_GridSearchCV(classifier, data, labels, param_grid):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "            \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # do 3-fold cross validation for each of the possible combinations of the parameter values above\n",
    "    grid = GridSearchCV(clf, cv=3, param_grid=param_grid, scoring='f1_micro')\n",
    "    grid.fit(trainX, trainY)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid.best_score_, \n",
    "        grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    # train and predict on test instances using the best configs found in the CV step\n",
    "        \n",
    "    #predictions = grid.best_estimator_.predict(testX)                   # this is how to find the best estimator \n",
    "    #testX = grid.best_estimator_.named_steps['tfidf'].transform(testX)  # this is how to find indiv. components (same for pipeline)\n",
    "    predictions=grid.predict(testX)                                      # called on the best estimator by default\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} cross-validated F-1 score with grid search: {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    # return the best classifier to run it on the full dataset    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Running 7 options on the limited data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8455\n",
      "Confusion matrix:\n",
      "[[   2    0  827]\n",
      " [   1    3  492]\n",
      " [   0    0 7220]]\n",
      "\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultinomialNB()\n",
    "nb = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "        'vect__max_df':[0.25, 0.5,0.75],\n",
    "        'vect__min_df':[5,15,25,50,100],\n",
    "        'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "        'clf__alpha':[0.1,0.25,0.5,0.75,1.0]\n",
    "    }\n",
    "clf_simple(nb, data, labels)    # straightforward NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8455\n",
      "Confusion matrix:\n",
      "[[   2    0  827]\n",
      " [   1    3  492]\n",
      " [   0    0 7220]]\n",
      "\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_pipe(nb, data, labels)    # pipeline NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of MultinomialNB: 0.8462 +/- 0.0082\n",
      "Confusion matrix:\n",
      "[[   2    0  827]\n",
      " [   1    3  492]\n",
      " [   0    0 7220]]\n",
      "\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(nb, data, labels)    # cross_val_score NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.866840 using {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.858589 (0.000847) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.864265 (0.001485) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.864968 (0.001109) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.859379 (0.000768) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866460 (0.000830) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.866840 (0.000687) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.858677 (0.000449) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863622 (0.000749) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864295 (0.000555) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.856307 (0.000154) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.860257 (0.000690) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.860345 (0.000876) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.854054 (0.000909) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.855868 (0.001119) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.856044 (0.001262) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.858648 (0.000926) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.864236 (0.001436) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.864734 (0.001008) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.859174 (0.000640) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866489 (0.000619) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.866694 (0.000837) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.858677 (0.000393) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863914 (0.000664) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864295 (0.000627) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.856570 (0.000414) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.860403 (0.000307) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.860579 (0.000516) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.854142 (0.001237) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.856044 (0.001333) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.856102 (0.001260) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.858589 (0.000988) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.863944 (0.001585) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.864441 (0.001191) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.858940 (0.000757) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866489 (0.000735) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.866577 (0.000768) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.858326 (0.000384) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863739 (0.000804) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864119 (0.000741) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.856190 (0.000287) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.860169 (0.000343) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.860315 (0.000376) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.853732 (0.001117) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.855868 (0.001152) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.855868 (0.001157) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.853761 (0.000078) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.856512 (0.000778) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856365 (0.000620) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.857272 (0.000772) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.863183 (0.000980) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.863622 (0.000839) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.856570 (0.000331) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862188 (0.000477) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862656 (0.000538) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.855663 (0.000367) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.859087 (0.000651) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.859291 (0.000831) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.853732 (0.000866) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.855692 (0.001224) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.855692 (0.001275) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.853995 (0.000081) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.856453 (0.000813) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856190 (0.000614) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.857097 (0.000650) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.863154 (0.000876) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.863651 (0.001013) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.856921 (0.000387) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862422 (0.000570) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862744 (0.000610) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.855722 (0.000406) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.859350 (0.000615) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.859467 (0.000690) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.853732 (0.001020) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.855722 (0.001546) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.855751 (0.001513) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.853615 (0.000113) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.856073 (0.000597) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.856044 (0.000759) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.857009 (0.000619) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.862949 (0.000893) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.863446 (0.000985) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.856599 (0.000482) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.861808 (0.000624) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862276 (0.000632) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.855488 (0.000302) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.858999 (0.000799) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.859204 (0.000865) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.853352 (0.001097) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.855458 (0.001522) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.855575 (0.001490) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.848933 (0.000276) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.849489 (0.000185) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.849460 (0.000337) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853703 (0.000675) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858297 (0.001085) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858443 (0.001010) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853732 (0.000494) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.858326 (0.000891) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.858326 (0.000958) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.854288 (0.000337) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.857302 (0.000471) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.857507 (0.000461) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.852971 (0.000865) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.854581 (0.000732) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.854581 (0.000732) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.848699 (0.000430) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.849489 (0.000228) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.849372 (0.000331) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853966 (0.000661) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858150 (0.000806) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858209 (0.000963) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.854142 (0.000361) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.858531 (0.001091) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.858618 (0.000903) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.854317 (0.000230) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.857565 (0.000725) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.857741 (0.000769) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.853264 (0.000981) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.854961 (0.001287) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.855019 (0.001233) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.848670 (0.000373) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.849109 (0.000457) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.849168 (0.000365) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853615 (0.000675) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858033 (0.000945) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858062 (0.000881) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853791 (0.000473) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.858150 (0.001192) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.858179 (0.001098) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.854054 (0.000172) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.857243 (0.000722) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.857448 (0.000688) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.853147 (0.001085) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.854522 (0.000996) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.854522 (0.001045) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847236 (0.000423) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846915 (0.000150) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846768 (0.000113) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.850718 (0.000349) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854083 (0.000969) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.853878 (0.000846) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851859 (0.000230) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.855897 (0.000778) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855985 (0.000778) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.852913 (0.000430) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.855488 (0.000452) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.855605 (0.000432) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.852328 (0.000763) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.854025 (0.000880) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.854054 (0.000867) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847295 (0.000242) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846768 (0.000102) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846739 (0.000150) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.850894 (0.000441) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.854142 (0.000869) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.853820 (0.000840) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851859 (0.000185) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.856014 (0.000774) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.856014 (0.000704) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.853147 (0.000563) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.855663 (0.000293) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.855780 (0.000331) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.852474 (0.000840) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.853995 (0.000926) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.854083 (0.000908) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847090 (0.000251) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846710 (0.000081) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846622 (0.000179) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.850718 (0.000512) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.853995 (0.000852) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.853615 (0.000820) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851625 (0.000151) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.855663 (0.000809) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855751 (0.000801) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.853059 (0.000556) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.855224 (0.000635) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.855312 (0.000516) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.852357 (0.000966) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.853820 (0.000718) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.853849 (0.000759) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.846008 (0.000352) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846125 (0.000312) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846125 (0.000190) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849109 (0.000268) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851069 (0.000448) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.850835 (0.000448) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850250 (0.000744) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.853469 (0.000571) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.853352 (0.000422) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851655 (0.000387) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853908 (0.000651) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853908 (0.000658) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.851976 (0.000621) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.853001 (0.000850) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.853088 (0.000785) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.846008 (0.000266) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846066 (0.000300) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846037 (0.000260) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849109 (0.000200) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851069 (0.000468) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.850894 (0.000406) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850426 (0.000497) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.853586 (0.000462) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.853469 (0.000383) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851801 (0.000401) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853644 (0.000550) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853732 (0.000613) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.851976 (0.000808) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.853264 (0.000921) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.853322 (0.000880) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845978 (0.000230) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846037 (0.000260) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846008 (0.000222) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.848963 (0.000164) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.850660 (0.000489) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.850455 (0.000408) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850221 (0.000455) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.853176 (0.000693) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.853264 (0.000482) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851625 (0.000333) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853527 (0.000606) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853615 (0.000626) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.851713 (0.000840) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.852942 (0.000829) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.853059 (0.000870) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB cross-validated F-1 score with grid search: 0.8683\n",
      "Confusion matrix:\n",
      "[[ 109   36  684]\n",
      " [  47  130  319]\n",
      " [  31    8 7181]]\n",
      "\n",
      "Wall time: 2h 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_NB = clf_GridSearchCV(nb, data, labels, nb_param_grid)    # parameter grid search NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC F-1 score:   0.8728\n",
      "Confusion matrix:\n",
      "[[ 201   62  566]\n",
      " [ 105  204  187]\n",
      " [ 136   31 7053]]\n",
      "\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC()\n",
    "svc_param_grid = {\n",
    "    'vect__max_df':[0.25,0.5,0.75],\n",
    "    'vect__min_df':[5,15,25,50,100],\n",
    "    'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "    'clf__C':[0.1,0.25,0.5,0.75,1.0]\n",
    "}\n",
    "clf_pipe(svc, data, labels)            # pipeline SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of LinearSVC: 0.8795 +/- 0.0113\n",
      "Confusion matrix:\n",
      "[[ 201   62  566]\n",
      " [ 105  204  187]\n",
      " [ 136   31 7053]]\n",
      "\n",
      "Wall time: 53.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(svc, data, labels)             # cross_val_score SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.878895 using {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867718 (0.001328) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.866197 (0.001418) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.866080 (0.001211) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868625 (0.001090) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.870644 (0.001855) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.870498 (0.001694) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.869415 (0.000987) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.871463 (0.002131) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.871200 (0.002200) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.869327 (0.001042) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.872019 (0.002054) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.871990 (0.002003) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.868479 (0.001581) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.870527 (0.001379) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.870468 (0.001522) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.869503 (0.001654) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867864 (0.001857) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867894 (0.001325) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.871083 (0.001476) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.872136 (0.001770) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.872019 (0.001874) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.871668 (0.001300) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.873190 (0.001990) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873219 (0.002249) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.872195 (0.001687) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.873453 (0.001663) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.873453 (0.001675) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.871961 (0.001228) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.872429 (0.001171) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.872400 (0.001149) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.869532 (0.001613) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867689 (0.001946) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867660 (0.001720) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.871229 (0.001574) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.872224 (0.002080) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.872283 (0.001938) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.871814 (0.001391) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.873248 (0.001879) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.873131 (0.002239) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.872312 (0.001086) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.873073 (0.001336) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.873160 (0.001448) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.872136 (0.001465) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.872341 (0.001235) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.872370 (0.001254) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.873746 (0.002287) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.875764 (0.001787) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.875794 (0.001593) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.875238 (0.001541) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.877578 (0.002253) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.877286 (0.001866) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.874243 (0.001276) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.877666 (0.001969) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.877666 (0.002058) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.873892 (0.001656) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.876525 (0.001235) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.876350 (0.001272) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.871990 (0.001528) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.874155 (0.001693) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.874389 (0.001699) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.876701 (0.001358) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.876467 (0.001606) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.876671 (0.002030) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.877110 (0.001707) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.878222 (0.002022) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.877725 (0.002097) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.877198 (0.002314) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.878281 (0.001901) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.878105 (0.001991) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.876613 (0.001908) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.877608 (0.001359) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.877696 (0.001288) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.875267 (0.001166) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.876262 (0.000968) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.876320 (0.000929) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.876584 (0.001397) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.876496 (0.001558) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.876701 (0.001714) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.877286 (0.001961) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.878193 (0.001678) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.877900 (0.001962) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.877315 (0.002085) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.878368 (0.002111) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.878222 (0.002046) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.876525 (0.002084) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.877754 (0.001163) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.877813 (0.001238) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.875618 (0.000998) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.876291 (0.001030) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.876350 (0.001063) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.873424 (0.001116) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.877754 (0.001769) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.877374 (0.001238) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.874594 (0.001714) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.877257 (0.001247) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.877169 (0.001306) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873746 (0.000860) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.876613 (0.001918) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.876671 (0.001752) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.873394 (0.001852) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.876876 (0.000799) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.876847 (0.000821) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.872253 (0.001772) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.874389 (0.001167) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.874331 (0.001226) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.876525 (0.001258) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.878456 (0.001499) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.878544 (0.001271) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.877637 (0.001378) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.877783 (0.000868) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.877813 (0.000998) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.877842 (0.001693) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.877988 (0.001640) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.877637 (0.001692) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.877461 (0.001985) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.877286 (0.001445) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.877227 (0.001462) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.876145 (0.001375) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.876496 (0.001610) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.876642 (0.001617) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.877052 (0.001085) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.878573 (0.001178) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.878895 (0.001177) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.877461 (0.001566) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.877959 (0.000964) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.878105 (0.001029) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.877900 (0.001475) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.877666 (0.001606) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.877432 (0.001624) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.877491 (0.002130) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.877608 (0.001626) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.877666 (0.001723) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.875852 (0.001561) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.876584 (0.001637) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.876642 (0.001713) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.872136 (0.000986) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.876174 (0.001485) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.876613 (0.000745) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.871990 (0.001293) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.875384 (0.001486) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.875677 (0.001784) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.871522 (0.001053) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.873804 (0.001733) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.874038 (0.001753) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.871639 (0.001520) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.874916 (0.000439) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.874799 (0.000731) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.872078 (0.002020) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.873892 (0.001343) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.873863 (0.001254) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.874711 (0.000665) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.877725 (0.001234) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.877696 (0.000737) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.875150 (0.001012) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.876496 (0.001568) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.876818 (0.001899) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.875764 (0.000424) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.875472 (0.001294) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.875560 (0.001314) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.876028 (0.001288) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.875647 (0.001087) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.875911 (0.001449) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.876467 (0.001448) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.876057 (0.001396) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.876086 (0.001381) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.875062 (0.000541) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.878017 (0.001000) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.878164 (0.000314) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.875413 (0.001198) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.876467 (0.001385) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.877081 (0.001411) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.876145 (0.000513) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.875677 (0.001197) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.875735 (0.001362) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.875969 (0.001699) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.875764 (0.001111) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.875677 (0.001477) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.876145 (0.001444) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.876028 (0.001716) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.876086 (0.001670) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.869795 (0.000639) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.875677 (0.001693) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.876086 (0.000847) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.869591 (0.001883) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.872926 (0.001234) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.872985 (0.001566) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.869474 (0.000601) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.871112 (0.001525) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.871961 (0.001806) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.870703 (0.001894) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.873511 (0.000506) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.873570 (0.000662) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.871375 (0.002315) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.873541 (0.000918) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.873336 (0.000926) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.872634 (0.000655) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.876701 (0.002063) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.876467 (0.000892) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.872692 (0.000797) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.874301 (0.001877) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.873746 (0.001612) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873394 (0.000585) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.872487 (0.000848) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.872283 (0.000999) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.874536 (0.001008) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.873863 (0.001238) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.873716 (0.000981) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.875940 (0.001603) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.875618 (0.001665) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.875589 (0.001678) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.873014 (0.000760) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.876847 (0.001909) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.876993 (0.000999) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.872721 (0.001161) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.874448 (0.001775) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.874038 (0.001396) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.873628 (0.000414) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.872517 (0.001229) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.872400 (0.001359) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.874360 (0.001249) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.873511 (0.001375) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.873687 (0.001073) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.876320 (0.001281) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.875589 (0.001591) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.875238 (0.001731) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC cross-validated F-1 score with grid search: 0.8765\n",
      "Confusion matrix:\n",
      "[[ 186   53  590]\n",
      " [  84  192  220]\n",
      " [  84   24 7112]]\n",
      "\n",
      "Wall time: 2h 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_SVM = clf_GridSearchCV(svc, data, labels, svc_param_grid)    # parameter grid search SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Running Naive Bayes and SVM with the Best Parameters from Grid Search on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full dataset and labels\n",
    "full_data = df_text['reviewText'].values\n",
    "full_labels = df_text['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Naive Bayes F-1 score on full dataset: 0.8810\n",
      "Confusion matrix:\n",
      "[[  5254   1553  12456]\n",
      " [  1816   5617   4028]\n",
      " [  3113    418 162269]]\n",
      "\n",
      "The best SVM F-1 score on full dataset: 0.8928\n",
      "Confusion matrix:\n",
      "[[  5617   2005  11641]\n",
      " [  1891   6785   2785]\n",
      " [  2244    500 163056]]\n",
      "\n",
      "Wall time: 22min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the two best classifier on it\n",
    "for best_clf in [best_NB, best_SVM]:\n",
    "        \n",
    "    # split into train and test sets\n",
    "    trainX, testX, trainY, testY = train_test_split(full_data, full_labels, test_size = 0.2, random_state = 43)\n",
    "    clf = best_clf.fit(trainX, trainY)\n",
    "    \n",
    "    # predict and compute metrics\n",
    "    predictions = clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('The best {} F-1 score on full dataset: {:0.4f}'.format('Naive Bayes' if best_clf==best_NB else 'SVM', score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
