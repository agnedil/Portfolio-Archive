{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Disambiguator Using Wordnet Synsets\n",
    "https://github.com/kevincobain2000/sentiment_classifier/blob/master/scripts/senti_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "import nltk\n",
    "import argparse\n",
    "import operator\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "import cPickle as pickle\n",
    "from pkg_resources import resource_string, resource_stream\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Word Disambiguator using nltk\n",
    "Sentiment Classifier as a combination of\n",
    "  -Bag of Words (nltk movie review corpus, words as features)\n",
    "  -Heuristics\n",
    "  \n",
    "--KATHURIA Pulkit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def word_similarity(word1, word2):\n",
    "    w1synsets = wn.synsets(word1)\n",
    "    w2synsets = wn.synsets(word2)\n",
    "    maxsim = 0\n",
    "    for w1s in w1synsets:\n",
    "        for w2s in w2synsets:\n",
    "            current = wn.path_similarity(w1s, w2s)\n",
    "            if (current > maxsim and current > 0):\n",
    "                maxsim = current\n",
    "    return maxsim\n",
    "\n",
    "\n",
    "def disambiguateWordSenses(sentence, word):\n",
    "    wordsynsets = wn.synsets(word)\n",
    "    bestScore = 0.0\n",
    "    result = None\n",
    "    for synset in wordsynsets:\n",
    "        for w in nltk.word_tokenize(sentence):\n",
    "            score = 0.0\n",
    "            for wsynset in wn.synsets(w):\n",
    "                sim = wn.path_similarity(wsynset, synset)\n",
    "                if sim is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    score += sim\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                result = synset\n",
    "    return result\n",
    "\n",
    "\n",
    "def SentiWordNet_to_pickle(swn):\n",
    "    synsets_scores = defaultdict(list)\n",
    "    for senti_synset in swn.all_senti_synsets():\n",
    "        if senti_synset.synset.name not in synsets_scores:\n",
    "            synsets_scores[senti_synset.synset.name] = defaultdict(float)\n",
    "        synsets_scores[senti_synset.synset.name]['pos'] += senti_synset.pos_score\n",
    "        synsets_scores[senti_synset.synset.name]['neg'] += senti_synset.neg_score\n",
    "    return synsets_scores\n",
    "\n",
    "\n",
    "def classify(text, synsets_scores, bag_of_words):\n",
    "    #synsets_scores = pickled object in data/SentiWN.p\n",
    "    pos = neg = 0\n",
    "    for line in text:\n",
    "        if not line.strip() or line.startswith('#'):\n",
    "            continue\n",
    "        for sentence in line.split('.'):\n",
    "            sentence = sentence.strip()\n",
    "            sent_score_pos = sent_score_neg = 0\n",
    "            for word in sentence.split():\n",
    "                if disambiguateWordSenses(sentence, word):\n",
    "                    disamb_syn = disambiguateWordSenses(sentence, word).name\n",
    "                    if disamb_syn in synsets_scores:\n",
    "                        #uncomment the disamb_syn.split... if also want to check synsets polarity\n",
    "                        if word.lower() in bag_of_words['neg']:\n",
    "                            sent_score_neg += synsets_scores[disamb_syn]['neg']\n",
    "                        if word.lower() in bag_of_words['pos']:\n",
    "                            sent_score_pos += synsets_scores[disamb_syn]['pos']\n",
    "            pos += sent_score_pos\n",
    "            neg += sent_score_neg\n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "senti_pickle = resource_stream('senti_classifier', 'data/SentiWn.p')\n",
    "bag_of_words_pickle = resource_stream('senti_classifier', 'data/bag_of_words.p')\n",
    "synsets_scores = pickle.load(senti_pickle)\n",
    "bag_of_words = pickle.load(bag_of_words_pickle)\n",
    "bag_of_words = classify_polarity(bag_of_words)\n",
    "\n",
    "\n",
    "def polarity_scores(lines_list):\n",
    "    scorer = defaultdict(list)\n",
    "    pos, neg = classify(lines_list, synsets_scores, bag_of_words)\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #print polarity_scores(['Excellent','Worst'])\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser = argparse.ArgumentParser(description='Sentiment classification')\n",
    "    parser.add_argument('-c', '--classify', action=\"store\",\n",
    "                        nargs='*', dest=\"files\", type=argparse.FileType('rt'),\n",
    "                        help='-c reviews')\n",
    "    myarguments = parser.parse_args()\n",
    "    if not myarguments.files:\n",
    "        parser.print_help()\n",
    "        exit(\"Documentation: %s\" % __documentation__)\n",
    "    for file in myarguments.files:\n",
    "        tpos = 0\n",
    "        tneg = 0\n",
    "        for lineno, line in enumerate(file.readlines()):\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            r = re.compile(\"[,.?()\\\\d]+ *\")\n",
    "            lines_list = r.split(line)\n",
    "            pos, neg = polarity_scores(lines_list)\n",
    "            print('{0:<40}... pos = {1:<5} \\tneg = {2:<5}'.format(str(lineno)+'. ' + line[:20],pos,neg))\n",
    "            tpos += pos\n",
    "            tneg += neg\n",
    "        print('-' * 75)\n",
    "        if tpos > tneg:\n",
    "            positive = file.name + ' ' + 'is Positive'\n",
    "            print('{0:<40}... pos = {1:<5} \\tneg = {2:<5}'.format(positive, tpos, tneg))\n",
    "        else:\n",
    "            negative = file.name + ' ' + 'is Negative'\n",
    "            print('{0:<40}... pos = {1:<5} \\tneg = {2:<5}'.format(negative, tpos, tneg))\n",
    "        print('Overall score of document\\nTotal Pos = %s\\nTotal Neg = %s'%(tpos, tneg))\n",
    "        print('-'*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEE FAVORITES FOR CURATED LINKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYWSD\n",
    "https://github.com/alvations/pywsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Simple Lesk Example + Wordnet Interface\n",
    "https://stackoverflow.com/questions/20896278/word-sense-disambiguation-algorithm-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSD using online encyclopedia\n",
    "https://github.com/liuhuanyong/WordMultiSenseDisambiguation/blob/master/wordsense_detect.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "https://github.com/Jeff09/Word-Sense-Disambiguation-using-Bidirectional-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSD Based On RDF Graph with Pyrhon Wrapper\n",
    "https://github.com/wastl/disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well-Written Simplified Lesk Algorithm (Wordnet)\n",
    "https://github.com/dropofwill/word-sense-disambiguation/blob/master/wordnet_lesk.py  \n",
    "COMPARE WITH PyWSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Language Experiments\n",
    "https://github.com/alexrudnick/chipa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
