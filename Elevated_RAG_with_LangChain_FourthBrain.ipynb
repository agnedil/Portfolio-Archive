{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agnedil/Portfolio/blob/master/Elevated_RAG_with_LangChain_FourthBrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elevated RAG with LangChain - FourthBrain\n",
        "\n",
        "In the following notebook, we'll examine a brief introduction to Retrieval Augmented Generation using LangChain! After that we'll focus on two major areas of possible improvement:\n",
        "\n",
        "1. Different embedding models and reranking\n",
        "2. Hybrid Retrieval stacks\n",
        "\n",
        "We'll also be using the LangChain Expression Language to build our solutions.\n",
        "\n",
        "LCEL is a production ready style of building and prototyping chains. With automatic async and built-in parallelization, LCEL ensures you're ready for production with very little developer-side lift!\n",
        "\n",
        "To get started, as always, we have to grab some dependencies and decide on some data!\n",
        "\n",
        "> NOTE: While we're going to be leveraging OpenAI/Cohere's endpoints for this demonstration - you could use any number of closed-source APIs, or open-source self-hosted models as a substitute."
      ],
      "metadata": {
        "id": "2jMUqvjq83TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies and API Keys\n",
        "\n",
        "We'll be leveraging OpenAI's `gpt-4-1106-preview` model today, and Cohere's `embedv3` embeddings.\n",
        "\n",
        "So we'll need to get both dependencies, as we as provide an API key!"
      ],
      "metadata": {
        "id": "6wJRT2dt-NLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2fP5wgE80G9",
        "outputId": "b64b5a19-8c38-453d-913e-4bc6bab7c4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.6/808.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai cohere tiktoken -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZWUUhOC9--Y",
        "outputId": "a2698c68-2217-400f-a07f-1ca32c9ef526"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkEJFehh-kG2",
        "outputId": "888e9d91-1f9e-4200-e240-2eff3a5ff37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Cohere API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model Access with LangChain\n",
        "\n",
        "Now that we have our dependencies, we can begin construction a basic RAG chain!"
      ],
      "metadata": {
        "id": "Uo9D8MVr_-Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Existing LLM Performance on our Domain\n",
        "\n",
        "Before we jump into RAG, let's see how our LLM does out of the box to see if we even need to do RAG in the first place!\n",
        "\n",
        "We'll do this by setting up a simple chain that will let us query our LLM.\n",
        "\n",
        "The domain I've selected today is World of Warcraft lore - it's a fairly niche topic, and might not be something that OpenAI's `gpt-4-1106-preview` is great at!\n",
        "\n",
        "Let's set up our simple QA chain."
      ],
      "metadata": {
        "id": "A9kn3rN9BFAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model\n",
        "\n",
        "We'll be using GPT-4 Preview as discussed - and we'll be setting a few parameters:\n",
        "\n",
        "- `model` - this allows us to specify our model\n",
        "- `temperature` - this will let us control how \"creative\" we want our model to be. Since we'll be using this as a factual retiever, we'll set this to a low value.\n",
        "- `model_kwargs` -> `seed` - setting the seed will let us ensure consistency across sessions!"
      ],
      "metadata": {
        "id": "NaSqa_E0Bo0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, model_kwargs={\"seed\" : 1337})"
      ],
      "metadata": {
        "id": "Fzs3lNIFBblc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt Template\n",
        "\n",
        "Since we need to pass in user-defined questions to our RAG chain, we'll want to set up a simple prompt template."
      ],
      "metadata": {
        "id": "RqppxPlwCSal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"{content}\")"
      ],
      "metadata": {
        "id": "sRBAzkgOCfUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output Parser\n",
        "\n",
        "If we look at our LLM - we'll notice that it's outputs are Message objects - we can convert the response into a `str` by chaining a `StrOutputParser` at the end.\n",
        "\n",
        "In the following cell's we'll explore how to look at output and input schema and then set-up our string-output parser."
      ],
      "metadata": {
        "id": "v_8zLqT5DD99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "str_output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "9FkFdgolDjc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see our inputs and outputs of each components in our chain to make sure they're compatible!"
      ],
      "metadata": {
        "id": "WDrSl4TODqJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM:"
      ],
      "metadata": {
        "id": "74bUloBUEJj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.input_schema.schema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFurPrOkECk-",
        "outputId": "56cfb4b0-99a5-4a96-9f13-bbaeae50d7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'ChatOpenAIInput', 'anyOf': [{'type': 'string'}, {'$ref': '#/definitions/StringPromptValue'}, {'$ref': '#/definitions/ChatPromptValueConcrete'}, {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}]}}], 'definitions': {'StringPromptValue': {'title': 'StringPromptValue', 'description': 'String prompt value.', 'type': 'object', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'title': 'Type', 'default': 'StringPromptValue', 'enum': ['StringPromptValue'], 'type': 'string'}}, 'required': ['text']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}, 'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete', 'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.', 'type': 'object', 'properties': {'messages': {'title': 'Messages', 'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}]}}, 'type': {'title': 'Type', 'default': 'ChatPromptValueConcrete', 'enum': ['ChatPromptValueConcrete'], 'type': 'string'}}, 'required': ['messages']}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.output_schema.schema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrlld0fyEE5U",
        "outputId": "1f8cb5fe-9dcb-4ff9-d56f-b5e1c7d80daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'ChatOpenAIOutput', 'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}], 'definitions': {'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrOutputParser:"
      ],
      "metadata": {
        "id": "2VTYiyQvEK8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(str_output_parser.input_schema.schema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQN0tlVND6XS",
        "outputId": "18eb2814-86a2-4faf-a0c3-023b321bed8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'StrOutputParserInput', 'anyOf': [{'type': 'string'}, {'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}], 'definitions': {'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str_output_parser.output_schema.schema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z5i1PO_D1-s",
        "outputId": "6fa1ac74-c6ce-4153-bb8f-ec6f0acf2bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'StrOutputParserOutput', 'type': 'string'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, all of our input and output's line up well - so we're good to go to construct our chain!"
      ],
      "metadata": {
        "id": "2QHw1yTmEP0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic Chain\n",
        "\n",
        "Now that we have our components, and we've checked they're compatible, we can build our chain.\n",
        "\n",
        "With LCEL - building a chain has never been easier!"
      ],
      "metadata": {
        "id": "vPok31yHEWCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | str_output_parser"
      ],
      "metadata": {
        "id": "vCeujhGUEvsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it!\n",
        "\n",
        "Let's test our chain and see how it does on some common Warcraft lore questions."
      ],
      "metadata": {
        "id": "YyoZFOecEzuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"content\" : \"In World of Warcraft - who is Fyrakk?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "PjPoBTPIE7Pl",
        "outputId": "3fd3044b-dede-4232-991b-aa8dbfefd92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As of my last update in April 2023, Fyrakk is not a widely recognized character in the lore of World of Warcraft. It's possible that Fyrakk could be a minor character, a new addition to the game that was introduced after my knowledge cutoff, or a character from a specific quest or instance that is not central to the main storyline.\\n\\nWorld of Warcraft is a constantly evolving game with regular updates, expansions, and patches that introduce new content, characters, and storylines. If Fyrakk is a character that was added after April 2023, I would not have information on them.\\n\\nTo get the most accurate and up-to-date information about Fyrakk, you should check the latest World of Warcraft patch notes, the official forums, or the WoW community resources such as Wowhead or the WoW Wiki. These sources are frequently updated with new information as it becomes available and can provide details on characters, quests, and lore that have been recently added to the game.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, Fyrakk is the current major threat being faced by the Heroes of Azeroth!\n",
        "\n",
        "We'll need to add some additional data in order to ensure our application is able to answer even the most current questions!"
      ],
      "metadata": {
        "id": "6gVws0BTFYLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation with LangChain - Simple Implementation\n",
        "\n",
        "Now that we see how the base solution underperforms - we'll implement a simple RAG chain to boost the performance and allow our application to have an understanding of even the most current lore!"
      ],
      "metadata": {
        "id": "j_s_mSkiKTnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collection and Parsing\n",
        "\n",
        "Before we construct a RAG chain, we'll need to source some data that we wish to perform RAG over.\n",
        "\n",
        "We could use the webpage dump from WoWPedia to achieve this goal - but there are over 500,000 pages of content - so we'll limit ourselves to a small subset of the lore, in this case: Information about the Primal Incarnates that were introduced in the Dragonflight Expansion."
      ],
      "metadata": {
        "id": "bxTD8u4SAftY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading Data from Webpages\n",
        "\n",
        "We will be using the `UnstructuredURLLoader` - powered by [Unstructured](https://unstructured.io/) to load our documents from their respective web-pages - keep in mind that, given an `.xml` sitemap - we could download ever page in an automated fashion - but we're limited the source data for this demo specifically."
      ],
      "metadata": {
        "id": "4HP049c6JqcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "nm6IPS8TJ_r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saeT2xNVMfcD",
        "outputId": "7f5ba652-4e3b-4045-b6b0-7b985d3c56eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "URL_LIST = [\n",
        "    \"https://wowpedia.fandom.com/wiki/Primal_Incarnates\",\n",
        "    \"https://wowpedia.fandom.com/wiki/Raszageth\",\n",
        "    \"https://wowpedia.fandom.com/wiki/Iridikron\",\n",
        "    \"https://wowpedia.fandom.com/wiki/Vyranoth\",\n",
        "    \"https://wowpedia.fandom.com/wiki/Fyrakk\"\n",
        "]"
      ],
      "metadata": {
        "id": "OBQ0WTgTJb_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredURLLoader(urls=URL_LIST)"
      ],
      "metadata": {
        "id": "uv5fMF5HMaRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CZbiZ7lMxL-",
        "outputId": "7db974f0-8f4c-4d78-f5af-bc640aed2f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfbRnU2rNHL7",
        "outputId": "5437da5d-8d91-4327-8c21-1e501dc46197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting/Chunking\n",
        "\n",
        "Now that we have our 5 web-pages - we want to split them into smaller bite-sized pieces to be used in our Retrieval Pipeline.\n",
        "\n",
        "We'll use the naive solution of the `RecursiveCharacterTextSplitter` first, which will simply split our documents recursively by a set of predefined characters.\n",
        "\n",
        "This is a great strategy for simple text documents and more - but can be easily upgraded to a more performant solution."
      ],
      "metadata": {
        "id": "VvWQVcmENJ1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0,\n",
        "    model_name = \"gpt-4-1106-preview\"\n",
        ")"
      ],
      "metadata": {
        "id": "leqXwFbqNouG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_documents = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "uVbdvXiaN9NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVJEyhS3OKZn",
        "outputId": "6c655a3f-5ef4-4271-cef9-c6387658a609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've created a set of 139 documents from our original 5 web-pages - and we can use those, in combination with a VectorStore, to retrieve appropriate context for our questions!"
      ],
      "metadata": {
        "id": "Bct4ubcGOMhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embeddings Model\n",
        "\n",
        "Now that we've chunked our documents, we'll need to vectorize them and move them into a VectorStore - a place that will associate Vectors with Text Chunks.\n",
        "\n",
        "We'll be using OpenAI's `text-embedding-ada-002` to start, which is a fine place to start if you're ever searching for a solution that will work great out of the box for a relatively low cost."
      ],
      "metadata": {
        "id": "yrjXd6--OqRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings_oai = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "rFwkfBQzPEVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VectorStore - FAISS\n",
        "\n",
        "We'll be using the simple FAISS VectorStore today - though you could substitute this for any VectorStore or Vector Database that you prefer!"
      ],
      "metadata": {
        "id": "hFHS7nNtOZYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os6ODWctOYyN",
        "outputId": "8b231338-db5f-4057-f37e-be4fac5bd2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(split_documents, embeddings_oai)"
      ],
      "metadata": {
        "id": "Ams0Wi6JOkSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retriever\n",
        "\n",
        "Now that we have a VectorStore - we'll need to convert it to a retriever. Luckily, this is a straight forward process with LangChain!"
      ],
      "metadata": {
        "id": "J-mjxDcpPOD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "mToEk7vcPUhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Prompt Template\n",
        "\n",
        "Now that we have a few of our base components:\n",
        "\n",
        "1. LLM\n",
        "2. Retriever\n",
        "\n",
        "We need to create a PromptTemplate that will let us instruct our LLM in how to use what we provide to it!"
      ],
      "metadata": {
        "id": "yBzXX9yePYeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Use the following context to answer the user's questions. If you don't know the answer, please respond with 'I don't know'.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "7gffCQ9wPrHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up RAG chain\n",
        "\n",
        "With that, we finally have everything we need!\n",
        "\n",
        "We have:\n",
        "\n",
        "1. Our Retriever (FAISS-backed VectorStore Retriever with WoWPedia web-pages)\n",
        "2. Our Augmentor (PromptTemplate with context and question format options)\n",
        "3. Our Generator (`gpt-4-1106-preview`)\n",
        "\n",
        "Let's throw them into a chain!"
      ],
      "metadata": {
        "id": "nngGbGuwP9mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "entry_point_and_retriever = RunnableParallel(\n",
        "    {\n",
        "        \"context\" : retriever,\n",
        "        \"question\" : RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_chain = entry_point_and_retriever | rag_prompt | llm | str_output_parser"
      ],
      "metadata": {
        "id": "hEwobHg7QR-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"In World of Warcraft - who is Fyrakk?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "d6-3_GYpQrsH",
        "outputId": "92ed0ea8-a072-41d7-8504-8acc5a7cc047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fyrakk is a proto-dragon with the title <The Blazing>. He is male and belongs to the dragonkin race. Fyrakk is a boss-level character with an unknown level, and his resource is mana. He is affiliated with the Primal Incarnates and the Primalists. His location is the Vault of the Incarnates, and according to the lore, he is deceased but still killable in the game. Fyrakk is known for his mastery over fire and Shadowflame, and he was ultimately defeated before he could corrupt the core of Amirdrassil, a World Tree. He is also voiced by Matthew Mercer in the game.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! Now we've added current data to our application - but we can still do better!\n",
        "\n",
        "Let's take this RAG application to the next level!"
      ],
      "metadata": {
        "id": "eNV2v4J1Q9yF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elevated RAG with LangChain\n",
        "\n",
        "Now it's time to apply a few basic patterns that will substantially improve your RAG application.\n",
        "\n",
        "We'll do 2 major things:\n",
        "\n",
        "1. Combine our dense vector search retrieval with sparse search to provide a better range of context.\n",
        "2. Add a Reranker to our retrieved context to ensure we have the most relevant information."
      ],
      "metadata": {
        "id": "eJNkTuluRJ4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Retrieval\n",
        "\n",
        "We'll be using a strategy called \"hybrid retrieval\" to improve our Retrieval Augmented Generation pipeline today.\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Dense Vector Search is very good at retrieving semantically related context.\n",
        "2. Sparse Search is great at retrieving context based on keywords.\n",
        "\n",
        "Dense Vector Search can over-index on semantic relatedness due to noise within the user's query - and sparse search can miss obvious connections because of different keywords.\n",
        "\n",
        "By their powers combined - we can build a better system!\n",
        "\n",
        ">NOTE: In preparation for the next step - we will be retrieving 10 documents."
      ],
      "metadata": {
        "id": "qBPCrkX1TCaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 -qU"
      ],
      "metadata": {
        "id": "XJ5Jt6hQUhYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever"
      ],
      "metadata": {
        "id": "6grQFX_xUWgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
        "bm25_retriever.k = 5"
      ],
      "metadata": {
        "id": "Lsh5-05YUdJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import CohereEmbeddings\n",
        "\n",
        "cohere_embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
        "\n",
        "faiss_vectorstore = FAISS.from_documents(split_documents, cohere_embeddings)\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "bgLpvkd7UrLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.75, 0.25]\n",
        ")"
      ],
      "metadata": {
        "id": "gvbI3JBiVO21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now we can move onto the next step which will let us leverage our newly created retriever with a reranker!"
      ],
      "metadata": {
        "id": "f-GxNRNhVXXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reranking with LangChain\n",
        "\n",
        "For the most part, you can think of reranking as follows:\n",
        "\n",
        "1. Retrieve a large number of potential documents using a computationally efficient method.\n",
        "2. Rerank the retrieved contexts using a computationally more expensive method with greater effectiveness.\n",
        "3. Provide the `top_k` reranked documents as context to the LLM.\n",
        "\n",
        "Essentially, this lets us grab a large pool of potentially relevant documents, and then rerank them based on a more performant method that is more expensive - but due to reranker over a small subset of our total documents, we wind up retaining overall performance of our application.\n",
        "\n",
        "Thanks to LangChain and Cohere - this is a simple process to implement!"
      ],
      "metadata": {
        "id": "USLMuNJQRkA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "reranker = CohereRerank(top_n=5)"
      ],
      "metadata": {
        "id": "1GQaTIuASZPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker, base_retriever=ensemble_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "gM5_OZIhSnBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Now we have a reranker backed by a hybrid retriever!\n",
        "\n",
        "Let's see how this does in a new chain!"
      ],
      "metadata": {
        "id": "54MhzxckVeDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elevated RAG Chain\n",
        "\n",
        "We'll use exactly the same process to build our chain as we did before!"
      ],
      "metadata": {
        "id": "AxVnU9lZVj_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entry_point_and_elevated_retriever = RunnableParallel(\n",
        "    {\n",
        "        \"context\" : rerank_retriever,\n",
        "        \"question\" : RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "elevated_rag_chain = entry_point_and_elevated_retriever | rag_prompt | llm | str_output_parser"
      ],
      "metadata": {
        "id": "RucgiM09VjW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elevated_rag_chain.invoke(\"In World of Warcraft - who is Fyrakk?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "tIywBFmKVxqL",
        "outputId": "57d6fda7-3a34-458b-d5a0-9f222f4e2e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Fyrakk, also known as Fyrakk the Blazing, is a character in World of Warcraft. He was imprisoned within the Vault of the Incarnates in Thaldraszus until he was released by Raszageth. Fyrakk is a Primal Incarnate who was given a powerful new weapon, the axe Fyr'alath, the Dream Render, which allowed him and his forces to invade the Emerald Dream. He led a siege against the World Tree but faced obstacles such as the Temple that barred his path to the tree's core. Despite his efforts and the use of his mastery over fire and Shadowflame, Fyrakk was ultimately defeated by Azeroth's heroes and leaders at the heart of Amirdrassil, before he could corrupt the World Tree's core. His status is deceased.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}