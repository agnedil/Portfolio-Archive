{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. ReviewText + asin (Product ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\n",
      "Scikit-learn 0.20.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "#nltk.download('stopwords')    # this is done just once\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from platform import python_version\n",
    "print('Python {}'.format(python_version()))\n",
    "print('Scikit-learn {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in json, combine reviewText and asin columns, get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 982619\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path=''\n",
    "file='kindle_reviews.json'\n",
    "df = pd.read_json(path_or_buf=path+file, lines=True, encoding='utf-8')    #, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, chunksize=None, compression='infer')\n",
    "print('Length of text: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1F6404F1VG29J</td>\n",
       "      <td>Avidreader</td>\n",
       "      <td>Nice vintage story</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "      <td>01 6, 2014</td>\n",
       "      <td>AN0N05A9LIJEQ</td>\n",
       "      <td>critters</td>\n",
       "      <td>Different...</td>\n",
       "      <td>1388966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "      <td>04 4, 2014</td>\n",
       "      <td>A795DMNCJILA6</td>\n",
       "      <td>dot</td>\n",
       "      <td>Oldie</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>A1FV0SX13TWVXQ</td>\n",
       "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
       "      <td>I really liked it.</td>\n",
       "      <td>1392768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "      <td>03 19, 2014</td>\n",
       "      <td>A3SPTOKDG7WBLN</td>\n",
       "      <td>Father Dowling Fan</td>\n",
       "      <td>Period Mystery</td>\n",
       "      <td>1395187200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  B000F83SZQ  [0, 0]        5   \n",
       "1  B000F83SZQ  [2, 2]        4   \n",
       "2  B000F83SZQ  [2, 2]        4   \n",
       "3  B000F83SZQ  [1, 1]        5   \n",
       "4  B000F83SZQ  [0, 1]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I enjoy vintage books and movies so I enjoyed ...   05 5, 2014   \n",
       "1  This book is a reissue of an old one; the auth...   01 6, 2014   \n",
       "2  This was a fairly interesting read.  It had ol...   04 4, 2014   \n",
       "3  I'd never read any of the Amy Brewster mysteri...  02 19, 2014   \n",
       "4  If you like period pieces - clothing, lingo, y...  03 19, 2014   \n",
       "\n",
       "       reviewerID                         reviewerName             summary  \\\n",
       "0  A1F6404F1VG29J                           Avidreader  Nice vintage story   \n",
       "1   AN0N05A9LIJEQ                             critters        Different...   \n",
       "2   A795DMNCJILA6                                  dot               Oldie   \n",
       "3  A1FV0SX13TWVXQ  Elaine H. Turley \"Montana Songbird\"  I really liked it.   \n",
       "4  A3SPTOKDG7WBLN                   Father Dowling Fan      Period Mystery   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1399248000  \n",
       "1      1388966400  \n",
       "2      1396569600  \n",
       "3      1392768000  \n",
       "4      1395187200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the \"asin\" column to reviewText\n",
    "df['reviewText'] = df['asin'].map(str) + ' ' + df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['reviewText', 'overall']].copy()        # copy only certain columns to another df\n",
    "df = None                                             # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ I enjoy vintage books and movies so...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ This book is a reissue of an old on...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ This was a fairly interesting read....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ I'd never read any of the Amy Brews...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ If you like period pieces - clothin...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  B000F83SZQ I enjoy vintage books and movies so...        5\n",
       "1  B000F83SZQ This book is a reissue of an old on...        4\n",
       "2  B000F83SZQ This was a fairly interesting read....        4\n",
       "3  B000F83SZQ I'd never read any of the Amy Brews...        5\n",
       "4  B000F83SZQ If you like period pieces - clothin...        4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert 5 ratings to 3 ('neg', 'mixed', and 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 5 ratings to 3 ('neg', 'mixed', and 'pos')\n",
    "df_text['overall'] = df_text['overall'].apply(lambda x: 'pos' if x > 3 else 'neg' if x < 3 else 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ I enjoy vintage books and movies so...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ This book is a reissue of an old on...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ This was a fairly interesting read....</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ I'd never read any of the Amy Brews...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ If you like period pieces - clothin...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText overall\n",
       "0  B000F83SZQ I enjoy vintage books and movies so...     pos\n",
       "1  B000F83SZQ This book is a reissue of an old on...     pos\n",
       "2  B000F83SZQ This was a fairly interesting read....     pos\n",
       "3  B000F83SZQ I'd never read any of the Amy Brews...     pos\n",
       "4  B000F83SZQ If you like period pieces - clothin...     pos"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the dataset is 982619\n"
     ]
    }
   ],
   "source": [
    "print('Total length of the dataset is {}'.format(len(df_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reduce size of data, get reduced-size dataset and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sample is 42722\n"
     ]
    }
   ],
   "source": [
    "# get a random sample from the dataframe whose size is manageable for cross-validation and grid search\n",
    "# with more computing resources and/or time, this can be done on a larger data set\n",
    "length = len(df_text)\n",
    "df_text = df_text.sample(n=length)\n",
    "df_text_short = df_text.sample(n=int(length/23))\n",
    "print('Length of the sample is {}'.format(len(df_text_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos      36042\n",
       "mixed     4244\n",
       "neg       2436\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique labels\n",
    "df_text_short.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our train set and labels\n",
    "data = df_text_short['reviewText'].values\n",
    "labels = df_text_short['overall'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Experimenting with different lists of stopwords and selecting the most efficient one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['namely', 'neither', 'amoungst', 'perhaps', 'whether', 'else', 'thick', 'such', 'toward', 'her', 'un', 'sincere', 'detail', 'back', 'serious', 'were', 'hereby', 'hundred', 'other', 'several', 'amount', 'this', 'into', 'own', 'ie', 'should', 'being', 'cannot', 'herein', 'without', 'through', 'least', 'above', 'across', 'nine', 'hence', 'until', 'per', 'on', 'nevertheless', 'must', 'either', 'what', 'anything', 'bottom', 'inc', 'be', 'most', 'sixty', 'though', 'at', 'each', 'would', 'another', 'thereupon', 'seems', 'whole', 'two', 'whatever', 'anywhere', 'mill', 'find', 'next', 'over', 'yourselves', 'done', 'first', 'hers', 'seem', 'many', 'whereafter', 'hasnt', 'whom', 'few', 'when', 'myself', 'so', 'now', 'much', 'onto', 'put', 'eg', 'we', 'its', 'thin', 'after', 'more', 'against', 'via', 'not', 'those', 'an', 'since', 'have', 'your', 'under', 'front', 'ours', 'twenty', 'by', 'nobody', 'whither', 'system', 'elsewhere', 'i', 'down', 'but', 'between', 'six', 'that', 'cant', 'throughout', 'always', 'cry', 'co', 'take', 'becomes', 'yet', 'ten', 'themselves', 'becoming', 'one', 'everyone', 'sometime', 'both', 'already', 'four', 'off', 'thus', 'may', 'towards', 'am', 'to', 'part', 'made', 'afterwards', 'become', 'around', 'bill', 'where', 'thru', 'they', 'beyond', 'sometimes', 'fifty', 'and', 'further', 'before', 'very', 'no', 'ourselves', 'behind', 'him', 'below', 'name', 'his', 'who', 'couldnt', 'during', 'de', 'upon', 'any', 'somehow', 'ever', 'never', 'of', 'then', 'whereas', 'itself', 'therefore', 'also', 'than', 'whence', 'almost', 'as', 'whenever', 'mine', 'has', 'anyhow', 'amongst', 'go', 'once', 'while', 'my', 'there', 'whose', 'ltd', 'up', 'seemed', 'every', 'except', 'call', 'whereby', 'yours', 'well', 'was', 'do', 'even', 'you', 'side', 'eight', 're', 'show', 'keep', 'otherwise', 'latter', 'with', 'indeed', 'are', 'move', 'interest', 'eleven', 'less', 'here', 'will', 'something', 'formerly', 'too', 'fifteen', 'latterly', 'the', 'about', 'again', 'anyone', 'within', 'she', 'it', 'found', 'how', 'these', 'fill', 'together', 'became', 'all', 'please', 'give', 'others', 'had', 'or', 'somewhere', 'everywhere', 'in', 'thereby', 'everything', 'our', 'wherever', 'a', 'me', 'wherein', 'etc', 'if', 'nowhere', 'same', 'three', 'which', 'third', 'none', 'nothing', 'former', 'thereafter', 'herself', 'see', 'whereupon', 'however', 'besides', 'might', 'from', 'twelve', 'hereupon', 'get', 'beside', 'can', 'last', 'often', 'enough', 'five', 'himself', 'still', 'con', 'meanwhile', 'full', 'describe', 'seeming', 'beforehand', 'mostly', 'noone', 'although', 'fire', 'hereafter', 'out', 'among', 'moreover', 'thence', 'yourself', 'therein', 'anyway', 'because', 'empty', 'top', 'only', 'rather', 'along', 'been', 'is', 'someone', 'why', 'whoever', 'nor', 'could', 'alone', 'due', 'he', 'forty', 'them', 'some', 'us', 'their', 'for']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "153\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['namely', 'shown', \"we'd\", 'neither', 'amoungst', 'perhaps', 'whether', 'ugh', 'else', 'thick', 'instead', 'such', 'toward', 'her', 'sideways', 'mr', 'underneath', 'un', 'spoke', \"isn't\", \"i've\", 'thou', 'upwards', 'sincere', \"you've\", 'detail', 'wherewith', 'back', 'serious', 'were', 'having', 'whereunto', 'hereby', 'hundred', 'other', 'several', 'amount', 'this', 'need', 'into', 'own', 'ie', 'should', 'being', 'slunk', 'cannot', 'herein', 'without', 'vs', 'through', 'least', 'above', \"won't\", 'spat', 'across', ',', 'hither', 'nine', \"you're\", 'slew', 'farthest', 'spake', 'hence', 'slung', 'y', 'until', 'per', 'on', \"she's\", 'nevertheless', 'must', 'either', 'whereto', \"'\", '-', 'what', 'anything', 'seen', 'bottom', \"let's\", 'inc', \"mustn't\", 'shouldn', \"that'll\", 'albeit', \"why's\", 'inasmuch', 'be', 'most', 'shan', 'inside', 'sixty', 'haven', 'though', 'whew', 'spoken', 'at', 'seeing', 'each', 'exclusive', \"should've\", \"didn't\", 'thereof', 'would', \"you'd\", 'everybody', 'another', 'thereupon', 'saw', 'seems', 'whole', \"they'll\", 'two', 'whatever', 'anywhere', 'mill', 'find', 'next', 'over', 'yourselves', 'according', 'done', 'first', 'slept', 'hers', 'wouldn', 'seem', 'seldom', 'whereinto', 'many', 'ain', 'whereafter', 'hasnt', 'whom', 'including', 'thereabout', \"hasn't\", 'use', \"here's\", \"aren't\", 'hasn', 'few', 'when', 'hereto', 'myself', 'forth', 'so', 'thenceforth', 'now', 'much', 'onto', 'hath', 'indoors', 'used', 'don', 'isn', 'put', 'eg', 'we', 'dost', \"they're\", 'its', 'double', 'thin', 'after', \"hadn't\", 'more', 'against', 'via', 'not', 'ought', '`', 'those', 'an', 'cu', 'worst', \"he'll\", 'since', 'hereabouts', 'sprang', 'won', 'have', 'your', 'under', 'front', 'ours', 'twenty', 'meantime', 'whichsoever', 'by', 'nobody', 'whither', 'system', 'kg', 'ff', 'nope', \"what's\", 'mustn', \"shan't\", 'elsewhere', 'i', 'down', 'but', 'couldn', 'between', 'six', 'that', 'cant', 'quite', 'throughout', 'always', 'cry', 'co', 'take', 'becomes', '\"', 'yet', 'ten', 'themselves', 'becoming', 'one', 'everyone', 'km', 'sometime', 'both', 'wherefrom', 'whatsoever', \"when's\", 'already', 'four', 'off', 'thus', 'may', 'towards', 'am', 'to', 'whereon', 'part', 'kind', 'unless', 'made', 'afterwards', 'day', 'does', 'exception', 'selves', 'become', 'around', 'bill', 'et', 'yippee', 'where', 'thru', 'they', 'beyond', 'sometimes', 'fifty', \"where's\", 'and', \"needn't\", 'further', 'nonetheless', 'thy', 'certain', 'before', \"i'll\", \"mightn't\", '!', 'very', 'no', 'ourselves', 'behind', \"i'm\", 'him', 'hindmost', 'below', 'name', 'hast', 'cos', 'his', 'till', 'who', 'couldnt', 'during', 'de', 'upon', \"they'd\", 'any', 'somehow', 'ever', 'want', 'never', 'of', 'then', 'whereas', 'itself', 'therefore', 'also', 'than', 'using', 'll', 'whence', 'almost', 'as', 'whenever', 'notwithstanding', 'week', 'mine', 'has', 'needn', 'anyhow', 'amongst', 'go', 'ms', 'aren', '/', 'once', 'while', 'canst', 'my', 'worse', \"there's\", '.', 'there', 'whose', 'ltd', 'up', 'seemed', 'every', 'except', 'theirs', 'call', 'whereby', 'yours', \"you'll\", \"couldn't\", 'thee', 'well', 'was', 'inwards', 'do', 'even', 'really', 'exclude', 'you', 'side', '?', 'eight', 're', 'choose', 'show', 'ma', 'whilst', 'o', '+', 'keep', 'otherwise', 'thereto', 'hardly', \"we've\", 'latter', 'with', 'indeed', 'are', 'move', 'interest', 'eleven', \"i'd\", 'did', 'less', 'here', 'smote', 'inward', 'will', 'something', \"it's\", 'insomuch', 'henceforth', 'plenty', 'formerly', 'too', 'forward', 'sent', \"she'll\", 'fifteen', 'unlike', 'latterly', 'lest', 'the', 'about', 'again', 'anyone', 'within', 'dual', 'furthermore', \"he's\", 'she', 'it', 'contrariwise', 'whereof', 'found', 'far', 'how', 'these', 'fill', 'together', 'became', 'whereat', 'thyself', 'all', 'please', 'supposing', 'sake', \"she'd\", 'give', 'others', 'maybe', 'had', 'or', 'weren', 'somewhere', \"weren't\", 'halves', 'everywhere', 'in', 'thereby', 'everything', 'our', 'wherever', 'wherefore', 'a', 'me', 'wherein', 'etc', 'if', 'provide', \"they've\", 'farther', 'nowadays', 'nowhere', 'thrice', 'same', 'three', 'wilt', 'didn', 's', 'which', 'let', 'whichever', 'excepted', 'stave', 'third', 'whosoever', 'none', 'excepting', 'nothing', 'former', 'thereafter', 'herself', 'whensoever', 'upward', 'see', 'whereupon', 'included', 'however', 'apart', \"that's\", 'whereabouts', 'besides', 'ye', 'might', 't', \"haven't\", 'doing', 'from', 'twelve', '*', 'somebody', 'hereupon', 'get', 'ok', 've', 'include', 'beside', 'outside', 'doth', 'can', 'last', 'often', 'furthest', 'enough', 'five', 'himself', 'still', 'con', 'meanwhile', 'excluding', 'unable', 'hadn', 'full', 'describe', 'seeming', 'beforehand', 'mrs', \"wouldn't\", 'like', 'shalt', 'mostly', 'noone', 'although', 'fire', 'hereafter', 'wow', \"shouldn't\", 'mightn', 'out', 'wasn', 'among', 'moreover', 'thence', 'thereon', 'yourself', 'therein', 'anyway', '``', 'wheresoever', 'anybody', 'hitherto', 'because', \"we'll\", 'empty', 'top', 'only', 'year', 'rather', 'round', 'along', 'save', 'sprung', 'little', 'staves', 'been', 'is', 'someone', \"how's\", \"don't\", \"we're\", 'whoa', 'why', 'whoever', 'thereabouts', \"wasn't\", 'whomever', 'cf', 'said', 'nor', 'whomsoever', 'could', 'd', \"he'd\", 'alone', \"doesn't\", \"''\", 'somewhat', 'howsoever', 'due', 'he', 'sang', 'm', 'forty', 'them', \"who's\", 'some', 'us', 'doesn', 'their', 'av', 'for', 'just']\n"
     ]
    }
   ],
   "source": [
    "# GENERATING A LIST OF STOPWORDS\n",
    "# these various stopword lists and the combined joint list were tested on the same classifier (MultinomialNB) with the same\n",
    "# parameters, and it was found that the lemur list and the combined list, the latter includes the former, were the most\n",
    "# efficient ones\n",
    "\n",
    "from sklearn.feature_extraction import stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Straightforward Implementation of a text classifier (as a benchmark)\n",
    "Using the same two classifiers - Naive Bayes and SVM. The classification functions are generic, so you can use any other classifiers by just making minor modifications of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple straightforward\n",
    "def clf_simple(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; use TfidfVectorizer\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))\n",
    "    matrix_train = vectorizer.fit_transform(trainX)    # lowercase=True by default, initially min_df=15, max_df=0.23\n",
    "    matrix_test = vectorizer.transform(testX)\n",
    "                   \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = classifier\n",
    "    clf = clf.fit(matrix_train, trainY)  \n",
    "        \n",
    "    # predict and compute metrics    \n",
    "    predictions=clf.predict(matrix_test)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text Classifier with Pipeline (as a benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "def clf_pipe(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "       \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    predictions=clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text Classifier Using 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cross_val_score\n",
    "def clf_cv(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "        \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(clf, trainX, trainY, cv=kfold, scoring='f1_micro')\n",
    "    print('Cross-validated Accuracy of {}: {:0.4f} +/- {:0.4f}'.format(classifier_name, scores.mean(), scores.std() * 2))\n",
    "    predictions = clf.predict(testX)\n",
    "    cm          = metrics.confusion_matrix(testY, predictions)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Text Classifier with Cross-Validated Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "def clf_GridSearchCV(classifier, data, labels, param_grid):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "            \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # do 3-fold cross validation for each of the possible combinations of the parameter values above\n",
    "    grid = GridSearchCV(clf, cv=3, param_grid=param_grid, scoring='f1_micro')\n",
    "    grid.fit(trainX, trainY)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid.best_score_, \n",
    "        grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    # train and predict on test instances using the best configs found in the CV step\n",
    "        \n",
    "    #predictions = grid.best_estimator_.predict(testX)                   # this is how to find the best estimator \n",
    "    #testX = grid.best_estimator_.named_steps['tfidf'].transform(testX)  # this is how to find indiv. components (same for pipeline)\n",
    "    predictions=grid.predict(testX)                                      # called on the best estimator by default\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} cross-validated F-1 score with grid search: {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    # return the best classifier to run it on the full dataset    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Running 7 options on the limited data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8367\n",
      "Confusion matrix:\n",
      "[[   0    0  924]\n",
      " [   0    4  471]\n",
      " [   0    0 7146]]\n",
      "\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultinomialNB()\n",
    "nb = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "        'vect__max_df':[0.25, 0.5,0.75],\n",
    "        'vect__min_df':[5,15,25,50,100],\n",
    "        'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "        'clf__alpha':[0.1,0.25,0.5,0.75,1.0]\n",
    "    }\n",
    "clf_simple(nb, data, labels)    # straightforward NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8367\n",
      "Confusion matrix:\n",
      "[[   0    0  924]\n",
      " [   0    4  471]\n",
      " [   0    0 7146]]\n",
      "\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_pipe(nb, data, labels)    # pipeline NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of MultinomialNB: 0.8457 +/- 0.0078\n",
      "Confusion matrix:\n",
      "[[   0    0  924]\n",
      " [   0    4  471]\n",
      " [   0    0 7146]]\n",
      "\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(nb, data, labels)    # cross_val_score NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.858677 using {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.853059 (0.000391) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857945 (0.000879) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.858326 (0.001028) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853937 (0.000391) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858414 (0.000736) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858677 (0.000194) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852884 (0.000481) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.857068 (0.000221) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.857126 (0.000431) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850923 (0.000466) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853849 (0.000410) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853761 (0.000369) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848143 (0.000118) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849811 (0.000012) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849841 (0.000029) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.852913 (0.000540) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.858033 (0.000879) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.858092 (0.001041) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.854025 (0.000267) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858414 (0.000645) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858560 (0.000405) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852913 (0.000428) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.856951 (0.000545) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.856980 (0.000416) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850777 (0.000359) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853849 (0.000528) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853878 (0.000283) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847909 (0.000077) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849958 (0.000037) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.850016 (0.000077) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.852766 (0.000555) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857916 (0.000908) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.858062 (0.000926) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.854025 (0.000324) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858267 (0.000730) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858326 (0.000481) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852766 (0.000525) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.856921 (0.000584) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.856951 (0.000457) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850835 (0.000343) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853703 (0.000407) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853556 (0.000366) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847968 (0.000194) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849694 (0.000121) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849811 (0.000061) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.850338 (0.000397) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.850894 (0.000301) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.850543 (0.000218) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.852298 (0.000447) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.855868 (0.000391) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.855809 (0.000119) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851742 (0.000506) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.855049 (0.000927) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855224 (0.000903) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850748 (0.000532) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853147 (0.000391) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853088 (0.000351) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847997 (0.000172) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849548 (0.000133) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849577 (0.000098) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.850543 (0.000360) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.850806 (0.000161) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.850543 (0.000098) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.852386 (0.000283) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.855722 (0.000161) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.855692 (0.000178) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851801 (0.000484) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.855283 (0.000717) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855312 (0.000764) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850865 (0.000468) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.853118 (0.000379) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.853118 (0.000263) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847968 (0.000013) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849431 (0.000121) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849460 (0.000083) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.850484 (0.000343) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.850660 (0.000121) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.850426 (0.000247) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.852357 (0.000267) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.855546 (0.000219) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.855575 (0.000037) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851596 (0.000545) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.855078 (0.000589) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.855166 (0.000725) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850660 (0.000466) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852854 (0.000371) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852825 (0.000238) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847880 (0.000131) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849285 (0.000154) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849314 (0.000119) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847178 (0.000249) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846593 (0.000614) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846534 (0.000411) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849958 (0.000461) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851859 (0.000407) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.851742 (0.000411) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850396 (0.000362) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852591 (0.000349) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852649 (0.000449) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849665 (0.000671) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852152 (0.000620) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852064 (0.000600) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847617 (0.000182) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849109 (0.000154) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849168 (0.000158) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847061 (0.000241) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846593 (0.000371) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846446 (0.000411) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849899 (0.000451) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851684 (0.000372) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.851713 (0.000380) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850631 (0.000405) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852532 (0.000305) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852562 (0.000399) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849723 (0.000630) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852152 (0.000532) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852152 (0.000505) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847675 (0.000179) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849021 (0.000194) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848992 (0.000222) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.847032 (0.000280) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.846534 (0.000411) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.846359 (0.000362) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849723 (0.000573) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851508 (0.000477) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.851421 (0.000443) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850426 (0.000256) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852562 (0.000264) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852562 (0.000388) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849548 (0.000506) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851976 (0.000465) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851976 (0.000400) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847617 (0.000186) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848904 (0.000222) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848904 (0.000222) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.846095 (0.000137) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845773 (0.000061) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845715 (0.000141) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847997 (0.000492) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.848963 (0.000723) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.848758 (0.000683) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849314 (0.000424) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.850279 (0.000307) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850221 (0.000461) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848875 (0.000420) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850777 (0.000584) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850748 (0.000554) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847383 (0.000169) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848495 (0.000242) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848524 (0.000264) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.846037 (0.000249) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845773 (0.000132) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845744 (0.000100) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847968 (0.000472) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.848875 (0.000789) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.848787 (0.000789) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849138 (0.000253) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.850367 (0.000182) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850279 (0.000302) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848992 (0.000292) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850952 (0.000497) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851069 (0.000576) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847353 (0.000204) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848524 (0.000388) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848612 (0.000325) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.846008 (0.000241) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845744 (0.000170) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845686 (0.000132) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.848026 (0.000505) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.848787 (0.000896) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.848729 (0.000862) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849080 (0.000213) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.850426 (0.000262) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850338 (0.000401) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848846 (0.000384) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850894 (0.000593) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850894 (0.000579) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847266 (0.000118) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848202 (0.000346) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848290 (0.000349) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845744 (0.000100) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845627 (0.000141) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845569 (0.000108) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847061 (0.000480) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847500 (0.000411) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847471 (0.000440) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847968 (0.000194) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.848758 (0.000751) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.848641 (0.000772) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848378 (0.000362) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850075 (0.000796) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850075 (0.000796) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847119 (0.000077) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847880 (0.000249) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847909 (0.000210) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845686 (0.000132) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845598 (0.000118) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845539 (0.000077) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847002 (0.000468) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847529 (0.000472) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847383 (0.000467) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848026 (0.000343) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.848787 (0.000614) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.848670 (0.000565) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848436 (0.000210) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850016 (0.000783) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850104 (0.000783) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847119 (0.000113) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848085 (0.000264) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848114 (0.000290) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845656 (0.000137) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845598 (0.000118) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845510 (0.000061) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847032 (0.000503) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.847383 (0.000503) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.847295 (0.000475) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.847939 (0.000362) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.848553 (0.000600) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.848495 (0.000630) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848407 (0.000256) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849899 (0.000823) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849958 (0.000850) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847032 (0.000217) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847968 (0.000371) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847997 (0.000399) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB cross-validated F-1 score with grid search: 0.8520\n",
      "Confusion matrix:\n",
      "[[  67   22  835]\n",
      " [  42  100  333]\n",
      " [  25    8 7113]]\n",
      "\n",
      "Wall time: 2h 16min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_NB = clf_GridSearchCV(nb, data, labels, nb_param_grid)    # parameter grid search NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC F-1 score:   0.8609\n",
      "Confusion matrix:\n",
      "[[ 191   79  654]\n",
      " [  95  203  177]\n",
      " [ 145   39 6962]]\n",
      "\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC()\n",
    "svc_param_grid = {\n",
    "    'vect__max_df':[0.25,0.5,0.75],\n",
    "    'vect__min_df':[5,15,25,50,100],\n",
    "    'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "    'clf__C':[0.1,0.25,0.5,0.75,1.0]\n",
    "}\n",
    "clf_pipe(svc, data, labels)            # pipeline SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of LinearSVC: 0.8696 +/- 0.0095\n",
      "Confusion matrix:\n",
      "[[ 191   79  654]\n",
      " [  95  203  177]\n",
      " [ 145   39 6962]]\n",
      "\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(svc, data, labels)             # cross_val_score SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.871463 using {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.860608 (0.001475) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.859438 (0.000769) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.859642 (0.000856) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.861837 (0.001706) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.861808 (0.000689) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.861954 (0.000912) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.861866 (0.001641) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862539 (0.001419) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862510 (0.001482) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.862832 (0.001372) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863154 (0.001536) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863183 (0.001543) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.860842 (0.000958) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.861515 (0.000699) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.861457 (0.000738) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.861457 (0.002084) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.860023 (0.001340) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.859935 (0.001231) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.862247 (0.001980) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.862276 (0.000949) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862364 (0.000880) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.862510 (0.002021) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863095 (0.001551) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.863124 (0.001551) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863241 (0.001409) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863505 (0.001465) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863651 (0.001575) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.862042 (0.001335) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.862744 (0.001043) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.862715 (0.001067) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.861339 (0.001951) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.859994 (0.001164) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.859877 (0.001294) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.862188 (0.002052) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.862012 (0.000959) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862276 (0.000880) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.862510 (0.001929) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862949 (0.001344) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.863154 (0.001520) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863241 (0.001164) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863563 (0.001601) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863534 (0.001710) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.862100 (0.001264) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.862744 (0.000929) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.862685 (0.000961) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866987 (0.001951) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868684 (0.001949) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868420 (0.001877) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867133 (0.001497) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.869561 (0.001422) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.869766 (0.001543) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867074 (0.002079) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.869064 (0.002524) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.869035 (0.002122) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866782 (0.002226) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867221 (0.002055) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867279 (0.002275) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863651 (0.001817) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865290 (0.001521) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865202 (0.001566) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867835 (0.001983) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868771 (0.001889) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868684 (0.002147) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868098 (0.001118) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.869766 (0.001952) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.869883 (0.001869) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867981 (0.000878) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.869474 (0.002411) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.869210 (0.002307) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867923 (0.001414) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867777 (0.001486) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.868011 (0.001755) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864382 (0.000999) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865641 (0.001731) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865582 (0.001707) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867952 (0.001947) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868596 (0.002008) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868596 (0.001990) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868069 (0.001161) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.869737 (0.001806) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.869971 (0.001966) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867981 (0.001061) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.869795 (0.002231) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.869444 (0.002339) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867630 (0.001313) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867923 (0.001550) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867864 (0.001645) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864587 (0.001111) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865582 (0.001731) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865436 (0.001690) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867864 (0.001519) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.871024 (0.002689) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.870761 (0.002552) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866840 (0.001661) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.870820 (0.000418) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.870644 (0.000731) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.866080 (0.001907) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.870059 (0.001777) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.869503 (0.001737) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866021 (0.000959) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867455 (0.000665) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.866957 (0.000628) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.862861 (0.001163) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864734 (0.001635) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864821 (0.001548) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.868859 (0.000874) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.871405 (0.002440) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.871346 (0.002054) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.869240 (0.001146) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.871054 (0.000713) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.870995 (0.001051) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867864 (0.001520) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.869678 (0.001459) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.869298 (0.001443) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866928 (0.001185) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867367 (0.000738) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867308 (0.000619) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864265 (0.001774) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865348 (0.001758) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865465 (0.001627) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.868859 (0.000783) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.871463 (0.002457) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.871288 (0.001952) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.869240 (0.000916) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.871229 (0.000754) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.871112 (0.000969) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.868157 (0.001350) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.869971 (0.001541) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.869474 (0.001623) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866928 (0.001453) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.867484 (0.000704) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.867308 (0.000517) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864090 (0.001590) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865436 (0.001667) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865494 (0.001731) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866138 (0.001440) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.870176 (0.002519) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869942 (0.002547) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.865407 (0.001904) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868069 (0.000726) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867952 (0.000539) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863710 (0.002076) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.866343 (0.001460) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.866197 (0.001709) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864558 (0.000843) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.864909 (0.000944) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.864968 (0.001226) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.862100 (0.001460) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863212 (0.001408) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863358 (0.001638) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867660 (0.001290) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.870585 (0.002095) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.870176 (0.002178) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866723 (0.001353) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868040 (0.000172) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867835 (0.000142) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.865377 (0.001544) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.866694 (0.001610) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.866343 (0.001701) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.865055 (0.001089) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.865787 (0.000689) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.865875 (0.001001) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863446 (0.001524) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864295 (0.001427) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864353 (0.001340) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867747 (0.001102) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.870585 (0.002068) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.870381 (0.002162) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.866665 (0.001460) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868011 (0.000099) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.867894 (0.000318) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.865933 (0.001326) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.866635 (0.001500) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.866548 (0.001594) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.865172 (0.001058) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.865904 (0.000796) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.865816 (0.000983) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863622 (0.001457) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863944 (0.001813) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863944 (0.001757) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.863534 (0.001539) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869386 (0.002196) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869240 (0.002406) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.862100 (0.001312) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.864646 (0.000447) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.864880 (0.000228) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.861808 (0.002258) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863534 (0.002140) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.863358 (0.002374) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.862364 (0.001195) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863680 (0.001067) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863885 (0.000943) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.861398 (0.001119) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.862422 (0.001324) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.862276 (0.001442) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864412 (0.001479) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.868976 (0.001907) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868537 (0.002300) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.863388 (0.001376) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.864880 (0.000160) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.864675 (0.000371) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863300 (0.001665) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863358 (0.002256) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.863271 (0.001809) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863007 (0.001202) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.864002 (0.000915) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.864119 (0.000999) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.862920 (0.001568) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864090 (0.001836) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864353 (0.001899) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864646 (0.001370) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869152 (0.001907) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.868567 (0.002388) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.863534 (0.001505) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.864997 (0.000183) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.864997 (0.000197) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863241 (0.001303) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.863592 (0.002070) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.863475 (0.001758) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863095 (0.001070) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.864090 (0.001090) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.864002 (0.000979) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863241 (0.001670) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864002 (0.001728) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864148 (0.001836) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC cross-validated F-1 score with grid search: 0.8634\n",
      "Confusion matrix:\n",
      "[[ 162   68  694]\n",
      " [  80  191  204]\n",
      " [  95   26 7025]]\n",
      "\n",
      "Wall time: 3h 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_SVM = clf_GridSearchCV(svc, data, labels, svc_param_grid)    # parameter grid search SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Running Naive Bayes and SVM with the Best Parameters from Grid Search on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full dataset and labels\n",
    "full_data = df_text['reviewText'].values\n",
    "full_labels = df_text['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Naive Bayes F-1 score on full dataset: 0.8759\n",
      "Confusion matrix:\n",
      "[[  4618   1517  13124]\n",
      " [  1744   4858   4829]\n",
      " [  2722    452 162660]]\n",
      "\n",
      "The best SVM F-1 score on full dataset: 0.8846\n",
      "Confusion matrix:\n",
      "[[  4965   2016  12278]\n",
      " [  1920   6044   3467]\n",
      " [  2380    627 162827]]\n",
      "\n",
      "Wall time: 22min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the two best classifier on it\n",
    "for best_clf in [best_NB, best_SVM]:\n",
    "        \n",
    "    # split into train and test sets\n",
    "    trainX, testX, trainY, testY = train_test_split(full_data, full_labels, test_size = 0.2, random_state = 43)\n",
    "    clf = best_clf.fit(trainX, trainY)\n",
    "    \n",
    "    # predict and compute metrics\n",
    "    predictions = clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('The best {} F-1 score on full dataset: {:0.4f}'.format('Naive Bayes' if best_clf==best_NB else 'SVM', score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
