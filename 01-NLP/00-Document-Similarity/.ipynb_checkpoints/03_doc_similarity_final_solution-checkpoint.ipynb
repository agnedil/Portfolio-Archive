{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install scikit-learn\n",
    "#!pip install sqlalchemy\n",
    "\n",
    "#!pip install psycopg2-binary            # only this worked\n",
    "#!pip install psycopg2                   # error installing\n",
    "\n",
    "# error installing\n",
    "#!python -m pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org --trusted-host pypi.python.org psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity\n",
    "Steps:\n",
    "* Connect to remote relational database\n",
    "* Run SQL query and retrieve document text\n",
    "* Convert documents into word vectors\n",
    "* Compute cosine similarity for all pairs of documents (numpy allows for a faster implementation)\n",
    "* Write the resutls back to the remote relational database as a separate table / relation with the following schema: doc1, doc2, similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note 1](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50): with cosine similarity, we need to convert sentences into vectors. One way to do that is to use bag of words with either TF (term frequency) or TF-IDF (term frequency- inverse document frequency). The choice of TF or TF-IDF depends on application and is immaterial to how cosine similarity is actually performed â€” which just needs vectors. **TF is good for text similarity in general, but TF-IDF is good for search query relevance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SQL query and load data from remote PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6316</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6317</td>\n",
       "      <td>VHDX to VMDK conversions Do you need to conver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6318</td>\n",
       "      <td>VHDX to VMDK conversions Do you need to conver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6319</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6320</td>\n",
       "      <td>Erwin Pong 227 E. 4th Street Walnet Creek, CA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6321</td>\n",
       "      <td>\\tOPentext Corporation  \\tVM Manifest  \\tby Ky...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                            summary\n",
       "0    6316                                              test \n",
       "1    6317  VHDX to VMDK conversions Do you need to conver...\n",
       "2    6318  VHDX to VMDK conversions Do you need to conver...\n",
       "3    6319                                              test \n",
       "4    6320  Erwin Pong 227 E. 4th Street Walnet Creek, CA ...\n",
       "5    6321  \\tOPentext Corporation  \\tVM Manifest  \\tby Ky..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN SQL QUERY, LOAD DATA INTO A PANDAS DATAFRAME\n",
    "eng = create_engine('postgresql://server-credentials')\n",
    "\n",
    "sql = \"\"\"\n",
    "select c.doc_id, summary\n",
    "from content_metadata_attributes c\n",
    "full join document d\n",
    "on c.doc_id = d.doc_id\n",
    "where d.repository_id = '10'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(sql, eng)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if the corpus is large\n",
    "#df.to_csv('docs_raw.txt', index=False)\n",
    "#df = pd.read_csv('docs_raw.txt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer + Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_cosine(query, X, j):\n",
    "        \n",
    "    ''' Compute cosine similarity between a select doc and the rest of the corpus\n",
    "        Timewise, pairwise.cosine_similarity = 1 - pairwise.pairwise_distance'''\n",
    "        \n",
    "    X_copy = X.copy()\n",
    "    X_copy = np.delete(X_copy, j, axis=0)                    # DELETE DOC IN QUESTION (copy trick to examples)\n",
    "        \n",
    "    #sims = 1 - pairwise_distances(query.reshape(1, -1), X_copy, metric='cosine').reshape(len(X_copy))\n",
    "    sims = cosine_similarity(query.reshape(1, -1), X_copy).reshape(len(X_copy))\n",
    "    sims = np.insert(sims, j, 0.0, axis=0)                  # INSERT DOC IN QUESTION (copy trick to examples)    \n",
    "    res = sims.argsort()[-1:]\n",
    "    sims_relevant = sims[res]\n",
    "        \n",
    "    return res[0], sims_relevant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GET DOCUMENT IDs AND TEXT\n",
    "files = df.doc_id.tolist()\n",
    "raw_docs = df.summary.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONVERT TO WORD VECTORS\n",
    "cv = TfidfVectorizer()#(max_df=0.75, min_df=2)\n",
    "X = np.array(cv.fit_transform(raw_docs).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed: 0\r",
      "Time elapsed 0.00021291573842366536 min\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE COSINE SIMILARITY SCORES FOR ALL POSSIBLE PAIRS OF DOCUMENTS IN THE COLLECTION USING NUMPY (FASTER)\n",
    "start = time()\n",
    "results = []\n",
    "sims_relevants = []\n",
    "for i in range(len(X)):\n",
    "    if i % 10 == 0: print('Documents processed: %d\\r'%i, end=\"\")    # carriage return \\r takes cursor to beginning of line\n",
    "    result, sims_relevant = query_cosine(X[i], X, i)\n",
    "    results.append(result)\n",
    "    sims_relevants.append(sims_relevant)\n",
    "end = time()\n",
    "print('Time elapsed {} min'.format((end - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COPY RESULTS TO ORIGINAL DATAFRAME\n",
    "df['sim_indices'] = results\n",
    "df['sim_score'] = sims_relevants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GET DOC IDs OF SIMILAR FILES\n",
    "def convert_idx(idx, df):            \n",
    "    return df.loc[idx]['doc_id']\n",
    "\n",
    "df['doc_id_sim'] = df['sim_indices'].apply(lambda x: convert_idx(x, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>sim_indices</th>\n",
       "      <th>sim_score</th>\n",
       "      <th>doc_id_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6316</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6317</td>\n",
       "      <td>VHDX to VMDK conversions Do you need to conver...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6318</td>\n",
       "      <td>VHDX to VMDK conversions Do you need to conver...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6319</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6320</td>\n",
       "      <td>Erwin Pong 227 E. 4th Street Walnet Creek, CA ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.146863</td>\n",
       "      <td>6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6321</td>\n",
       "      <td>\\tOPentext Corporation  \\tVM Manifest  \\tby Ky...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.623872</td>\n",
       "      <td>6318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                            summary  sim_indices  \\\n",
       "0    6316                                              test             3   \n",
       "1    6317  VHDX to VMDK conversions Do you need to conver...            2   \n",
       "2    6318  VHDX to VMDK conversions Do you need to conver...            1   \n",
       "3    6319                                              test             0   \n",
       "4    6320  Erwin Pong 227 E. 4th Street Walnet Creek, CA ...            5   \n",
       "5    6321  \\tOPentext Corporation  \\tVM Manifest  \\tby Ky...            2   \n",
       "\n",
       "   sim_score  doc_id_sim  \n",
       "0   1.000000        6319  \n",
       "1   1.000000        6318  \n",
       "2   1.000000        6317  \n",
       "3   1.000000        6316  \n",
       "4   0.146863        6321  \n",
       "5   0.623872        6318  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DISPLAY RESULTS IN THE FORM: DOC_ID, SIMILAR DOC_IS, COSINE SIMILARITY SCORE\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save back to remote PostgreSQL as a separate new relation (for use in a dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng = create_engine('postgresql://server-credentials')\n",
    "with eng.connect() as con:\n",
    "    df[['doc_id', 'doc_id_sim', 'sim_score']].to_sql('doc_similarity', con, if_exists='replace', index=False)    #,dtype=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
