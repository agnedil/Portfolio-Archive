{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from nltk.tokenize import word_tokenize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn & Numpy (a lot shorter and faster than Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cosine(query, X):\n",
    "    sims = 1 - pairwise_distances(query.reshape(1, -1), X, metric='cosine').reshape(len(X))        \n",
    "    res = np.where(sims > 0.1)[0]    # tuple of arrays by axis; since I have 1D array, only one axis, hence [0]\n",
    "            \n",
    "    sims_relevant = None\n",
    "    if res.size == 0:\n",
    "        res = sims.argsort()[-1:]\n",
    "        sims_relevant = sims[res]   # if a single similarity less than 0.79\n",
    "    else:\n",
    "        sims_relevant = sims[res]\n",
    "    \n",
    "    return res, sims_relevant    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\"Once upon a time on Sunday.\",\n",
    "                 \"Two crocodiles in the river of Nile.\",\n",
    "                 \"The barber next door cuts his hair.\",\n",
    "                 \"Calm you mind, calm your senses.\",\n",
    "                 \"Fun is always fun.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23904572186687867: Once upon a time on Sunday.\n",
      "0.20203050891044216: Two crocodiles in the river of Nile.\n",
      "0.10101525445522108: The barber next door cuts his hair.\n",
      "0.7559289460184544: Calm you mind, calm your senses.\n",
      "0.21821789023599236: Fun is always fun.\n",
      "Wall time: 1.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer()\n",
    "X = np.array(cv.fit_transform(raw_documents).todense())\n",
    "\n",
    "ml_tweet = 'Calm you mind, calm your senses on Sunday of Nile a barber fun.'\n",
    "query = np.array(cv.transform([ml_tweet]).todense())[0]\n",
    "result, sims_relevant = query_cosine(query, X)\n",
    "\n",
    "for item in zip(sims_relevant, result):\n",
    "    print('{}: {}'.format(item[0], raw_documents[item[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "Loosely based on:  \n",
    "https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dir(gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\"Once upon a time on Sunday.\",\n",
    "                 \"Two crocodiles in the river of Nile.\",\n",
    "                 \"The barber next door cuts his hair.\",\n",
    "                 \"Calm you mind, calm your senses.\",\n",
    "                 \"Fun is always fun.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random word: upon\n",
      "Its index: 5\n",
      "\n",
      "Some examples from corpus:\n",
      "Once upon a time on Sunday.\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "\n",
      "Two crocodiles in the river of Nile.\n",
      "[(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
      "\n",
      "TfidfModel(num_docs=5, num_nnz=29)\n",
      "Tokens: 29\n",
      "Similarity index with 5 documents in 0 shards (stored under C:\\Users\\anedilko\\AppData\\Local\\Temp\\index)\n",
      "<class 'gensim.similarities.docsim.Similarity'> \n",
      "\n",
      "0.3061862289905548: Once upon a time on Sunday.\n",
      "0.19882437586784363: Two crocodiles in the river of Nile.\n",
      "0.7499999403953552: Calm you mind, calm your senses.\n",
      "0.20412415266036987: Fun is always fun.\n",
      "Wall time: 7.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text) if w != '.'] for text in raw_documents]\n",
    "\n",
    "# A dictionary maps every word to a number\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "random_word = dictionary[5]\n",
    "print('Random word:', dictionary[5])\n",
    "print('Its index:', dictionary.token2id[random_word])\n",
    "#print(dictionary.token2id)\n",
    "\n",
    "# corpus = list of bags of words (based on the number of times each word occurs in the document)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print('\\nSome examples from corpus:')\n",
    "min_idx = 2\n",
    "if len(corpus) < min_idx:\n",
    "    min_idx = len(corpus)\n",
    "for doc, sent in list(zip(raw_documents, corpus))[:min_idx]:\n",
    "    print(doc)\n",
    "    print(sent)\n",
    "    print()\n",
    "\n",
    "# tf-idf model from the corpus (num_nnz = number of tokens)\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "num_tokens = 0\n",
    "for i in corpus: num_tokens += len(i)\n",
    "print('Tokens:', num_tokens)\n",
    "\n",
    "# a similarity measure object in tf-idf space ('get_tmpfile' lets you store index in a temp file)\n",
    "# Alternative to get_tmpfile:\n",
    "# use 'folder/fname.txt' instead of index_temp, but you have to create this folder within the current directory\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "sims = gensim.similarities.Similarity(index_temp, tf_idf[corpus], num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims), '\\n')\n",
    "\n",
    "# query a document and find matches greater than a threshold or just the top match\n",
    "test_doc = 'Calm you mind, calm your senses on Sunday of Nile a barber fun.'\n",
    "query_doc_bow = dictionary.doc2bow([w.lower() for w in word_tokenize(test_doc) if w != '.'])\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "# See all similarities\n",
    "similarities = sims[query_doc_tf_idf]\n",
    "\n",
    "# Average similarity over all docs in the corpus\n",
    "#ave_sim =(np.sum(sims[query_doc_tf_idf], dtype=np.float32)) / len(raw_documents)\n",
    "#print('Average similarity over corpus:', round(ave_sim, 4))\n",
    "\n",
    "res = np.where(similarities > 0.1)[0]    # tuple of arrays by axis; since I have 1D array, only one axis, hence [0]\n",
    "\n",
    "sims_relevant = None\n",
    "if res.size == 0:\n",
    "    res = similarities.argsort()[-1:]\n",
    "    sims_relevant = similarities[res]   # if a single similarity less than 0.79\n",
    "else:\n",
    "    sims_relevant = similarities[res]\n",
    "\n",
    "for item in zip(sims_relevant, res):\n",
    "    print('{}: {}'.format(item[0], raw_documents[item[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim uses slightly different similarity scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
