{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. ReviewText + reviewerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\n",
      "Scikit-learn 0.20.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "#nltk.download('stopwords')    # this is done just once\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "from platform import python_version\n",
    "print('Python {}'.format(python_version()))\n",
    "print('Scikit-learn {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in json, combine reviewText and reviewerID columns, get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 982619\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path=''\n",
    "file='kindle_reviews.json'\n",
    "df = pd.read_json(path_or_buf=path+file, lines=True, encoding='utf-8')    #, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, chunksize=None, compression='infer')\n",
    "print('Length of text: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1F6404F1VG29J</td>\n",
       "      <td>Avidreader</td>\n",
       "      <td>Nice vintage story</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "      <td>01 6, 2014</td>\n",
       "      <td>AN0N05A9LIJEQ</td>\n",
       "      <td>critters</td>\n",
       "      <td>Different...</td>\n",
       "      <td>1388966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "      <td>04 4, 2014</td>\n",
       "      <td>A795DMNCJILA6</td>\n",
       "      <td>dot</td>\n",
       "      <td>Oldie</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>A1FV0SX13TWVXQ</td>\n",
       "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
       "      <td>I really liked it.</td>\n",
       "      <td>1392768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "      <td>03 19, 2014</td>\n",
       "      <td>A3SPTOKDG7WBLN</td>\n",
       "      <td>Father Dowling Fan</td>\n",
       "      <td>Period Mystery</td>\n",
       "      <td>1395187200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  B000F83SZQ  [0, 0]        5   \n",
       "1  B000F83SZQ  [2, 2]        4   \n",
       "2  B000F83SZQ  [2, 2]        4   \n",
       "3  B000F83SZQ  [1, 1]        5   \n",
       "4  B000F83SZQ  [0, 1]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I enjoy vintage books and movies so I enjoyed ...   05 5, 2014   \n",
       "1  This book is a reissue of an old one; the auth...   01 6, 2014   \n",
       "2  This was a fairly interesting read.  It had ol...   04 4, 2014   \n",
       "3  I'd never read any of the Amy Brewster mysteri...  02 19, 2014   \n",
       "4  If you like period pieces - clothing, lingo, y...  03 19, 2014   \n",
       "\n",
       "       reviewerID                         reviewerName             summary  \\\n",
       "0  A1F6404F1VG29J                           Avidreader  Nice vintage story   \n",
       "1   AN0N05A9LIJEQ                             critters        Different...   \n",
       "2   A795DMNCJILA6                                  dot               Oldie   \n",
       "3  A1FV0SX13TWVXQ  Elaine H. Turley \"Montana Songbird\"  I really liked it.   \n",
       "4  A3SPTOKDG7WBLN                   Father Dowling Fan      Period Mystery   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1399248000  \n",
       "1      1388966400  \n",
       "2      1396569600  \n",
       "3      1392768000  \n",
       "4      1395187200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the \"reviewerID\" column to reviewText\n",
    "df['reviewText'] = df['reviewerID'].map(str) + ' ' + df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['reviewText', 'overall']].copy()        # copy only certain columns to another df\n",
    "df = None                                             # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1F6404F1VG29J I enjoy vintage books and movie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AN0N05A9LIJEQ This book is a reissue of an old...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A795DMNCJILA6 This was a fairly interesting re...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1FV0SX13TWVXQ I'd never read any of the Amy B...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3SPTOKDG7WBLN If you like period pieces - clo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  A1F6404F1VG29J I enjoy vintage books and movie...        5\n",
       "1  AN0N05A9LIJEQ This book is a reissue of an old...        4\n",
       "2  A795DMNCJILA6 This was a fairly interesting re...        4\n",
       "3  A1FV0SX13TWVXQ I'd never read any of the Amy B...        5\n",
       "4  A3SPTOKDG7WBLN If you like period pieces - clo...        4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert 5 ratings to 3 ('neg', 'mixed', and 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 5 ratings to 3 ('neg', 'mixed', and 'pos')\n",
    "df_text['overall'] = df_text['overall'].apply(lambda x: 'pos' if x > 3 else 'neg' if x < 3 else 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1F6404F1VG29J I enjoy vintage books and movie...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AN0N05A9LIJEQ This book is a reissue of an old...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A795DMNCJILA6 This was a fairly interesting re...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1FV0SX13TWVXQ I'd never read any of the Amy B...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3SPTOKDG7WBLN If you like period pieces - clo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText overall\n",
       "0  A1F6404F1VG29J I enjoy vintage books and movie...     pos\n",
       "1  AN0N05A9LIJEQ This book is a reissue of an old...     pos\n",
       "2  A795DMNCJILA6 This was a fairly interesting re...     pos\n",
       "3  A1FV0SX13TWVXQ I'd never read any of the Amy B...     pos\n",
       "4  A3SPTOKDG7WBLN If you like period pieces - clo...     pos"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the dataset is 982619\n"
     ]
    }
   ],
   "source": [
    "print('Total length of the dataset is {}'.format(len(df_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reduce size of data, get reduced-size dataset and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sample is 42722\n"
     ]
    }
   ],
   "source": [
    "# get a random sample from the dataframe whose size is manageable for cross-validation and grid search\n",
    "# with more computing resources and/or time, this can be done on a larger data set\n",
    "length = len(df_text)\n",
    "df_text = df_text.sample(n=length)\n",
    "df_text_short = df_text.sample(n=int(length/23))\n",
    "print('Length of the sample is {}'.format(len(df_text_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos      36140\n",
       "mixed     4102\n",
       "neg       2480\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique labels\n",
    "df_text_short.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our train set and labels\n",
    "data = df_text_short['reviewText'].values\n",
    "labels = df_text_short['overall'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Experimenting with different lists of stopwords and selecting the most efficient one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['third', 'please', 'our', 'and', 'latterly', 'why', 'thereafter', 'at', 'are', 'no', 'which', 'move', 'towards', 'such', 'herein', 'found', 'there', 'always', 'must', 'nor', 'take', 'whence', 'cant', 'am', 'eleven', 'where', 'its', 'further', 'eg', 'de', 'via', 'still', 'from', 'beside', 'onto', 'whether', 'during', 'three', 'perhaps', 'otherwise', 'though', 'without', 'eight', 'seem', 'i', 'cry', 'empty', 'thus', 'find', 'now', 'between', 'they', 'us', 'into', 'most', 'neither', 'another', 'other', 'together', 'could', 'formerly', 'nevertheless', 'however', 'therefore', 'out', 'as', 'latter', 'bottom', 'while', 'then', 'first', 'off', 'sometime', 'elsewhere', 'hers', 'would', 'detail', 'the', 'might', 'fifty', 'ours', 'every', 'was', 'done', 'more', 'many', 'he', 're', 'back', 'whose', 'became', 'seeming', 'name', 'sixty', 'system', 'do', 'twelve', 'only', 'either', 'again', 'wherever', 'top', 'somehow', 'on', 'in', 'indeed', 'bill', 'those', 'this', 'two', 'or', 'less', 'seems', 'four', 'whereas', 'become', 'ie', 'with', 'everywhere', 'across', 'me', 'once', 'their', 'thick', 'keep', 'namely', 'himself', 'nothing', 'whither', 'than', 'whole', 'her', 'a', 'because', 'may', 'alone', 'been', 'amount', 'ten', 'whatever', 'least', 'these', 'get', 'hundred', 'go', 'both', 'being', 'enough', 'everyone', 'side', 'almost', 'ever', 'yet', 'what', 'etc', 'several', 'is', 'often', 'hereupon', 'ltd', 'anyway', 'fifteen', 'meanwhile', 'before', 'becoming', 'amoungst', 'somewhere', 'forty', 'hasnt', 'throughout', 'themselves', 'give', 'never', 'over', 'to', 'anyhow', 'mostly', 'next', 'around', 'by', 'along', 'yourself', 'any', 'can', 'few', 'so', 'when', 'amongst', 'moreover', 'after', 'whereafter', 'five', 'be', 'fill', 'nine', 'sincere', 'fire', 'anything', 'them', 'cannot', 'will', 'put', 'everything', 'per', 'his', 'toward', 'your', 'here', 'former', 'call', 'couldnt', 'whom', 'becomes', 'nowhere', 'she', 'describe', 'last', 'that', 'noone', 'none', 'something', 'it', 'who', 'has', 'my', 'made', 'besides', 'all', 'if', 'serious', 'whereby', 'too', 'very', 'already', 'behind', 'down', 'mill', 'un', 'for', 'herself', 'much', 'although', 'yourselves', 'thereby', 'thru', 'how', 'nobody', 'others', 'afterwards', 'one', 'we', 'itself', 'interest', 'sometimes', 'six', 'yours', 'mine', 'had', 'therein', 'own', 'about', 'twenty', 'some', 'within', 'have', 'thence', 'beforehand', 'not', 'thin', 'con', 'rather', 'show', 'seemed', 'anyone', 'upon', 'against', 'under', 'see', 'ourselves', 'part', 'but', 'due', 'myself', 'until', 'above', 'hereafter', 'beyond', 'hence', 'through', 'inc', 'well', 'thereupon', 'else', 'anywhere', 'whereupon', 'him', 'you', 'except', 'front', 'full', 'among', 'each', 'were', 'co', 'also', 'an', 'someone', 'since', 'hereby', 'up', 'wherein', 'below', 'even', 'same', 'whoever', 'should', 'whenever', 'of']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "153\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['third', 'please', 'our', 'whoa', 'and', 'latterly', \"we've\", 'isn', 'anybody', 'km', 'nowadays', 'why', 'thereafter', 'at', 'are', 'cf', 'used', 'said', 'no', 'sent', 'which', 'move', 'shan', \"couldn't\", 'towards', 'such', 'herein', 'found', 'there', 'seen', '!', 'always', 'must', 'sideways', 'nor', 'take', 'whence', 'hardly', 'cant', 'somewhat', 'am', 'eleven', 'contrariwise', 'underneath', 'where', 'its', 'further', 'eg', 'de', 'via', 'still', 'from', '?', 'beside', 'onto', 'whether', \"they've\", 'during', 'three', 'perhaps', 'otherwise', 'though', 'without', 'eight', 'seem', 'i', 'cry', 'empty', 'thus', 'find', 'unable', 'now', 'hast', 'between', \"'\", 'they', \"don't\", 'till', 'us', 'into', 'meantime', \"you'll\", 'most', \"i've\", 'neither', \"let's\", 'another', 'whatsoever', 'other', 'together', 'could', 'formerly', 'nevertheless', 'however', 'therefore', 'out', 'sprang', 'insomuch', 'theirs', 'aren', 'as', 'latter', 'nope', 'bottom', 'while', \"she'd\", 'then', 'first', 'unlike', 'off', 'hitherto', 'sometime', 'don', 'elsewhere', 'hers', 'would', 'detail', 'the', 'might', 'y', \"doesn't\", 'fifty', 'ours', 'every', 'want', 'staves', 'was', 'whereabouts', 'done', 'more', 'doing', 'many', 'dual', 'he', 'thy', 'back', 're', 'really', \"he's\", 'whose', 'exclusive', 'using', 'thereto', 'became', 'seeming', \"hasn't\", \"weren't\", 'save', \"they're\", 'name', 'sixty', 'system', 'shown', 'do', 'twelve', 'unless', 'only', 'either', 'again', 'excepted', \"they'll\", 'wherever', 'top', 'somehow', 'furthest', 'maybe', 'according', 'excepting', 'on', 'in', 'indeed', 'bill', 'those', 'd', 'this', 'two', 'plenty', \"there's\", 'needn', 'or', 'less', 'whereof', 'seems', 'four', '-', 'whensoever', 'whereon', \"aren't\", \"we're\", 'whereas', 'whomever', 'become', 'wherefrom', 's', 'ie', 'with', 'everywhere', 'mrs', 'across', \"i'm\", 'excluding', 'me', 'once', 'ugh', 'their', 'thick', 'keep', \"it's\", 'namely', 'hindmost', 'whichever', 'himself', 'nothing', 'whither', 'than', 'whole', 'o', 'her', 'a', 'doth', 'week', 'because', 'thenceforth', 'wow', 'may', 'alone', 'been', 'kg', 'spat', 'everybody', 'indoors', 'hadn', \"you've\", 'slept', 'exclude', 'won', 'amount', 'ten', 'whatever', 'least', 'these', 'get', 'hundred', 'go', 'both', 'apart', 'smote', 'being', 'enough', 've', 'everyone', 'side', 'almost', \"didn't\", 'ye', 'ever', 'yet', 'm', 'thrice', 'whereinto', \"won't\", 'what', 'etc', 'spoken', 'thereabout', 'several', \"i'll\", 'sang', 'is', 'often', 'hereupon', 'ltd', 'supposing', 'cos', 'anyway', 'fifteen', 'meanwhile', 'before', 'becoming', 'amoungst', 'yippee', 'somewhere', 'let', 'forty', 'hasnt', 'wheresoever', 'hereto', 'throughout', 'spoke', 'themselves', 'sprung', \"i'd\", 'give', 'seldom', 'never', 'over', 'choose', 'hasn', 'wilt', 'to', 'whilst', 'anyhow', 'weren', 'exception', 'mostly', 'next', 'around', \"we'd\", 'by', 'along', 'ain', 'yourself', 'any', \"why's\", \"shouldn't\", 'can', 'few', 'so', 'did', 'provide', 'inside', \"she's\", 'somebody', 'll', 'when', \"wouldn't\", '+', 'amongst', 'moreover', 'after', 'whereafter', 'five', 'inward', 'be', 'halves', 'include', 'fill', 'nine', 'sincere', 'fire', 'hath', 'anything', '\"', 'furthermore', 'quite', \"here's\", 'ought', 'wasn', 'them', 'cannot', 'will', \"she'll\", 'put', 'certain', 'everything', 'vs', 'per', 'forward', 'selves', 'his', 'toward', 'henceforth', 'your', 'inasmuch', 'thereon', 'here', 'former', 'call', 'couldnt', 'whom', 'ms', \"should've\", 'thou', 'cu', \"he'll\", 'becomes', 'notwithstanding', 'nowhere', \"how's\", 'she', 'describe', 'last', 'that', 'noone', 'none', 'hereabouts', 'slew', 'something', 'it', 'far', 'who', 'has', 'my', 'ma', 'forth', 'made', 'besides', 'all', '.', 'included', 'sake', 'ok', 'et', 'av', 'use', \"isn't\", 'whichsoever', 'howsoever', 'if', 'mightn', 'whomsoever', 'serious', ',', 'whereby', \"needn't\", 'instead', 'shouldn', '`', \"shan't\", 'saw', 'slung', 'too', 'very', 'thereabouts', 'already', 'behind', 'down', 'mill', 'un', 'day', \"wasn't\", 'wouldn', 'for', 'upward', 'wherewith', \"what's\", 'herself', 'much', 'whosoever', \"he'd\", \"haven't\", \"you'd\", 'although', 'yourselves', 'thereby', 'thru', 'couldn', 'how', 'upwards', \"you're\", 'having', 'nobody', 'others', 'including', 'afterwards', 'one', 'we', '``', 'itself', 'didn', 'interest', 'sometimes', 'canst', 'six', 'yours', 'mine', 'had', 'therein', 'own', \"mustn't\", 'whereto', 'about', 'twenty', 'dost', 'slunk', 'thereof', 'some', 'within', 'have', 'thence', '*', 'farthest', 'lest', 'mustn', 'spake', 'year', 'albeit', 'round', 'beforehand', 'kind', 'not', 'thin', 'con', 'rather', 'show', 'seemed', 'mr', 'shalt', 'little', 'whew', 'anyone', 'upon', 'against', 'whereunto', \"where's\", 'hither', 'thyself', 'haven', 'under', 'see', \"hadn't\", 'ourselves', \"that'll\", 'part', 'thee', 'but', 'due', 'myself', 'like', \"mightn't\", 'nonetheless', 't', 'until', 'above', 'hereafter', 'just', 'worst', 'beyond', 'hence', 'through', 'inc', \"who's\", 'whereat', 'well', \"when's\", 'need', 'thereupon', 'else', 'stave', 'farther', 'anywhere', '/', \"that's\", 'whereupon', 'him', 'outside', 'you', 'except', 'doesn', 'front', 'full', \"''\", 'ff', 'worse', 'among', 'each', \"they'd\", 'were', 'co', 'an', 'also', 'someone', 'since', 'inwards', \"we'll\", 'wherefore', 'hereby', 'up', 'wherein', 'below', 'even', 'double', 'does', 'same', 'whoever', 'seeing', 'should', 'whenever', 'of']\n"
     ]
    }
   ],
   "source": [
    "# GENERATING A LIST OF STOPWORDS\n",
    "# these various stopword lists and the combined joint list were tested on the same classifier (MultinomialNB) with the same\n",
    "# parameters, and it was found that the lemur list and the combined list, the latter includes the former, were the most\n",
    "# efficient ones\n",
    "\n",
    "from sklearn.feature_extraction import stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Straightforward Implementation of a text classifier (as a benchmark)\n",
    "Using the same two classifiers - Naive Bayes and SVM. The classification functions are generic, so you can use any other classifiers by just making minor modifications of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple straightforward\n",
    "def clf_simple(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; use TfidfVectorizer\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))\n",
    "    matrix_train = vectorizer.fit_transform(trainX)    # lowercase=True by default, initially min_df=15, max_df=0.23\n",
    "    matrix_test = vectorizer.transform(testX)\n",
    "                   \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = classifier\n",
    "    clf = clf.fit(matrix_train, trainY)  \n",
    "        \n",
    "    # predict and compute metrics    \n",
    "    predictions=clf.predict(matrix_test)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text Classifier with Pipeline (as a benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "def clf_pipe(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "       \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    predictions=clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} F-1 score:   {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text Classifier Using 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cross_val_score\n",
    "def clf_cv(classifier, data, labels):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()),                                                                # initially min_df=15, max_df=0.23,\n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "        \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "        \n",
    "    # fit classifier, predict, and compute metrics\n",
    "    clf = clf.fit(trainX, trainY)\n",
    "    seed = 7\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(clf, trainX, trainY, cv=kfold, scoring='f1_micro')\n",
    "    print('Cross-validated Accuracy of {}: {:0.4f} +/- {:0.4f}'.format(classifier_name, scores.mean(), scores.std() * 2))\n",
    "    predictions = clf.predict(testX)\n",
    "    cm          = metrics.confusion_matrix(testY, predictions)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Text Classifier with Cross-Validated Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "def clf_GridSearchCV(classifier, data, labels, param_grid):\n",
    "\n",
    "    # split data into train and test sets; create pipeline    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size = 0.2, random_state = 43)\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stopwords_combined, min_df=5, max_df=0.5, ngram_range=(1, 2))),  # lowercase=True by default\n",
    "                   ('tfidf', TfidfTransformer()), \n",
    "                   ('clf', classifier),\n",
    "                 ])   \n",
    "            \n",
    "    # get classifier's name to print results; otherwise, this function needs another argument\n",
    "    clf_string = str(classifier)\n",
    "    idx = clf_string.find(\"(\")\n",
    "    classifier_name = clf_string[:idx]\n",
    "    \n",
    "    # do 3-fold cross validation for each of the possible combinations of the parameter values above\n",
    "    grid = GridSearchCV(clf, cv=3, param_grid=param_grid, scoring='f1_micro')\n",
    "    grid.fit(trainX, trainY)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid.best_score_, \n",
    "        grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    # train and predict on test instances using the best configs found in the CV step\n",
    "        \n",
    "    #predictions = grid.best_estimator_.predict(testX)                   # this is how to find the best estimator \n",
    "    #testX = grid.best_estimator_.named_steps['tfidf'].transform(testX)  # this is how to find indiv. components (same for pipeline)\n",
    "    predictions=grid.predict(testX)                                      # called on the best estimator by default\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('{} cross-validated F-1 score with grid search: {:0.4f}'.format(classifier_name, score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    # return the best classifier to run it on the full dataset    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Running 7 options on the limited data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8525\n",
      "Confusion matrix:\n",
      "[[   0    0  804]\n",
      " [   1    0  455]\n",
      " [   0    0 7285]]\n",
      "\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultinomialNB()\n",
    "nb = MultinomialNB()\n",
    "nb_param_grid = {\n",
    "        'vect__max_df':[0.25, 0.5,0.75],\n",
    "        'vect__min_df':[5,15,25,50,100],\n",
    "        'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "        'clf__alpha':[0.1,0.25,0.5,0.75,1.0]\n",
    "    }\n",
    "clf_simple(nb, data, labels)    # straightforward NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB F-1 score:   0.8525\n",
      "Confusion matrix:\n",
      "[[   0    0  804]\n",
      " [   1    0  455]\n",
      " [   0    0 7285]]\n",
      "\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_pipe(nb, data, labels)    # pipeline NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of MultinomialNB: 0.8443 +/- 0.0136\n",
      "Confusion matrix:\n",
      "[[   0    0  804]\n",
      " [   1    0  455]\n",
      " [   0    0 7285]]\n",
      "\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(nb, data, labels)    # cross_val_score NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.858062 using {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.853088 (0.001000) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857419 (0.000886) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.857799 (0.000751) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853703 (0.000774) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858062 (0.000833) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858033 (0.001103) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852825 (0.001233) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.856131 (0.000983) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.856336 (0.000718) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.851040 (0.000881) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852649 (0.000571) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852825 (0.000545) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848524 (0.000714) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849138 (0.000819) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849109 (0.000749) with: {'clf__alpha': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.853030 (0.001038) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857389 (0.000946) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.857594 (0.000830) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853586 (0.000678) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.858033 (0.001049) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.858004 (0.001120) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852766 (0.000953) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.856336 (0.000747) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.856482 (0.000655) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850835 (0.000759) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852796 (0.000370) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852942 (0.000248) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848261 (0.000545) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.849021 (0.000640) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.849021 (0.000640) with: {'clf__alpha': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.852884 (0.001079) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.857389 (0.000790) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.857682 (0.000753) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.853469 (0.000696) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.857828 (0.001147) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.857916 (0.001157) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.852766 (0.000932) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.856278 (0.000820) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.856307 (0.000725) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850718 (0.000785) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852649 (0.000430) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852679 (0.000436) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848114 (0.000556) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848904 (0.000748) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848875 (0.000714) with: {'clf__alpha': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.849431 (0.000671) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.851157 (0.000299) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.851069 (0.000442) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.851830 (0.000497) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.856044 (0.001059) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.856307 (0.001065) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851918 (0.000938) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854756 (0.000600) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854727 (0.000439) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850631 (0.000611) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852415 (0.000573) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852445 (0.000578) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848261 (0.000572) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848875 (0.000643) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848816 (0.000676) with: {'clf__alpha': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.849460 (0.000807) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.850952 (0.000266) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.851069 (0.000535) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.851625 (0.000355) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.855985 (0.000901) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.856102 (0.000942) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851947 (0.000995) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854522 (0.000573) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854785 (0.000540) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850484 (0.000532) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852445 (0.000425) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852445 (0.000282) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.848143 (0.000538) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848729 (0.000533) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848758 (0.000568) with: {'clf__alpha': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.849372 (0.000819) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.850865 (0.000327) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.850894 (0.000467) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.851596 (0.000334) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.855751 (0.000852) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.855868 (0.000876) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.851801 (0.000939) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.854551 (0.000439) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.854698 (0.000497) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.850162 (0.000629) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.852357 (0.000491) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.852357 (0.000353) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847968 (0.000497) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848670 (0.000568) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848670 (0.000568) with: {'clf__alpha': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845715 (0.000182) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845891 (0.000313) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845832 (0.000232) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849021 (0.000626) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851538 (0.000329) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.851362 (0.000319) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.850104 (0.000837) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852328 (0.000640) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852386 (0.000548) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849402 (0.000800) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851157 (0.000926) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851128 (0.000817) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847822 (0.000339) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848553 (0.000587) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848582 (0.000627) with: {'clf__alpha': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845744 (0.000340) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845832 (0.000347) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845891 (0.000123) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.849168 (0.000541) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851362 (0.000450) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.851333 (0.000478) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849841 (0.000712) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852298 (0.000668) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852357 (0.000369) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849372 (0.000677) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.851069 (0.000599) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.851040 (0.000438) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847822 (0.000462) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848378 (0.000390) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848465 (0.000425) with: {'clf__alpha': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.845539 (0.000235) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.845744 (0.000347) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.845686 (0.000160) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.848933 (0.000585) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.851333 (0.000501) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.851157 (0.000501) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.849694 (0.000819) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.852035 (0.000737) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.852094 (0.000559) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.849285 (0.000622) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850923 (0.000657) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850923 (0.000519) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847529 (0.000503) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.848026 (0.000282) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.848114 (0.000282) with: {'clf__alpha': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844749 (0.000042) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844691 (0.000099) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844779 (0.000119) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.847090 (0.000320) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.848495 (0.000386) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.848378 (0.000541) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848114 (0.000708) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.850045 (0.000651) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850045 (0.000603) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848699 (0.000800) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850192 (0.000776) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.850104 (0.000774) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847295 (0.000665) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847822 (0.000407) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847880 (0.000396) with: {'clf__alpha': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844720 (0.000017) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844662 (0.000193) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844632 (0.000017) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846973 (0.000108) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.848378 (0.000232) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.848231 (0.000318) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848436 (0.000668) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849987 (0.000760) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.850075 (0.000642) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848729 (0.000631) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.850016 (0.000663) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849899 (0.000547) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847295 (0.000645) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847880 (0.000441) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847909 (0.000400) with: {'clf__alpha': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844720 (0.000068) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844574 (0.000123) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844574 (0.000031) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.846973 (0.000140) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.848143 (0.000252) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.848114 (0.000333) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.848143 (0.000671) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.849870 (0.000774) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.849841 (0.000751) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.848641 (0.000636) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849958 (0.000694) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849841 (0.000650) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.847090 (0.000759) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847705 (0.000376) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847705 (0.000376) with: {'clf__alpha': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844428 (0.000026) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844340 (0.000048) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844369 (0.000055) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845803 (0.000088) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.846710 (0.000447) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.846710 (0.000447) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.846944 (0.000473) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.848407 (0.000737) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.848378 (0.000830) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.847939 (0.000570) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849255 (0.000383) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849314 (0.000466) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.846915 (0.000568) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847353 (0.000500) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847441 (0.000438) with: {'clf__alpha': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844398 (0.000067) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844340 (0.000048) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844340 (0.000071) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845803 (0.000182) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.846710 (0.000587) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.846534 (0.000658) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.846710 (0.000392) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.848231 (0.000649) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.848202 (0.000823) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.847939 (0.000629) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849343 (0.000443) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849402 (0.000466) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.846915 (0.000569) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847412 (0.000616) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847412 (0.000616) with: {'clf__alpha': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.844369 (0.000055) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.844311 (0.000031) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.844369 (0.000068) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.845773 (0.000140) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.846563 (0.000554) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.846359 (0.000549) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.846622 (0.000381) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.848026 (0.000777) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.847939 (0.000797) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.847763 (0.000507) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.849168 (0.000507) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.849285 (0.000608) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.846739 (0.000503) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.847236 (0.000671) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.847236 (0.000671) with: {'clf__alpha': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB cross-validated F-1 score with grid search: 0.8651\n",
      "Confusion matrix:\n",
      "[[  51   20  733]\n",
      " [  30   92  334]\n",
      " [  31    5 7249]]\n",
      "\n",
      "Wall time: 2h 14min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_NB = clf_GridSearchCV(nb, data, labels, nb_param_grid)    # parameter grid search NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC F-1 score:   0.8749\n",
      "Confusion matrix:\n",
      "[[ 168   74  562]\n",
      " [  91  188  177]\n",
      " [ 133   32 7120]]\n",
      "\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC()\n",
    "svc_param_grid = {\n",
    "    'vect__max_df':[0.25,0.5,0.75],\n",
    "    'vect__min_df':[5,15,25,50,100],\n",
    "    'vect__ngram_range':[(1, 1),(1, 2),(1, 3)],\n",
    "    'clf__C':[0.1,0.25,0.5,0.75,1.0]\n",
    "}\n",
    "clf_pipe(svc, data, labels)            # pipeline SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy of LinearSVC: 0.8687 +/- 0.0100\n",
      "Confusion matrix:\n",
      "[[ 168   74  562]\n",
      " [  91  188  177]\n",
      " [ 133   32 7120]]\n",
      "\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_cv(svc, data, labels)             # cross_val_score SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.870322 using {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.860637 (0.001145) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.858648 (0.001566) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.858443 (0.001287) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.862159 (0.001093) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.860901 (0.000779) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.860696 (0.000697) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.862832 (0.000769) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.861457 (0.001294) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.861398 (0.001236) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863358 (0.001302) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862978 (0.000895) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863007 (0.001017) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.860520 (0.000695) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.860901 (0.000746) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.860901 (0.000746) with: {'clf__C': 0.1, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.861808 (0.001112) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.859145 (0.000992) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.859057 (0.001129) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.863066 (0.001109) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.861310 (0.000821) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.861135 (0.000697) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863534 (0.000500) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862100 (0.000887) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862100 (0.000892) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864148 (0.000712) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862920 (0.000970) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862802 (0.000826) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.861895 (0.000298) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.861778 (0.000334) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.861866 (0.000334) with: {'clf__C': 0.1, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.861866 (0.001251) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.858940 (0.000917) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.858940 (0.000854) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.863241 (0.001049) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.861281 (0.000868) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.861193 (0.000588) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863622 (0.000618) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862042 (0.001102) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862100 (0.001007) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864236 (0.000709) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862949 (0.000922) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862890 (0.000934) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.861954 (0.000334) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.862042 (0.000122) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.862042 (0.000103) with: {'clf__C': 0.1, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867484 (0.001148) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867074 (0.000355) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867425 (0.000605) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867074 (0.000745) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868274 (0.000690) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.868450 (0.000679) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867513 (0.000372) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867396 (0.000520) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867835 (0.000446) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866957 (0.000978) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.866460 (0.000917) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.866372 (0.001106) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864061 (0.000763) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864382 (0.000963) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864412 (0.001019) with: {'clf__C': 0.25, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.868420 (0.000749) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867601 (0.000259) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867747 (0.000218) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868508 (0.000564) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868362 (0.000676) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.868420 (0.000748) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.868215 (0.000475) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.868040 (0.000667) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.868245 (0.000562) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867747 (0.001409) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.866665 (0.001440) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.866957 (0.001392) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864382 (0.000512) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865290 (0.000615) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865290 (0.000615) with: {'clf__C': 0.25, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.868303 (0.000605) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867572 (0.000355) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867777 (0.000330) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868391 (0.000610) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868567 (0.000745) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.868567 (0.000729) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.868333 (0.000574) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867806 (0.000725) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.868128 (0.000682) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.867923 (0.001452) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.866343 (0.001302) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.866548 (0.001246) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864675 (0.000500) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865085 (0.000951) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865143 (0.001028) with: {'clf__C': 0.25, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.868274 (0.000854) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869913 (0.000138) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869708 (0.000489) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.867074 (0.000494) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868128 (0.000589) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.868450 (0.000916) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867221 (0.000673) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867016 (0.000384) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.866987 (0.000661) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.865377 (0.001394) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.865114 (0.001359) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.865026 (0.001430) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863914 (0.001087) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864470 (0.000153) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864675 (0.000156) with: {'clf__C': 0.5, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.869035 (0.000782) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.870234 (0.000161) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.870322 (0.000473) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868215 (0.000801) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868274 (0.000512) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.868303 (0.000663) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867806 (0.000765) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867396 (0.000746) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867572 (0.001051) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866635 (0.000577) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.865699 (0.001680) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.865699 (0.001930) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864675 (0.000923) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865085 (0.000687) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865026 (0.000792) with: {'clf__C': 0.5, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.869123 (0.000815) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.870234 (0.000161) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.870147 (0.000541) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.868479 (0.000619) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.868391 (0.000563) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.868274 (0.000676) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.867601 (0.000943) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.867660 (0.000680) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.867601 (0.000786) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.866694 (0.000479) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.865787 (0.001448) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.865699 (0.001609) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864821 (0.001194) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.865231 (0.000915) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.865202 (0.000839) with: {'clf__C': 0.5, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.866080 (0.000980) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869298 (0.000514) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869123 (0.001264) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.863856 (0.000655) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.865465 (0.000544) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.865641 (0.000158) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.864763 (0.001024) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.864997 (0.001617) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864968 (0.000994) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864061 (0.001132) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863388 (0.002258) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863358 (0.002214) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863300 (0.000989) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864002 (0.000845) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863856 (0.000852) with: {'clf__C': 0.75, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867133 (0.001265) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869357 (0.000598) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869357 (0.000910) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.864997 (0.000643) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866080 (0.000344) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.865875 (0.000374) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.866167 (0.000913) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.864821 (0.001071) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.864909 (0.001194) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864821 (0.001109) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863710 (0.002568) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863797 (0.002338) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.864295 (0.001664) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864295 (0.000686) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864412 (0.000718) with: {'clf__C': 0.75, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.867191 (0.001160) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.869298 (0.000698) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.869357 (0.000875) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.865260 (0.000833) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.866197 (0.000276) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.866050 (0.000331) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.865875 (0.001183) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.864938 (0.001018) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.865319 (0.001169) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.864734 (0.001150) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.863944 (0.002433) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.863827 (0.002305) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863973 (0.001883) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.864119 (0.000687) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.864148 (0.000774) with: {'clf__C': 0.75, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.863563 (0.001322) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.866870 (0.000622) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867981 (0.000392) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.860169 (0.001142) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.863154 (0.000421) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.862364 (0.000342) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.862159 (0.000563) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.861837 (0.001312) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.861486 (0.001431) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.862539 (0.000629) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862159 (0.002253) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862217 (0.002322) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.862451 (0.001523) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863095 (0.000466) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863066 (0.000385) with: {'clf__C': 1.0, 'vect__max_df': 0.25, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864617 (0.001847) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867484 (0.000982) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867806 (0.000444) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.862685 (0.001747) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.863417 (0.000678) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.863475 (0.000424) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863358 (0.001342) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862217 (0.000956) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862451 (0.001016) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863680 (0.001174) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862042 (0.001943) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.861954 (0.001722) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863973 (0.001768) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863358 (0.000769) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863241 (0.000562) with: {'clf__C': 1.0, 'vect__max_df': 0.5, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n",
      "0.864763 (0.001518) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}\n",
      "0.867689 (0.000968) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 2)}\n",
      "0.867894 (0.000946) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}\n",
      "0.862773 (0.001504) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 1)}\n",
      "0.863505 (0.000546) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 2)}\n",
      "0.863358 (0.000279) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}\n",
      "0.863358 (0.001399) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 1)}\n",
      "0.862100 (0.001016) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 2)}\n",
      "0.862276 (0.000862) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 25, 'vect__ngram_range': (1, 3)}\n",
      "0.863768 (0.001169) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 1)}\n",
      "0.862510 (0.002278) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 2)}\n",
      "0.862539 (0.001873) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 50, 'vect__ngram_range': (1, 3)}\n",
      "0.863914 (0.001943) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 1)}\n",
      "0.863358 (0.000840) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 2)}\n",
      "0.863212 (0.000883) with: {'clf__C': 1.0, 'vect__max_df': 0.75, 'vect__min_df': 100, 'vect__ngram_range': (1, 3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC cross-validated F-1 score with grid search: 0.8786\n",
      "Confusion matrix:\n",
      "[[ 137   65  602]\n",
      " [  77  177  202]\n",
      " [  72   19 7194]]\n",
      "\n",
      "Wall time: 3h 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_SVM = clf_GridSearchCV(svc, data, labels, svc_param_grid)    # parameter grid search SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Running Naive Bayes and SVM with the Best Parameters from Grid Search on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full dataset and labels\n",
    "full_data = df_text['reviewText'].values\n",
    "full_labels = df_text['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Naive Bayes F-1 score on full dataset: 0.8753\n",
      "Confusion matrix:\n",
      "[[  4298   1406  13830]\n",
      " [  1596   4767   5069]\n",
      " [  2169    430 162959]]\n",
      "\n",
      "The best SVM F-1 score on full dataset: 0.8879\n",
      "Confusion matrix:\n",
      "[[  5421   1922  12191]\n",
      " [  1947   6253   3232]\n",
      " [  2189    557 162812]]\n",
      "\n",
      "Wall time: 22min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the two best classifier on it\n",
    "for best_clf in [best_NB, best_SVM]:\n",
    "        \n",
    "    # split into train and test sets\n",
    "    trainX, testX, trainY, testY = train_test_split(full_data, full_labels, test_size = 0.2, random_state = 43)\n",
    "    clf = best_clf.fit(trainX, trainY)\n",
    "    \n",
    "    # predict and compute metrics\n",
    "    predictions = clf.predict(testX)\n",
    "    score = metrics.f1_score(testY, predictions, average='micro')\n",
    "    cm    = metrics.confusion_matrix(testY, predictions)\n",
    "    print('The best {} F-1 score on full dataset: {:0.4f}'.format('Naive Bayes' if best_clf==best_NB else 'SVM', score))\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
