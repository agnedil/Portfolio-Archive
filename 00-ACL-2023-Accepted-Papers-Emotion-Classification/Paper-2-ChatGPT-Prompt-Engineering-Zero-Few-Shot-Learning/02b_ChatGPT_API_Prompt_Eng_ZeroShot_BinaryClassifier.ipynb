{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d224855",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Text Classification (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "## The Association for Computational Linguistics\n",
    "## WASSA 2023 Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages\n",
    "See more details [here](https://codalab.lisn.upsaclay.fr/competitions/10864#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import time\n",
    "import zipfile, pickle\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "import random\n",
    "import tiktoken\n",
    "import backoff\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91371be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    '''Return number of tokens used in a list of messages for ChatGPT'''\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        #print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        #print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        #print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 4) (1191, 10) (1191, 1) (1191, 1)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/mcec_train_translated.pkl'\n",
    "df_train = pd.read_pickle(file1)\n",
    "\n",
    "file2    = 'data/mcec_dev_translated.pkl'\n",
    "df_dev   = pd.read_pickle(file2)\n",
    "\n",
    "file3    = 'data/mcec_test.csv'\n",
    "df_test  = pd.read_csv(file3)\n",
    "\n",
    "file4    = 'data/sample_submission/predictions_MCEC.csv'\n",
    "sample_submission = pd.read_csv(file4)\n",
    "\n",
    "print(df_train.shape, df_dev.shape, df_test.shape, sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb30baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                        Tension lene ki koi baat ni   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   \n",
       "2            Nai mje nai mili mail..mene check ki ti   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   \n",
       "\n",
       "                                          text_clean    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti  pessimism       0   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       0   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \n",
       "0                         Any talk of taking tangoes  \n",
       "1              I have gone home punch and now dreams  \n",
       "2                                Ni Ni Ni Mille Mail  \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  \n",
       "4                              But Wu runs the cedar  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy manually arbitrated translation into English from column 'gpt_translated2_corrected'\n",
    "#file = 'data/mcec_dev.xlsx'\n",
    "#df_dev2 = pd.read_excel( file )\n",
    "#print(df_dev2.shape)\n",
    "#df_dev2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea2ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dev['gpt_translated2_corrected'] = df_dev2['gpt_translated2_corrected'].values\n",
    "\n",
    "#file2  = 'data/mcec_dev_translated.pkl'\n",
    "#df_dev.to_pickle( file2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "33c8c2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Razia bta rahe the but wo sure nahe the</td>\n",
       "      <td>Razia bta rahe the but wo sure nahe the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me phr kuch parh hi lun :-P</td>\n",
       "      <td>Me phr kuch parh hi lun :- P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoxtl life ma hm bht jald matur hO jaty hai</td>\n",
       "      <td>Hoxtl life ma hm bht jald matur hO jaty hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yar A4 me seminar ha a ja..</td>\n",
       "      <td>Yar A 4 me seminar ha a ja ..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K.Quid e azam k 400 bnay hain</td>\n",
       "      <td>K . Quid e azam k 400 bnay hain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text  \\\n",
       "0      Razia bta rahe the but wo sure nahe the   \n",
       "1                  Me phr kuch parh hi lun :-P   \n",
       "2  Hoxtl life ma hm bht jald matur hO jaty hai   \n",
       "3                  Yar A4 me seminar ha a ja..   \n",
       "4                K.Quid e azam k 400 bnay hain   \n",
       "\n",
       "                                    text_clean  \n",
       "0      Razia bta rahe the but wo sure nahe the  \n",
       "1                 Me phr kuch parh hi lun :- P  \n",
       "2  Hoxtl life ma hm bht jald matur hO jaty hai  \n",
       "3                Yar A 4 me seminar ha a ja ..  \n",
       "4              K . Quid e azam k 400 bnay hain  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8a2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pessimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotion\n",
       "0    neutral\n",
       "1    neutral\n",
       "2  pessimism\n",
       "3    disgust\n",
       "4       fear"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission format\n",
    "print( type(sample_submission) )\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = df_train['emotion'].apply( lambda x: 0 if x=='neutral' else 1 )\n",
    "df_dev['target']   = df_dev['emotion'].apply( lambda x: 0 if x=='neutral' else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         3262\n",
      "trust           1118\n",
      "joy             1022\n",
      "optimism         880\n",
      "anticipation     832\n",
      "disgust          687\n",
      "sadness          486\n",
      "fear             453\n",
      "anger            226\n",
      "surprise         199\n",
      "love             187\n",
      "pessimism        178\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "1    6268\n",
      "0    3262\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes.I am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yes.i am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>Y. Um in Fap Lab Cabin. Butt Fap Presentations...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yar insan ka bcha bn chawliyn na mar :p</td>\n",
       "      <td>joy</td>\n",
       "      <td>Dude become a child of a human being, do not die.</td>\n",
       "      <td>Dude human beings do not die: P: P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terai uncle nai kahna hai kai ham nai to bahr ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>Your Uncle Nai says that we had sent out money</td>\n",
       "      <td>Your Ankali says that we sent out money and wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr ajao I m cming in the club</td>\n",
       "      <td>neutral</td>\n",
       "      <td>YR AJAO I'M Coming in the Club</td>\n",
       "      <td>Yer organs were the club</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...</td>\n",
       "      <td>joy</td>\n",
       "      <td>Mje wes nimra ahmad ka qur'aan ki aayaat k bar...</td>\n",
       "      <td>Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion  \\\n",
       "0  Yes.I am in fyp lab cabin.but fyp presentation...  neutral   \n",
       "1           Yar insan ka bcha bn chawliyn na mar :p       joy   \n",
       "2  Terai uncle nai kahna hai kai ham nai to bahr ...  disgust   \n",
       "3                      Yr ajao I m cming in the club  neutral   \n",
       "4  Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...      joy   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0  Yes.i am in fyp lab cabin.but fyp presentation...   \n",
       "1  Dude become a child of a human being, do not die.   \n",
       "2     Your Uncle Nai says that we had sent out money   \n",
       "3                     YR AJAO I'M Coming in the Club   \n",
       "4  Mje wes nimra ahmad ka qur'aan ki aayaat k bar...   \n",
       "\n",
       "                                       translated_ur  target  \n",
       "0  Y. Um in Fap Lab Cabin. Butt Fap Presentations...       0  \n",
       "1                 Dude human beings do not die: P: P       1  \n",
       "2  Your Ankali says that we sent out money and wa...       1  \n",
       "3                           Yer organs were the club       0  \n",
       "4  Mje Wese Nimra Ahmad Ka Qur'aan Ki Aayaaat K B...       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['emotion'].value_counts(), '\\n')\n",
    "print(df_train['target'].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1742539f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         388\n",
      "joy             131\n",
      "trust           125\n",
      "disgust         113\n",
      "optimism        110\n",
      "anticipation     94\n",
      "sadness          62\n",
      "fear             52\n",
      "surprise         35\n",
      "anger            35\n",
      "pessimism        29\n",
      "love             17\n",
      "Name: emotion, dtype: int64 \n",
      "\n",
      "1    803\n",
      "0    388\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gtp_translated</th>\n",
       "      <th>translated_hi</th>\n",
       "      <th>translated_ur</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>gpt_pred_num</th>\n",
       "      <th>gpt_translated2</th>\n",
       "      <th>gpt_translated2_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>There's no need to take tension.</td>\n",
       "      <td>There is nothing to take tension</td>\n",
       "      <td>Any talk of taking tangoes</td>\n",
       "      <td>Tension lene ki koi baat ni</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "      <td>There's no need to worry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have gone home punch and now I am Sapni</td>\n",
       "      <td>I have gone home punch and now dreams</td>\n",
       "      <td>Main ghar punch gya hun or ab spny laga hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "      <td>I have reached home and now I am going to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nai mje nai mili mail..mene check ki ti</td>\n",
       "      <td>pessimism</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any mail, I had checked.</td>\n",
       "      <td>Nai Maje Nai Mile Mail .. I checked</td>\n",
       "      <td>Ni Ni Ni Mille Mail</td>\n",
       "      <td>Nai mje nai mili mail .. mene check ki ti</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "      <td>I didn't receive any new mail. I had checked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>That day, they were busy all day and not givin...</td>\n",
       "      <td>YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>Yr us din mai pura din bzy rahe vo mujy awne h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "      <td>I was busy the whole day on that day, they wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks cautiously.</td>\n",
       "      <td>But it still moves at the rate</td>\n",
       "      <td>But Wu runs the cedar</td>\n",
       "      <td>Lakin wo abhe dar dar ka chalata ha</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "      <td>But he still walks with fear and hesitation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    emotion  target  \\\n",
       "0                        Tension lene ki koi baat ni    neutral       0   \n",
       "1        Main ghar punch gya hun or ab spny laga hun    neutral       0   \n",
       "2            Nai mje nai mili mail..mene check ki ti  pessimism       1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...    disgust       1   \n",
       "4                Lakin wo abhe dar dar ka chalata ha       fear       1   \n",
       "\n",
       "                                      gtp_translated  \\\n",
       "0                   There's no need to take tension.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2          I didn't receive any mail, I had checked.   \n",
       "3  That day, they were busy all day and not givin...   \n",
       "4                     But he still walks cautiously.   \n",
       "\n",
       "                                       translated_hi  \\\n",
       "0                   There is nothing to take tension   \n",
       "1          I have gone home punch and now I am Sapni   \n",
       "2                Nai Maje Nai Mile Mail .. I checked   \n",
       "3  YR Us Din Mai Pura Din Bzy Rahe Vo Mujy Awne H...   \n",
       "4                     But it still moves at the rate   \n",
       "\n",
       "                                       translated_ur  \\\n",
       "0                         Any talk of taking tangoes   \n",
       "1              I have gone home punch and now dreams   \n",
       "2                                Ni Ni Ni Mille Mail   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...   \n",
       "4                              But Wu runs the cedar   \n",
       "\n",
       "                                          text_clean  gpt_pred  gpt_pred_num  \\\n",
       "0                        Tension lene ki koi baat ni   neutral             1   \n",
       "1        Main ghar punch gya hun or ab spny laga hun   neutral             1   \n",
       "2          Nai mje nai mili mail .. mene check ki ti   neutral             1   \n",
       "3  Yr us din mai pura din bzy rahe vo mujy awne h...  negative             0   \n",
       "4                Lakin wo abhe dar dar ka chalata ha   neutral             1   \n",
       "\n",
       "                                     gpt_translated2  \\\n",
       "0                          There's no need to worry.   \n",
       "1   I have reached home and now I am going to sleep.   \n",
       "2      I didn't receive any new mail. I had checked.   \n",
       "3  I was busy the whole day on that day, they wer...   \n",
       "4       But he still walks with fear and hesitation.   \n",
       "\n",
       "                           gpt_translated2_corrected  \n",
       "0                          There's no need to worry.  \n",
       "1   I have reached home and now I am going to sleep.  \n",
       "2      I didn't receive any new mail. I had checked.  \n",
       "3  I was busy the whole day on that day, they wer...  \n",
       "4       But he still walks with fear and hesitation.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_dev['emotion'].value_counts(), '\\n')\n",
    "print(df_dev['target'].value_counts())\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# light text cleaning (should I use clean regex for better accuracy?)\n",
    "pad_punct    = re.compile('([^a-zA-Z ]+)')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "#clean        = re.compile('[^a-zA-Z0-9,.?!\\'\\s]+')\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = pad_punct.sub(r' \\1 ', s)\n",
    "    #s = clean.sub(' ', s)\n",
    "    s = multi_spaces.sub(' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply( clean_text )\n",
    "df_dev['text_clean']   = df_dev['text'].apply( clean_text )\n",
    "df_test['text_clean']  = df_test['Text'].apply( clean_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e42251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 6)\n",
      "(4222, 6)\n",
      "(4221, 6)\n",
      "(4221, 6)\n"
     ]
    }
   ],
   "source": [
    "# 2K duplicates - these may affect claa imbalance during training! TO BE REDUCED\n",
    "print(df_train.shape)\n",
    "temp1 = df_train[ df_train.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_train[ df_train.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_train[ df_train.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7c2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 11)\n",
      "(82, 11)\n",
      "(82, 11)\n",
      "(68, 11)\n"
     ]
    }
   ],
   "source": [
    "# 82 duplicates ['clean_text', 'emotion'] - can't reduce because this is a dev set\n",
    "print(df_dev.shape)\n",
    "temp1 = df_dev[ df_dev.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp2 = df_dev[ df_dev.duplicated(subset=['text_clean', 'emotion'], keep=False) ]\n",
    "print(temp2.shape)\n",
    "temp3 = df_dev[ df_dev.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c179df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191, 2)\n",
      "(93, 2)\n",
      "(93, 2)\n"
     ]
    }
   ],
   "source": [
    "# 93 complete duplicates - can't reduce because this is a test set\n",
    "print(df_test.shape)\n",
    "temp1 = df_test[ df_test.duplicated(subset=['text_clean'], keep=False) ]\n",
    "print(temp1.shape)\n",
    "temp3 = df_test[ df_test.duplicated(keep=False) ]\n",
    "print(temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f49ff482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 554, 526, 526)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train vs. df_dev: half of the dev set is in train set\n",
    "overlap1 = [t for t in df_train['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "overlap2 = [t for t in df_dev['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap1), len(overlap2), len(set(overlap1)), len(set(overlap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae49f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 584, 557, 557)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. rest\n",
    "overlap3 = [ t for t in df_train['text_clean'].tolist() + df_dev['text_clean'].tolist()\\\n",
    "             if t in df_test['text_clean'].tolist() ]\n",
    "overlap4 = [ t for t in df_test['text_clean'].tolist() if t in\\\n",
    "             df_train['text_clean'].tolist() + df_dev['text_clean'].tolist() ]\n",
    "len(overlap3), len(overlap4), len(set(overlap3)), len(set(overlap4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b78132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 97, 88, 88)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_dev\n",
    "overlap5 = [t for t in df_dev['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap6 = [t for t in df_test['text_clean'].values if t in df_dev['text_clean'].values]\n",
    "len(overlap5), len(overlap6), len(set(overlap5)), len(set(overlap6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13bff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727, 540, 519, 519)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test vs. df_train: half of the dev set is in train set\n",
    "overlap7 = [t for t in df_train['text_clean'].values if t in df_test['text_clean'].values]\n",
    "overlap8 = [t for t in df_test['text_clean'].values if t in df_train['text_clean'].values]\n",
    "len(overlap7), len(overlap8), len(set(overlap7)), len(set(overlap8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a84db",
   "metadata": {},
   "source": [
    "The reason why baseline ML models perform better than ChatGPT is because they get a lot of hints due to duplicates from the training set! ChatGPT doesn't have this knowledge because it's doing a zero-shot classification! The number of duplicates is such that they would not fit the context window of ChatGPT anyway.\n",
    "\n",
    "The only way to compare ML and ChatGPT correctly is to remove all the duplicates from the TRAINING SET, then train ML model and test it the dev set and compare with ChatGPT! (also, deduplicate the training set)\n",
    "\n",
    "Submission: use non-overfit ML or ChatGPT (whichever is better) on those samples from the test set that don't have duplicates in the training or dev set. Use training/dev set labels for the duploicates in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fbd6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 2206\n",
      "(9530, 6)\n",
      "(8151, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove overlap with validation sets\n",
    "val_sets = df_dev['text_clean'].tolist() + df_test['text_clean'].tolist()\n",
    "print(len(val_sets), len(set(val_sets)))\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train = df_train[ ~df_train['text_clean'].isin(val_sets) ]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1293d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates from train set\n",
    "df_train = df_train.drop_duplicates(subset=['text_clean', 'emotion'])\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40905d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 2127),\n",
       " ('k', 1244),\n",
       " ('to', 1231),\n",
       " ('ha', 1214),\n",
       " ('hai', 804),\n",
       " ('ho', 793),\n",
       " ('ka', 726),\n",
       " ('me', 640),\n",
       " ('?', 615),\n",
       " ('b', 604),\n",
       " ('kr', 568),\n",
       " ('ga', 559),\n",
       " ('ni', 553),\n",
       " ('ko', 543),\n",
       " ('ki', 532),\n",
       " ('tha', 528),\n",
       " (',', 518),\n",
       " ('...', 502),\n",
       " ('na', 497),\n",
       " ('hn', 473),\n",
       " ('hy', 464),\n",
       " ('wo', 461),\n",
       " ('ma', 453),\n",
       " ('nai', 450),\n",
       " ('..', 450),\n",
       " ('a', 446),\n",
       " ('se', 415),\n",
       " ('p', 409),\n",
       " ('yar', 401),\n",
       " ('or', 392),\n",
       " ('yr', 389),\n",
       " ('h', 388),\n",
       " ('i', 385),\n",
       " ('han', 385),\n",
       " ('tu', 371),\n",
       " ('e', 331),\n",
       " (':', 327),\n",
       " ('ne', 324),\n",
       " ('kia', 321),\n",
       " ('he', 287),\n",
       " ('hain', 284),\n",
       " ('main', 281),\n",
       " ('ab', 254),\n",
       " ('koi', 252),\n",
       " ('us', 251),\n",
       " ('nae', 250),\n",
       " ('ap', 250),\n",
       " ('sir', 250),\n",
       " ('sy', 248),\n",
       " ('tm', 237),\n",
       " ('is', 223),\n",
       " ('nahi', 223),\n",
       " ('hi', 222),\n",
       " ('raha', 220),\n",
       " ('kal', 218),\n",
       " ('rha', 214),\n",
       " ('ja', 202),\n",
       " ('ny', 200),\n",
       " ('aj', 199),\n",
       " ('g', 199),\n",
       " ('m', 198),\n",
       " ('phr', 195),\n",
       " (':-', 193),\n",
       " ('aur', 192),\n",
       " ('mai', 192),\n",
       " ('....', 187),\n",
       " ('gya', 184),\n",
       " ('d', 183),\n",
       " ('bht', 181),\n",
       " ('u', 173),\n",
       " ('pta', 172),\n",
       " ('kar', 171),\n",
       " ('the', 169),\n",
       " ('mein', 168),\n",
       " ('jana', 168),\n",
       " ('ya', 167),\n",
       " ('time', 166),\n",
       " ('ok', 165),\n",
       " ('ye', 163),\n",
       " ('nhi', 161),\n",
       " ('kya', 161),\n",
       " ('jo', 161),\n",
       " ('in', 157),\n",
       " ('pe', 157),\n",
       " ('n', 154),\n",
       " ('0', 153),\n",
       " ('hu', 152),\n",
       " ('hun', 151),\n",
       " ('kam', 150),\n",
       " ('aa', 149),\n",
       " ('bhi', 147),\n",
       " ('abi', 142),\n",
       " (\"'\", 140),\n",
       " ('do', 138),\n",
       " ('you', 135),\n",
       " ('mera', 135),\n",
       " ('kuch', 135),\n",
       " ('uni', 134),\n",
       " ('o', 131),\n",
       " ('msg', 127),\n",
       " ('ke', 125),\n",
       " ('aya', 124),\n",
       " ('sb', 123),\n",
       " ('!', 122),\n",
       " ('pata', 119),\n",
       " ('say', 119),\n",
       " ('bhai', 118),\n",
       " ('tk', 118),\n",
       " ('sath', 118),\n",
       " ('le', 117),\n",
       " ('gy', 117),\n",
       " ('send', 116),\n",
       " ('thi', 114),\n",
       " ('ana', 114),\n",
       " ('ge', 112),\n",
       " ('kaha', 111),\n",
       " ('bta', 110),\n",
       " ('2', 110),\n",
       " ('pas', 109),\n",
       " ('ly', 109),\n",
       " ('plz', 108),\n",
       " ('gi', 107),\n",
       " ('ghr', 106),\n",
       " ('bs', 105),\n",
       " ('??', 104),\n",
       " ('r', 103),\n",
       " ('bat', 103),\n",
       " ('no', 102),\n",
       " ('and', 101),\n",
       " ('dy', 100),\n",
       " ('meri', 98),\n",
       " ('keh', 97),\n",
       " ('mje', 96),\n",
       " ('din', 96),\n",
       " ('allah', 96),\n",
       " ('krna', 95),\n",
       " ('for', 95),\n",
       " ('kiya', 95),\n",
       " ('agr', 94),\n",
       " ('1', 92),\n",
       " ('mere', 92),\n",
       " ('bt', 91),\n",
       " ('acha', 90),\n",
       " ('lo', 89),\n",
       " ('ghar', 87),\n",
       " ('hon', 86),\n",
       " ('but', 85),\n",
       " ('thy', 85),\n",
       " ('baat', 85),\n",
       " ('de', 84),\n",
       " ('so', 84),\n",
       " ('par', 82),\n",
       " ('hota', 82),\n",
       " ('of', 82),\n",
       " ('my', 82),\n",
       " ('liye', 82),\n",
       " ('tak', 81),\n",
       " ('mjy', 81),\n",
       " ('be', 80),\n",
       " ('s', 80),\n",
       " ('yaar', 80),\n",
       " ('khud', 79),\n",
       " ('abhi', 79),\n",
       " ('class', 78),\n",
       " ('hua', 78),\n",
       " ('at', 77),\n",
       " ('aik', 77),\n",
       " ('py', 77),\n",
       " ('t', 76),\n",
       " ('kha', 76),\n",
       " ('jao', 76),\n",
       " ('kisi', 75),\n",
       " ('gai', 74),\n",
       " ('on', 73),\n",
       " ('wese', 73),\n",
       " ('mil', 73),\n",
       " ('may', 73),\n",
       " ('sa', 73),\n",
       " ('pr', 73),\n",
       " ('kro', 73),\n",
       " ('ra', 72),\n",
       " ('wala', 71),\n",
       " ('bi', 71),\n",
       " ('thk', 71),\n",
       " ('will', 71),\n",
       " ('have', 70),\n",
       " (':)', 70),\n",
       " ('c', 70),\n",
       " ('???', 70),\n",
       " ('it', 69),\n",
       " ('jani', 69),\n",
       " ('kb', 69),\n",
       " ('tum', 68),\n",
       " ('.....', 68),\n",
       " ('bus', 66),\n",
       " ('hum', 66),\n",
       " ('wali', 65),\n",
       " ('hay', 65),\n",
       " ('log', 64),\n",
       " ('call', 63),\n",
       " ('krta', 63),\n",
       " ('tw', 63),\n",
       " ('rhi', 63),\n",
       " ('we', 62),\n",
       " ('phir', 62),\n",
       " ('free', 61),\n",
       " ('kis', 61),\n",
       " ('hahaha', 61),\n",
       " ('3', 61),\n",
       " ('apni', 61),\n",
       " ('thek', 60),\n",
       " ('lab', 59),\n",
       " ('un', 59),\n",
       " ('subha', 59),\n",
       " ('th', 59),\n",
       " ('dia', 58),\n",
       " ('chal', 58),\n",
       " ('jb', 58),\n",
       " ('mama', 57),\n",
       " ('w', 57),\n",
       " ('q', 57),\n",
       " ('that', 56),\n",
       " ('aye', 56),\n",
       " (':-)', 55),\n",
       " ('ata', 54),\n",
       " ('teri', 54),\n",
       " ('v', 54),\n",
       " ('paper', 53),\n",
       " ('ek', 53),\n",
       " ('min', 53),\n",
       " ('bna', 53),\n",
       " ('jaye', 53),\n",
       " ('bjy', 52),\n",
       " ('papa', 52),\n",
       " ('sai', 52),\n",
       " ('pass', 52),\n",
       " ('mene', 52),\n",
       " ('gae', 52),\n",
       " ('dil', 52),\n",
       " ('yad', 51),\n",
       " ('nd', 51),\n",
       " ('haha', 51),\n",
       " ('mjhe', 51),\n",
       " ('oye', 50),\n",
       " ('dena', 50),\n",
       " ('8', 50),\n",
       " ('mery', 50),\n",
       " ('pa', 50),\n",
       " ('wahan', 50),\n",
       " ('ur', 49),\n",
       " ('dekh', 48),\n",
       " ('ao', 48),\n",
       " ('tera', 48),\n",
       " ('rhy', 48),\n",
       " ('laga', 48),\n",
       " ('hoti', 48),\n",
       " ('jaldi', 48),\n",
       " ('rahi', 48),\n",
       " ('karna', 48),\n",
       " ('mujy', 47),\n",
       " ('office', 47),\n",
       " ('am', 46),\n",
       " ('hm', 46),\n",
       " ('gay', 46),\n",
       " ('late', 46),\n",
       " ('kch', 46),\n",
       " ('eid', 46),\n",
       " ('khana', 45),\n",
       " ('not', 45),\n",
       " ('kahan', 45),\n",
       " ('da', 45),\n",
       " ('tou', 45),\n",
       " ('check', 45),\n",
       " ('yeh', 45),\n",
       " ('reply', 45),\n",
       " ('skta', 45),\n",
       " ('wapis', 44),\n",
       " ('bje', 44),\n",
       " ('if', 44),\n",
       " ('gaya', 44),\n",
       " ('jata', 44),\n",
       " ('use', 44),\n",
       " ('apna', 43),\n",
       " ('10', 43),\n",
       " ('ay', 43),\n",
       " ('bad', 43),\n",
       " ('kafi', 43),\n",
       " ('thora', 43),\n",
       " ('aaj', 43),\n",
       " ('krwa', 42),\n",
       " ('sab', 42),\n",
       " ('lye', 42),\n",
       " (':(', 42),\n",
       " ('good', 42),\n",
       " ('theek', 42),\n",
       " ('mn', 42),\n",
       " ('di', 42),\n",
       " ('nikal', 42),\n",
       " ('nay', 41),\n",
       " ('ae', 41),\n",
       " ('ku', 41),\n",
       " ('sorry', 41),\n",
       " ('day', 40),\n",
       " ('usy', 40),\n",
       " ('li', 40),\n",
       " ('4', 40),\n",
       " ('muje', 40),\n",
       " ('5', 40),\n",
       " ('ai', 39),\n",
       " ('lahore', 39),\n",
       " ('plan', 39),\n",
       " ('dua', 39),\n",
       " ('start', 38),\n",
       " (',,,', 38),\n",
       " ('7', 38),\n",
       " ('hui', 38),\n",
       " ('fb', 38),\n",
       " ('agar', 38),\n",
       " ('test', 38),\n",
       " ('bike', 38),\n",
       " ('nh', 38),\n",
       " ('chala', 37),\n",
       " ('krny', 37),\n",
       " ('jae', 37),\n",
       " ('lia', 37),\n",
       " ('kbi', 37),\n",
       " ('lga', 37),\n",
       " ('your', 37),\n",
       " ('dost', 37),\n",
       " ('mila', 37),\n",
       " ('net', 37),\n",
       " ('masla', 36),\n",
       " ('last', 36),\n",
       " ('try', 36),\n",
       " ('rat', 36),\n",
       " ('kaam', 36),\n",
       " ('lena', 36),\n",
       " ('sakta', 36),\n",
       " ('per', 36),\n",
       " ('are', 35),\n",
       " ('la', 35),\n",
       " ('hoon', 35),\n",
       " ('assignment', 35),\n",
       " ('haan', 35),\n",
       " ('rahy', 35),\n",
       " ('change', 35),\n",
       " ('lag', 35),\n",
       " ('krni', 35),\n",
       " ('ta', 35),\n",
       " ('pay', 35)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is additional text cleaning necessary? I don't see why\n",
    "from collections import Counter\n",
    "train_words = ' '.join( df_train['text_clean'].tolist() ).lower().split()\n",
    "c = Counter( train_words )\n",
    "c.most_common(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afcaeedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "102 318 129\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/owaisraza009/roman-urdu-sentiment-analysis/notebook\n",
    "stopwords1 = [ 'ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
    "               'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
    "               'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se',\n",
    "               'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski',\n",
    "               'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya',\n",
    "               'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi',\n",
    "               'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain',\n",
    "               'krny', 'tou', ]\n",
    "\n",
    "# https://github.com/haseebelahi/roman-urdu-stopwords.git\n",
    "file = 'data/stopwords.txt'\n",
    "stopwords2 = open(file).read().split()\n",
    "print(stopwords2 == stopwords1)\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "stopwords_en  = _stop_words.ENGLISH_STOP_WORDS\n",
    "# selected from stopwords_en\n",
    "stopwords_en2 = [ 'a', 'about', 'also', 'am', 'an', 'and', 'are', 'as', 'at', 'be', \n",
    "                  'been', 'being', 'by', 'co', 'con', 'de', 'eg', 'eight', 'eleven', 'else', 'etc', \n",
    "                  'fifteen', 'fifty', 'five', 'for', 'forty', 'four', 'from', 'had',\n",
    "                  'has', 'hasnt', 'have', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', \n",
    "                  'his', 'how', 'i', 'ie', 'if', 'in', 'inc', 'into', 'is', 'it', 'its', 'itself',\n",
    "                  'ltd', 'me', 'mine', 'my', 'myself', 'nine', 'no', 'now', 'of', 'off', 'on',\n",
    "                  'once', 'one', 'onto', 'or', 'other', 'others', 'our', 'ours', 'ourselves',\n",
    "                  'out', 'part', 'per', 're', 'several', 'she', 'side', 'since', 'six', 'sixty',\n",
    "                  'so', 'ten', 'than', 'that', 'the', 'their', 'them',\n",
    "                  'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', \n",
    "                  'three', 'to', 'twelve', 'twenty', 'two', 'un','us', 'very',\n",
    "                  'via', 'was', 'we', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', \n",
    "                  'who', 'whom', 'whose', 'why', 'with', 'within', 'would', 'yet', 'you', 'your', 'yours',\n",
    "                   'yourself', 'yourselves', ]\n",
    "\n",
    "print( len(stopwords1), len(stopwords_en), len(stopwords_en2), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7a589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396d0fce",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ad9e7384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"This is a text sample\" \n",
      "\n",
      "Are you sure about that? Output only the category\n"
     ]
    }
   ],
   "source": [
    "#prompt_one   = '''The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English only. Then classify the translated text as 'emotional' if it contains emotions or 'neutral' if it does not contain emotions. Output only 'emotional' or 'neutral' and nothing else. Text: \"{}\"'''\n",
    "prompt_one   = '''Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"{}\"'''\n",
    "s = 'This is a text sample'\n",
    "print(prompt_one.format(s), '\\n')\n",
    "\n",
    "# Using followup questions improves the reponse. but ChatGPT can change its mind too easily sometimes\n",
    "followup1 = 'Are you sure about that? If yes, output the same category, if no change the category'\n",
    "followup2 = 'Output only the category and nothing else'\n",
    "print(followup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7583ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral', 'emotional'}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model          = 'gpt-3.5-turbo'\n",
    "labels_set     = {'emotional', 'neutral'}\n",
    "clean = re.compile(r'[^a-zA-Z ]+')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee57217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_label(label_):\n",
    "    '''\n",
    "       Verify if label_ contains any of the categories\n",
    "       from the predefined set of labels\n",
    "    '''\n",
    "    label_ = clean.sub(' ', label_)\n",
    "    label_ = multi_spaces.sub(' ', label_).lower().split()\n",
    "    res    = [i for i in label_ if i in labels_set]\n",
    "    res    = list(set(res))\n",
    "    return '/'.join(res) if res else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7adff9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_num_tokens(model, messages):\n",
    "    '''Check that there is enough tokens available for a ChatGPT repsonse'''\n",
    "    num_tokens_tiktoken = num_tokens_from_messages(messages, model)\n",
    "    if num_tokens_tiktoken > 4080:\n",
    "        print(f'Number of tokens is {num_tokens_tiktoken} which exceeds 3950')\n",
    "        print(f'TEXT: {text_}\\n')\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError, max_time=10)\n",
    "def get_response(model, messages, temperature=0, max_tokens=None):\n",
    "    '''Send request, return reponse'''\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = temperature,        # range(0,2), the more the less deterministic / focused\n",
    "        top_p = 1,                        # top probability mass, e.g. 0.1 = only tokens from top 10% proba mass\n",
    "        n = 1,                            # number of chat completions\n",
    "        #max_tokens = max_tokens,          # tokens to return\n",
    "        stream = False,        \n",
    "        stop=None,                        # sequence to stop generation (new line, end of text, etc.)\n",
    "        )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    #num_tokens_api = response['usage']['prompt_tokens']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0eb629b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text_, prompt_):\n",
    "    '''Translate text_ using prompt_ and ChatGPT API'''    \n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [            \n",
    "            { \"role\": \"system\", \"content\": \"You are an accurate translator from Roman Urdu.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    return get_response(model, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1f9fa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text_, prompt_):\n",
    "    '''Classify text_ using prompt_ and ChatGPT API'''\n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            { \"role\": \"system\", \"content\": \"You are a smart binary text classifier.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    label_    = get_response(model, messages)\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)        # get just the category if response is too long\n",
    "        \n",
    "    # if label not found in response text - second, extended chat\n",
    "    if label_ is None:\n",
    "        messages += [\n",
    "            { \"role\": \"assistant\", \"content\": old_label, },\n",
    "            { \"role\": \"user\", \"content\": followup1, }\n",
    "            ]        \n",
    "        label_    = get_response(model, messages)        \n",
    "        old_label = label_\n",
    "        label_    = verify_label(label_)        # get just the category if response is too long\n",
    "            \n",
    "    return label_ if label_ is not None else old_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9ba77bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_with_clarifying(text_, prompt_):\n",
    "    '''\n",
    "       Classify text_ using prompt_ and ChatGPT API,\n",
    "       then clarify response with followup1 question -\n",
    "       this can help make the response more precise\n",
    "    '''\n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            { \"role\": \"system\", \"content\": \"You are a smart binary text classifier.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    if not verify_num_tokens(model, messages): return None\n",
    "    label_    = get_response(model, messages)\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)                      # get just the category if response is too long\n",
    "        \n",
    "    # ask additional clarifying question - sometimes it helps\n",
    "    messages += [\n",
    "        { \"role\": \"assistant\", \"content\": old_label, },\n",
    "        { \"role\": \"user\", \"content\": followup1, }\n",
    "        ]\n",
    "    #time.sleep( random.uniform(1.1, 1.8) )                # wait not to overload ChatGPT\n",
    "    label2_    = get_response(model, messages)\n",
    "    old_label2 = label2_\n",
    "    label2_    = verify_label(label2_)                    # get just the category if response is too long\n",
    "\n",
    "    return old_label, label_, old_label2, label2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8bb18d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"Dude, when did I ever say no to you guys? Come on over, I'm free right now anyway.\"\n",
      "\n",
      "GROUNDTRUTH LABEL:\n",
      "n/e/u/t/r/a/l\n",
      "\n",
      "PREDICTED LABEL:\n",
      "('Category: Emotional. \\n\\nExplanation: The use of the word \"Dude\" and the exclamation mark suggests a friendly and enthusiastic tone, which is a sign of positive emotion.', 'emotional', 'Emotional.', 'emotional')\n"
     ]
    }
   ],
   "source": [
    "# test as single prompt\n",
    "idx = 11\n",
    "text, groundtruth_labels = df_dev[['gpt_translated2_corrected', 'emotion']].values[idx]\n",
    "label  = classify_text(text, prompt_one)\n",
    "#labels = classify_text(text, prompt_one)\n",
    "labels = classify_text_with_clarifying(text, prompt_one)\n",
    "\n",
    "print(prompt_one.format( text ))\n",
    "print(f\"\\nGROUNDTRUTH LABEL:\\n{'/'.join( groundtruth_labels )}\")\n",
    "print(f\"\\nPREDICTED LABEL:\\n{labels}\")\n",
    "#print(f'\\nTOTAL TOKENS: {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fabab",
   "metadata": {},
   "source": [
    "### Zero-shot classification using simple iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8e4471fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 10\n",
      "Processing text 20\n",
      "Processing text 30\n",
      "Processing text 40\n",
      "Processing text 50\n",
      "Processing text 60\n",
      "Processing text 70\n",
      "Processing text 80\n",
      "Processing text 90\n",
      "Processing text 100\n",
      "Processing text 110\n",
      "Processing text 120\n",
      "Processing text 130\n",
      "Processing text 140\n",
      "Processing text 150\n",
      "Processing text 160\n",
      "Processing text 170\n",
      "Processing text 180\n",
      "Processing text 190\n",
      "Processing text 200\n",
      "Processing text 210\n",
      "Processing text 220\n",
      "Processing text 230\n",
      "Processing text 240\n",
      "Processing text 250\n",
      "Processing text 260\n",
      "Processing text 270\n",
      "Processing text 280\n",
      "Processing text 290\n",
      "Processing text 300\n",
      "Processing text 310\n",
      "Processing text 320\n",
      "Processing text 330\n",
      "Processing text 340\n",
      "Processing text 350\n",
      "Processing text 360\n",
      "Processing text 370\n",
      "Processing text 380\n",
      "Processing text 390\n",
      "Processing text 400\n",
      "Processing text 410\n",
      "Processing text 420\n",
      "Processing text 430\n",
      "Processing text 440\n",
      "Processing text 450\n",
      "Processing text 460\n",
      "Processing text 470\n",
      "Processing text 480\n",
      "Processing text 490\n",
      "Processing text 500\n",
      "Processing text 510\n",
      "Processing text 520\n",
      "Processing text 530\n",
      "Processing text 540\n",
      "Processing text 550\n",
      "Processing text 560\n",
      "Processing text 570\n",
      "Processing text 580\n",
      "Processing text 590\n",
      "Processing text 600\n",
      "Processing text 610\n",
      "Processing text 620\n",
      "Processing text 630\n",
      "Processing text 640\n",
      "Processing text 650\n",
      "Processing text 660\n",
      "Processing text 670\n",
      "Processing text 680\n",
      "Processing text 690\n",
      "Processing text 700\n",
      "Processing text 710\n",
      "Processing text 720\n",
      "Processing text 730\n",
      "Processing text 740\n",
      "Processing text 750\n",
      "Processing text 760\n",
      "Processing text 770\n",
      "Processing text 780\n",
      "Processing text 790\n",
      "Processing text 800\n",
      "Processing text 810\n",
      "Processing text 820\n",
      "Processing text 830\n",
      "Processing text 840\n",
      "Processing text 850\n",
      "Processing text 860\n",
      "Processing text 870\n",
      "Processing text 880\n",
      "Processing text 890\n",
      "Processing text 900\n",
      "Processing text 910\n",
      "Processing text 920\n",
      "Processing text 930\n",
      "Processing text 940\n",
      "Processing text 950\n",
      "Processing text 960\n",
      "Processing text 970\n",
      "Processing text 980\n",
      "Processing text 990\n",
      "Processing text 1000\n",
      "Processing text 1010\n",
      "Processing text 1020\n",
      "Processing text 1030\n",
      "Processing text 1040\n",
      "Processing text 1050\n",
      "Processing text 1060\n",
      "Processing text 1070\n",
      "Processing text 1080\n",
      "Processing text 1090\n",
      "Processing text 1100\n",
      "Processing text 1110\n",
      "Processing text 1120\n",
      "Processing text 1130\n",
      "Processing text 1140\n",
      "Processing text 1150\n",
      "\n",
      "Time elapsed 78.8284 min\n"
     ]
    }
   ],
   "source": [
    "# run for the entire text column of the dataframe\n",
    "start = time.time()\n",
    "res   = dict()\n",
    "count = 0\n",
    "for t in df_dev['text'].tolist():\n",
    "    if t in res:\n",
    "        continue\n",
    "    try:\n",
    "        res[ t ] = classify_text_with_clarifying(t, prompt_one)\n",
    "    except openai.error.RateLimitError:\n",
    "        print(f'\\nText: {t}. Rate limit error\\n')\n",
    "    except Exception as e:\n",
    "        print(f'\\nText: {t}. Error: {e}\\n')\n",
    "                \n",
    "    count += 1    \n",
    "    if count % 10 == 0:\n",
    "        print(f'Processing text {count}')\n",
    "        #with open('data/res.pkl', 'wb') as f:\n",
    "        #    pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "        \n",
    "elapsed = (time.time() - start)/60\n",
    "print(f'\\nTime elapsed {round(elapsed, 4)} min')\n",
    "#file = 'data/res.pkl'\n",
    "#with open(file, 'rb') as f:\n",
    "#    res2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fa84328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1191, 13), 1156, 1150, 1156)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duplicates in df_dev\n",
    "df_dev.shape, len(res), len(set(df_dev['text_clean'].tolist())), len(set(df_dev['gpt_translated2_corrected'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e12fb5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                         0\n",
      "emotion                      0\n",
      "target                       0\n",
      "gtp_translated               0\n",
      "translated_hi                0\n",
      "translated_ur                0\n",
      "text_clean                   0\n",
      "gpt_pred                     0\n",
      "gpt_pred_num                 0\n",
      "gpt_translated2              0\n",
      "gpt_translated2_corrected    0\n",
      "gpt_pred_binary              0\n",
      "gpt_pred_clarified           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_dev['gpt_pred'] = df_dev['text'].map( res )\n",
    "#df_dev = df_dev.replace('neutral/emotional', 'emotional')\n",
    "print(df_dev.isna().sum())\n",
    "#df_dev['gpt_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21405ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    773\n",
       "1    418\n",
       "Name: gpt_pred_binary, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['gpt_pred_binary'] = df_dev['gpt_pred'].apply( lambda x: 0 if x[3]=='neutral' else 1 )\n",
    "df_dev['gpt_pred_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9de67d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3894    0.7758    0.5185       388\n",
      "           1     0.7919    0.4122    0.5422       803\n",
      "\n",
      "    accuracy                         0.5306      1191\n",
      "   macro avg     0.5906    0.5940    0.5303      1191\n",
      "weighted avg     0.6607    0.5306    0.5345      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev      = df_dev['target'].values\n",
    "y_dev_pred = df_dev['gpt_pred_binary'].values\n",
    "print( classification_report( y_dev, y_dev_pred, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80b429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7a5c9e",
   "metadata": {},
   "source": [
    "### Zero-shot classification using pandas\n",
    "NOTE: tqdm or pandas make the API calls twice as slow compared with the simple iteration above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e957b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f15e7c528448edba700794fb42b161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                         0\n",
      "emotion                      0\n",
      "target                       0\n",
      "gtp_translated               0\n",
      "translated_hi                0\n",
      "translated_ur                0\n",
      "text_clean                   0\n",
      "gpt_pred                     0\n",
      "gpt_pred_num                 0\n",
      "gpt_translated2              0\n",
      "gpt_translated2_corrected    0\n",
      "gpt_pred_binary              0\n",
      "gpt_pred_clarified           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# prompt 1 tqdm results - 1191/1191 [25:38<00:00, 1.18s/it]\n",
    "def apply_func_with_exception(text_, prompt_):\n",
    "    try:\n",
    "        return classify_text_with_clarifying(text_, prompt_)\n",
    "    except openai.error.RateLimitError:\n",
    "        print(f'Text: {text_}. Rate limit error\\n')\n",
    "        return np.nan\n",
    "    except Exception as e:\n",
    "        print(f'Text: {text_}. Another error: {e}\\n')\n",
    "        return np.nan\n",
    "    \n",
    "df_dev['gpt_pred_clarified'] = df_dev['gpt_translated2_corrected'].progress_apply( lambda x: apply_func_with_exception(x, prompt_one) )\n",
    "print( df_dev.isna().sum() )\n",
    "#df_dev['gpt_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7dedb2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    989\n",
       "1    202\n",
       "Name: gpt_pred_binary, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['gpt_pred_binary'] = df_dev['gpt_pred'].apply( lambda x: 0 if x=='neutral' else 1 )\n",
    "df_dev['gpt_pred_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5961083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ChatGPT made no prediction, choose the prediction coming from the classifier\n",
    "'''def improve_predictions(row):\n",
    "    if row['gpt_pred_binary'] is None:\n",
    "        row['gpt_pred_binary'] = row['clf_pred']\n",
    "    return row\n",
    "\n",
    "df_dev = df_dev.apply( improve_predictions, axis=1 )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f659274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3517    0.7088    0.4701       388\n",
      "           1     0.7237    0.3686    0.4884       803\n",
      "\n",
      "    accuracy                         0.4794      1191\n",
      "   macro avg     0.5377    0.5387    0.4793      1191\n",
      "weighted avg     0.6025    0.4794    0.4825      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev      = df_dev['target'].values\n",
    "y_dev_pred = df_dev['gpt_pred_binary'].values\n",
    "print( classification_report( y_dev, y_dev_pred, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfd118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3c932f",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693df45",
   "metadata": {},
   "source": [
    "## Prompts and results (in reverse chronological order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f8284",
   "metadata": {},
   "source": [
    "__Best zero-shot result, so far__  \n",
    "_Disregard translation and use the CODE-MIXED BILINGUAL TEXT column \"text\" hoping that ChatGPT will will figure it out on its own_  \n",
    "_Double prompting with classify_text_with_clarifying()_ ON ORIGINAL BILINGUAL TEXT:  \n",
    "_Prompt 1_: Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"This is a text sample\"  \n",
    "_Prompt 2_: 'Are you sure about that? Output only the category'\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3894    0.7758    0.5185       388\n",
    "           1     0.7919    0.4122    0.5422       803\n",
    "\n",
    "    accuracy                         0.5306      1191\n",
    "   macro avg     0.5906    0.5940    0.5303      1191\n",
    "weighted avg     0.6607    0.5306    0.5345      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462d434",
   "metadata": {},
   "source": [
    "_Disregard translation and use the CODE-MIXED BILINGUAL TEXT column \"text\" hoping that ChatGPT will will figure it out on its own_:  \n",
    "_PROMPT_: Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"This is a text sample\" \n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3782    0.9639    0.5432       388\n",
    "           1     0.9307    0.2341    0.3741       803\n",
    "\n",
    "    accuracy                         0.4719      1191\n",
    "   macro avg     0.6544    0.5990    0.4587      1191\n",
    "weighted avg     0.7507    0.4719    0.4292      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f1591",
   "metadata": {},
   "source": [
    "_Second double prompting witih classify_text_with_clarifying()_ ON THE GPT TRANSLATED ENGLISH TEXT:\n",
    "_Prompt 1_: Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"Dude, when did I ever say no to you guys? Come on over, I'm free right now anyway.\"  \n",
    "_Prompt 2_: 'Are you sure about that?'\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3517    0.7088    0.4701       388\n",
    "           1     0.7237    0.3686    0.4884       803\n",
    "\n",
    "    accuracy                         0.4794      1191\n",
    "   macro avg     0.5377    0.5387    0.4793      1191\n",
    "weighted avg     0.6025    0.4794    0.4825      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9175c",
   "metadata": {},
   "source": [
    "_Improved single prompt achieves the same result as double prompting_  ON THE GPT TRANSLATED ENGLISH TEXT:  \n",
    "_Prompt_: Act as a binary text classifier. Output the category \"emotional\" only and only if the text below suggests any human emotion. Otherwise output \"neutral\". Text: \"Dude, when did I ever say no to you guys? Come on over, I'm free right now anyway.\"\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3916    0.8840    0.5427       388\n",
    "           1     0.8571    0.3362    0.4830       803\n",
    "\n",
    "    accuracy                         0.5147      1191\n",
    "   macro avg     0.6243    0.6101    0.5129      1191\n",
    "weighted avg     0.7055    0.5147    0.5025      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df513e26",
   "metadata": {},
   "source": [
    "_Double prompting witih classify_text_with_clarifying()_  ON THE GPT TRANSLATED ENGLISH TEXT:  \n",
    "_Prompt 1_: Act as a text classifier. Classify the text below into one most relevant category from this list of categories: emotional, neutral. Use the emotional category only if the text below describes any emotions; use the neutral category only if the text below does not speak about emotions at all. Output only one word: 'emotional' or 'neutral', whichever is more relevant. Text: \"This is a text sample\"   \n",
    "_Prompt 2_: 'Are you sure about that?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75ea13",
   "metadata": {},
   "source": [
    "```\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3867    0.8273    0.5271       388\n",
    "           1     0.8144    0.3661    0.5052       803\n",
    "\n",
    "    accuracy                         0.5164      1191\n",
    "   macro avg     0.6006    0.5967    0.5161      1191\n",
    "weighted avg     0.6751    0.5164    0.5123      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1666d",
   "metadata": {},
   "source": [
    "_Prompt to classify after the corrected English translation (zero shot)_  ON THE GPT TRANSLATED ENGLISH TEXT:  \n",
    "Act as a text classifier. Classify the text below into one most relevant category from this list of categories: emotional, neutral. Use the emotional category only if the text below describes any emotions; use the neutral category only if the text below does not speak about emotions at all. Output only one word: 'emotional' or 'neutral', whichever is more relevant. Text: \"This is a text sample.\"\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3775    0.9253    0.5362       388\n",
    "           1     0.8792    0.2628    0.4046       803\n",
    "\n",
    "    accuracy                         0.4786      1191\n",
    "   macro avg     0.6283    0.5940    0.4704      1191\n",
    "weighted avg     0.7157    0.4786    0.4475      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26093b",
   "metadata": {},
   "source": [
    "_Prompt to classify after the first English translation (zero shot)_  ON THE GPT TRANSLATED ENGLISH TEXT:  \n",
    "Act as a careful and accurate text classifier. Classify the text below as 'emotional' only if it contains emotions; lassify the text below as 'neutral' only if it does not contain emotions. Output only one word: 'emotional' or 'neutral' whichever is more relevant. Text: \"This is a text sample\"\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3789    0.9510    0.5419       388\n",
    "           1     0.9124    0.2466    0.3882       803\n",
    "\n",
    "    accuracy                         0.4761      1191\n",
    "   macro avg     0.6456    0.5988    0.4650      1191\n",
    "weighted avg     0.7386    0.4761    0.4383      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c1326",
   "metadata": {},
   "source": [
    "_Prompt to translate and classify_ ON THE GPT TRANSLATED ENGLISH TEXT:  \n",
    "The text below may contain words or phrases in Roman Urdu along with English. Translate the text below into English only. Then classify the translated text as 'emotional' if it contains emotions or 'neutral' if it does not contain emotions. Output only 'emotional' or 'neutral' and nothing else. Text: \"This is a text sample\"\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.3662    0.9278    0.5252       388\n",
    "           1     0.8654    0.2242    0.3561       803\n",
    "\n",
    "    accuracy                         0.4534      1191\n",
    "   macro avg     0.6158    0.5760    0.4406      1191\n",
    "weighted avg     0.7028    0.4534    0.4112      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5b555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7ecba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
