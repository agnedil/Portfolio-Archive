{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7dfa760",
   "metadata": {},
   "source": [
    "# Copy labels leaked from training set. Classify the rest w/baseline classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "## The Association for Computational Linguistics\n",
    "## WASSA 2023 Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages\n",
    "See more details [here](https://codalab.lisn.upsaclay.fr/competitions/10864#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/p2r9brhx2335cwnww04p9w180000gn/T/ipykernel_44226/3622026854.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import time\n",
    "import zipfile\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "import tiktoken\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56940aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new new version (Dec 2022)\n",
    "def upsample_all( df_, labels_col='target', random_state=47 ):\n",
    "    '''\n",
    "        Upsample each class in column labels_col of pandas dataframe df_\n",
    "        to the number of data points in majority class\n",
    "    '''\n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_[labels_col].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_[labels_col] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "\n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "\n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "\n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbb2ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral',\n",
       " 1: 'joy',\n",
       " 2: 'trust',\n",
       " 3: 'disgust',\n",
       " 4: 'optimism',\n",
       " 5: 'anticipation',\n",
       " 6: 'sadness',\n",
       " 7: 'fear',\n",
       " 8: 'surprise',\n",
       " 9: 'anger',\n",
       " 10: 'pessimism',\n",
       " 11: 'love'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the order of decreasing frequency\n",
    "label2key = {\n",
    "    'neutral': 0,\n",
    "    'joy': 1,\n",
    "    'trust': 2,\n",
    "    'disgust': 3,\n",
    "    'optimism': 4,\n",
    "    'anticipation': 5,\n",
    "    'sadness': 6,\n",
    "    'fear': 7,\n",
    "    'surprise': 8,\n",
    "    'anger': 9,\n",
    "    'pessimism': 10,\n",
    "    'love':  11,\n",
    "}\n",
    "key2label = { v: k for k,v in label2key.items()}\n",
    "key2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 2) (1191, 2) (1191, 1)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/mcec_train.csv'\n",
    "df_train = pd.read_csv(file1)\n",
    "\n",
    "file2    = 'data/mcec_dev.csv'\n",
    "df_dev   = pd.read_csv(file2)\n",
    "\n",
    "file3    = 'data/mcec_test.csv'\n",
    "df_test  = pd.read_csv(file3)\n",
    "\n",
    "print(df_train.shape, df_dev.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         3262\n",
      "trust           1118\n",
      "joy             1022\n",
      "optimism         880\n",
      "anticipation     832\n",
      "disgust          687\n",
      "sadness          486\n",
      "fear             453\n",
      "anger            226\n",
      "surprise         199\n",
      "love             187\n",
      "pessimism        178\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes.I am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yar insan ka bcha bn chawliyn na mar :p</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terai uncle nai kahna hai kai ham nai to bahr ...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr ajao I m cming in the club</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion\n",
       "0  Yes.I am in fyp lab cabin.but fyp presentation...  neutral\n",
       "1           Yar insan ka bcha bn chawliyn na mar :p       joy\n",
       "2  Terai uncle nai kahna hai kai ham nai to bahr ...  disgust\n",
       "3                      Yr ajao I m cming in the club  neutral\n",
       "4  Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...      joy"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['Emotion'].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad79270",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = df_train['Emotion'].map( label2key )\n",
    "df_dev['target']   = df_dev['Emotion'].map( label2key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070f02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# light text cleaning (should I use clean regex for better accuracy?)\n",
    "pad_punct    = re.compile('([^a-zA-Z ]+)')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "#clean        = re.compile('[^a-zA-Z0-9,.?!\\'\\s]+')\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = pad_punct.sub(r' \\1 ', s)\n",
    "    #s = clean.sub(' ', s)\n",
    "    s = multi_spaces.sub(' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "df_train['text_clean'] = df_train['Text'].apply( clean_text )\n",
    "df_dev['text_clean']   = df_dev['Text'].apply( clean_text )\n",
    "df_test['text_clean']  = df_test['Text'].apply( clean_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbd6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 2206\n",
      "(9530, 4)\n",
      "(8151, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove overlap with validation sets\n",
    "val_sets = df_dev['text_clean'].tolist() + df_test['text_clean'].tolist()\n",
    "print(len(val_sets), len(set(val_sets)))\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train = df_train[ ~df_train['text_clean'].isin(val_sets) ]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1293d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates from train set\n",
    "df_train = df_train.drop_duplicates(subset=['text_clean', 'Emotion'])\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40905d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 2127),\n",
       " ('k', 1244),\n",
       " ('to', 1231),\n",
       " ('ha', 1214),\n",
       " ('hai', 804),\n",
       " ('ho', 793),\n",
       " ('ka', 726),\n",
       " ('me', 640),\n",
       " ('?', 615),\n",
       " ('b', 604),\n",
       " ('kr', 568),\n",
       " ('ga', 559),\n",
       " ('ni', 553),\n",
       " ('ko', 543),\n",
       " ('ki', 532),\n",
       " ('tha', 528),\n",
       " (',', 518),\n",
       " ('...', 502),\n",
       " ('na', 497),\n",
       " ('hn', 473),\n",
       " ('hy', 464),\n",
       " ('wo', 461),\n",
       " ('ma', 453),\n",
       " ('nai', 450),\n",
       " ('..', 450),\n",
       " ('a', 446),\n",
       " ('se', 415),\n",
       " ('p', 409),\n",
       " ('yar', 401),\n",
       " ('or', 392),\n",
       " ('yr', 389),\n",
       " ('h', 388),\n",
       " ('i', 385),\n",
       " ('han', 385),\n",
       " ('tu', 371),\n",
       " ('e', 331),\n",
       " (':', 327),\n",
       " ('ne', 324),\n",
       " ('kia', 321),\n",
       " ('he', 287),\n",
       " ('hain', 284),\n",
       " ('main', 281),\n",
       " ('ab', 254),\n",
       " ('koi', 252),\n",
       " ('us', 251),\n",
       " ('nae', 250),\n",
       " ('ap', 250),\n",
       " ('sir', 250),\n",
       " ('sy', 248),\n",
       " ('tm', 237),\n",
       " ('is', 223),\n",
       " ('nahi', 223),\n",
       " ('hi', 222),\n",
       " ('raha', 220),\n",
       " ('kal', 218),\n",
       " ('rha', 214),\n",
       " ('ja', 202),\n",
       " ('ny', 200),\n",
       " ('aj', 199),\n",
       " ('g', 199),\n",
       " ('m', 198),\n",
       " ('phr', 195),\n",
       " (':-', 193),\n",
       " ('aur', 192),\n",
       " ('mai', 192),\n",
       " ('....', 187),\n",
       " ('gya', 184),\n",
       " ('d', 183),\n",
       " ('bht', 181),\n",
       " ('u', 173),\n",
       " ('pta', 172),\n",
       " ('kar', 171),\n",
       " ('the', 169),\n",
       " ('mein', 168),\n",
       " ('jana', 168),\n",
       " ('ya', 167),\n",
       " ('time', 166),\n",
       " ('ok', 165),\n",
       " ('ye', 163),\n",
       " ('nhi', 161),\n",
       " ('kya', 161),\n",
       " ('jo', 161),\n",
       " ('in', 157),\n",
       " ('pe', 157),\n",
       " ('n', 154),\n",
       " ('0', 153),\n",
       " ('hu', 152),\n",
       " ('hun', 151),\n",
       " ('kam', 150),\n",
       " ('aa', 149),\n",
       " ('bhi', 147),\n",
       " ('abi', 142),\n",
       " (\"'\", 140),\n",
       " ('do', 138),\n",
       " ('you', 135),\n",
       " ('mera', 135),\n",
       " ('kuch', 135),\n",
       " ('uni', 134),\n",
       " ('o', 131),\n",
       " ('msg', 127),\n",
       " ('ke', 125),\n",
       " ('aya', 124),\n",
       " ('sb', 123),\n",
       " ('!', 122),\n",
       " ('pata', 119),\n",
       " ('say', 119),\n",
       " ('bhai', 118),\n",
       " ('tk', 118),\n",
       " ('sath', 118),\n",
       " ('le', 117),\n",
       " ('gy', 117),\n",
       " ('send', 116),\n",
       " ('thi', 114),\n",
       " ('ana', 114),\n",
       " ('ge', 112),\n",
       " ('kaha', 111),\n",
       " ('bta', 110),\n",
       " ('2', 110),\n",
       " ('pas', 109),\n",
       " ('ly', 109),\n",
       " ('plz', 108),\n",
       " ('gi', 107),\n",
       " ('ghr', 106),\n",
       " ('bs', 105),\n",
       " ('??', 104),\n",
       " ('r', 103),\n",
       " ('bat', 103),\n",
       " ('no', 102),\n",
       " ('and', 101),\n",
       " ('dy', 100),\n",
       " ('meri', 98),\n",
       " ('keh', 97),\n",
       " ('mje', 96),\n",
       " ('din', 96),\n",
       " ('allah', 96),\n",
       " ('krna', 95),\n",
       " ('for', 95),\n",
       " ('kiya', 95),\n",
       " ('agr', 94),\n",
       " ('1', 92),\n",
       " ('mere', 92),\n",
       " ('bt', 91),\n",
       " ('acha', 90),\n",
       " ('lo', 89),\n",
       " ('ghar', 87),\n",
       " ('hon', 86),\n",
       " ('but', 85),\n",
       " ('thy', 85),\n",
       " ('baat', 85),\n",
       " ('de', 84),\n",
       " ('so', 84),\n",
       " ('par', 82),\n",
       " ('hota', 82),\n",
       " ('of', 82),\n",
       " ('my', 82),\n",
       " ('liye', 82),\n",
       " ('tak', 81),\n",
       " ('mjy', 81),\n",
       " ('be', 80),\n",
       " ('s', 80),\n",
       " ('yaar', 80),\n",
       " ('khud', 79),\n",
       " ('abhi', 79),\n",
       " ('class', 78),\n",
       " ('hua', 78),\n",
       " ('at', 77),\n",
       " ('aik', 77),\n",
       " ('py', 77),\n",
       " ('t', 76),\n",
       " ('kha', 76),\n",
       " ('jao', 76),\n",
       " ('kisi', 75),\n",
       " ('gai', 74),\n",
       " ('on', 73),\n",
       " ('wese', 73),\n",
       " ('mil', 73),\n",
       " ('may', 73),\n",
       " ('sa', 73),\n",
       " ('pr', 73),\n",
       " ('kro', 73),\n",
       " ('ra', 72),\n",
       " ('wala', 71),\n",
       " ('bi', 71),\n",
       " ('thk', 71),\n",
       " ('will', 71),\n",
       " ('have', 70),\n",
       " (':)', 70),\n",
       " ('c', 70),\n",
       " ('???', 70),\n",
       " ('it', 69),\n",
       " ('jani', 69),\n",
       " ('kb', 69),\n",
       " ('tum', 68),\n",
       " ('.....', 68),\n",
       " ('bus', 66),\n",
       " ('hum', 66),\n",
       " ('wali', 65),\n",
       " ('hay', 65),\n",
       " ('log', 64),\n",
       " ('call', 63),\n",
       " ('krta', 63),\n",
       " ('tw', 63),\n",
       " ('rhi', 63),\n",
       " ('we', 62),\n",
       " ('phir', 62),\n",
       " ('free', 61),\n",
       " ('kis', 61),\n",
       " ('hahaha', 61),\n",
       " ('3', 61),\n",
       " ('apni', 61),\n",
       " ('thek', 60),\n",
       " ('lab', 59),\n",
       " ('un', 59),\n",
       " ('subha', 59),\n",
       " ('th', 59),\n",
       " ('dia', 58),\n",
       " ('chal', 58),\n",
       " ('jb', 58),\n",
       " ('mama', 57),\n",
       " ('w', 57),\n",
       " ('q', 57),\n",
       " ('that', 56),\n",
       " ('aye', 56),\n",
       " (':-)', 55),\n",
       " ('ata', 54),\n",
       " ('teri', 54),\n",
       " ('v', 54),\n",
       " ('paper', 53),\n",
       " ('ek', 53),\n",
       " ('min', 53),\n",
       " ('bna', 53),\n",
       " ('jaye', 53),\n",
       " ('bjy', 52),\n",
       " ('papa', 52),\n",
       " ('sai', 52),\n",
       " ('pass', 52),\n",
       " ('mene', 52),\n",
       " ('gae', 52),\n",
       " ('dil', 52),\n",
       " ('yad', 51),\n",
       " ('nd', 51),\n",
       " ('haha', 51),\n",
       " ('mjhe', 51),\n",
       " ('oye', 50),\n",
       " ('dena', 50),\n",
       " ('8', 50),\n",
       " ('mery', 50),\n",
       " ('pa', 50),\n",
       " ('wahan', 50),\n",
       " ('ur', 49),\n",
       " ('dekh', 48),\n",
       " ('ao', 48),\n",
       " ('tera', 48),\n",
       " ('rhy', 48),\n",
       " ('laga', 48),\n",
       " ('hoti', 48),\n",
       " ('jaldi', 48),\n",
       " ('rahi', 48),\n",
       " ('karna', 48),\n",
       " ('mujy', 47),\n",
       " ('office', 47),\n",
       " ('am', 46),\n",
       " ('hm', 46),\n",
       " ('gay', 46),\n",
       " ('late', 46),\n",
       " ('kch', 46),\n",
       " ('eid', 46),\n",
       " ('khana', 45),\n",
       " ('not', 45),\n",
       " ('kahan', 45),\n",
       " ('da', 45),\n",
       " ('tou', 45),\n",
       " ('check', 45),\n",
       " ('yeh', 45),\n",
       " ('reply', 45),\n",
       " ('skta', 45),\n",
       " ('wapis', 44),\n",
       " ('bje', 44),\n",
       " ('if', 44),\n",
       " ('gaya', 44),\n",
       " ('jata', 44),\n",
       " ('use', 44),\n",
       " ('apna', 43),\n",
       " ('10', 43),\n",
       " ('ay', 43),\n",
       " ('bad', 43),\n",
       " ('kafi', 43),\n",
       " ('thora', 43),\n",
       " ('aaj', 43),\n",
       " ('krwa', 42),\n",
       " ('sab', 42),\n",
       " ('lye', 42),\n",
       " (':(', 42),\n",
       " ('good', 42),\n",
       " ('theek', 42),\n",
       " ('mn', 42),\n",
       " ('di', 42),\n",
       " ('nikal', 42),\n",
       " ('nay', 41),\n",
       " ('ae', 41),\n",
       " ('ku', 41),\n",
       " ('sorry', 41),\n",
       " ('day', 40),\n",
       " ('usy', 40),\n",
       " ('li', 40),\n",
       " ('4', 40),\n",
       " ('muje', 40),\n",
       " ('5', 40),\n",
       " ('ai', 39),\n",
       " ('lahore', 39),\n",
       " ('plan', 39),\n",
       " ('dua', 39),\n",
       " ('start', 38),\n",
       " (',,,', 38),\n",
       " ('7', 38),\n",
       " ('hui', 38),\n",
       " ('fb', 38),\n",
       " ('agar', 38),\n",
       " ('test', 38),\n",
       " ('bike', 38),\n",
       " ('nh', 38),\n",
       " ('chala', 37),\n",
       " ('krny', 37),\n",
       " ('jae', 37),\n",
       " ('lia', 37),\n",
       " ('kbi', 37),\n",
       " ('lga', 37),\n",
       " ('your', 37),\n",
       " ('dost', 37),\n",
       " ('mila', 37),\n",
       " ('net', 37),\n",
       " ('masla', 36),\n",
       " ('last', 36),\n",
       " ('try', 36),\n",
       " ('rat', 36),\n",
       " ('kaam', 36),\n",
       " ('lena', 36),\n",
       " ('sakta', 36),\n",
       " ('per', 36),\n",
       " ('are', 35),\n",
       " ('la', 35),\n",
       " ('hoon', 35),\n",
       " ('assignment', 35),\n",
       " ('haan', 35),\n",
       " ('rahy', 35),\n",
       " ('change', 35),\n",
       " ('lag', 35),\n",
       " ('krni', 35),\n",
       " ('ta', 35),\n",
       " ('pay', 35)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is additional text cleaning necessary? I don't see why\n",
    "from collections import Counter\n",
    "train_words = ' '.join( df_train['text_clean'].tolist() ).lower().split()\n",
    "c = Counter( train_words )\n",
    "c.most_common(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afcaeedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "102 318 129\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/owaisraza009/roman-urdu-sentiment-analysis/notebook\n",
    "stopwords1 = [ 'ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh',\n",
    "               'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya',\n",
    "               'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se',\n",
    "               'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski',\n",
    "               'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya',\n",
    "               'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi',\n",
    "               'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain',\n",
    "               'krny', 'tou', ]\n",
    "\n",
    "# https://github.com/haseebelahi/roman-urdu-stopwords.git\n",
    "file = 'data/stopwords.txt'\n",
    "stopwords2 = open(file).read().split()\n",
    "print(stopwords2 == stopwords1)\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "stopwords_en  = _stop_words.ENGLISH_STOP_WORDS\n",
    "# selected from stopwords_en\n",
    "stopwords_en2 = [ 'a', 'about', 'also', 'am', 'an', 'and', 'are', 'as', 'at', 'be', \n",
    "                  'been', 'being', 'by', 'co', 'con', 'de', 'eg', 'eight', 'eleven', 'else', 'etc', \n",
    "                  'fifteen', 'fifty', 'five', 'for', 'forty', 'four', 'from', 'had',\n",
    "                  'has', 'hasnt', 'have', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', \n",
    "                  'his', 'how', 'i', 'ie', 'if', 'in', 'inc', 'into', 'is', 'it', 'its', 'itself',\n",
    "                  'ltd', 'me', 'mine', 'my', 'myself', 'nine', 'no', 'now', 'of', 'off', 'on',\n",
    "                  'once', 'one', 'onto', 'or', 'other', 'others', 'our', 'ours', 'ourselves',\n",
    "                  'out', 'part', 'per', 're', 'several', 'she', 'side', 'since', 'six', 'sixty',\n",
    "                  'so', 'ten', 'than', 'that', 'the', 'their', 'them',\n",
    "                  'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', \n",
    "                  'three', 'to', 'twelve', 'twenty', 'two', 'un','us', 'very',\n",
    "                  'via', 'was', 'we', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', \n",
    "                  'who', 'whom', 'whose', 'why', 'with', 'within', 'would', 'yet', 'you', 'your', 'yours',\n",
    "                   'yourself', 'yourselves', ]\n",
    "\n",
    "print( len(stopwords1), len(stopwords_en), len(stopwords_en2), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a48cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3bde67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set b4 upsampling:\n",
      "neutral         2097\n",
      "joy              722\n",
      "trust            684\n",
      "optimism         577\n",
      "anticipation     562\n",
      "disgust          441\n",
      "sadness          316\n",
      "fear             257\n",
      "anger            142\n",
      "surprise         136\n",
      "love             124\n",
      "pessimism        109\n",
      "Name: Emotion, dtype: int64\n",
      "\n",
      "Train set after upsampling:\n",
      "fear            2097\n",
      "pessimism       2097\n",
      "anticipation    2097\n",
      "optimism        2097\n",
      "sadness         2097\n",
      "anger           2097\n",
      "love            2097\n",
      "trust           2097\n",
      "surprise        2097\n",
      "neutral         2097\n",
      "joy             2097\n",
      "disgust         2097\n",
      "Name: Emotion, dtype: int64\n",
      "\n",
      "7     2097\n",
      "10    2097\n",
      "5     2097\n",
      "4     2097\n",
      "6     2097\n",
      "9     2097\n",
      "11    2097\n",
      "2     2097\n",
      "8     2097\n",
      "0     2097\n",
      "1     2097\n",
      "3     2097\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain set b4 upsampling:\\n', df_train['Emotion'].value_counts(), sep='')\n",
    "df_train = upsample_all( df_train.copy(), labels_col='target', random_state=random_state )\n",
    "print('\\nTrain set after upsampling:\\n', df_train['Emotion'].value_counts(), '\\n\\n', \n",
    "       df_train['target'].value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (25164,) (25164,) (1191,) (1191,)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['Text'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "X_dev = df_dev['Text'].values\n",
    "y_dev = df_dev['target'].values\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle( X_train, y_train, random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e08e5dd",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "df367a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,                 # maybe 4 w/gamma=0.55 (train accu=0.9 vs. 0.97 for depth 6)\n",
    "    'learning_rate': 0.3,                                 # eta  \n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 1.5,                                     # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,                             # 0-1\n",
    "    'colsample_bynode': 1.0,                              # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 12,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e92e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 8,            # best 8\n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,4),   # best (1,4)\n",
    "    'binary': True,\n",
    "    'stop_words': 'english',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(analyzer=&#x27;char&#x27;, binary=True, min_df=8,\n",
       "                                 ngram_range=(1, 4), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsample_bylevel=1.0, colsample_bynode=1.0,\n",
       "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=&#x27;merror&#x27;,\n",
       "                               feature_types=None, gamma=0,...=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=6, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=12,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(analyzer=&#x27;char&#x27;, binary=True, min_df=8,\n",
       "                                 ngram_range=(1, 4), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "                               colsample_bylevel=1.0, colsample_bynode=1.0,\n",
       "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=&#x27;merror&#x27;,\n",
       "                               feature_types=None, gamma=0,...=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=6, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=12,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, binary=True, min_df=8, ngram_range=(1, 4),\n",
       "                stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;merror&#x27;, feature_types=None, gamma=0, gpu_id=None,\n",
       "              grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
       "              max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
       "              num_class=12, num_parallel_tree=None, objective=&#x27;multi:softmax&#x27;, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(analyzer='char', binary=True, min_df=8,\n",
       "                                 ngram_range=(1, 4), stop_words='english')),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=1.0, colsample_bynode=1.0,\n",
       "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric='merror',\n",
       "                               feature_types=None, gamma=0,...=None,\n",
       "                               grow_policy=None, importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=6, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=12,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective='multi:softmax', ...))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "clf = XGBClassifier( **clf_params_xgb )\n",
    "\n",
    "\n",
    "model = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['neutral', 'joy', 'trust', 'disgust', 'optimism', 'anticipation', 'sadness', 'fear', 'surprise', 'anger', 'pessimism', 'love']\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_dev   = model.predict(X_dev)\n",
    "\n",
    "# add prediction to dataframe\n",
    "df_dev['clf_pred'] = y_pred_dev\n",
    "df_dev['clf_pred_emotion'] = df_dev['clf_pred'].map( key2label )\n",
    "\n",
    "# labels for classification report\n",
    "labels = list(label2key.keys())\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cfd5f419",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "CountVectorizer(analyzer='char', binary=True, min_df=8, ngram_range=(1, 4),\n",
      "                stop_words='english')\n",
      "\n",
      "Classifier:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric='merror', feature_types=None, gamma=0, gpu_id=None,\n",
      "              grow_policy=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
      "              max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
      "              num_class=12, num_parallel_tree=None, objective='multi:softmax', ...)\n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8907    0.8546    0.8722      2097\n",
      "         joy     0.9744    0.9800    0.9772      2097\n",
      "       trust     0.9641    0.9361    0.9499      2097\n",
      "     disgust     0.9741    0.9695    0.9718      2097\n",
      "    optimism     0.9495    0.9676    0.9584      2097\n",
      "anticipation     0.9262    0.9461    0.9361      2097\n",
      "     sadness     0.9867    0.9905    0.9886      2097\n",
      "        fear     0.9915    0.9990    0.9952      2097\n",
      "    surprise     0.9957    1.0000    0.9979      2097\n",
      "       anger     0.9976    1.0000    0.9988      2097\n",
      "   pessimism     0.9938    1.0000    0.9969      2097\n",
      "        love     0.9962    1.0000    0.9981      2097\n",
      "\n",
      "    accuracy                         0.9703     25164\n",
      "   macro avg     0.9700    0.9703    0.9701     25164\n",
      "weighted avg     0.9700    0.9703    0.9701     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5255    0.5851    0.5537       388\n",
      "         joy     0.7273    0.7328    0.7300       131\n",
      "       trust     0.3898    0.3680    0.3786       125\n",
      "     disgust     0.2778    0.2655    0.2715       113\n",
      "    optimism     0.4393    0.4273    0.4332       110\n",
      "anticipation     0.3223    0.4149    0.3628        94\n",
      "     sadness     0.3492    0.3548    0.3520        62\n",
      "        fear     0.2281    0.2500    0.2385        52\n",
      "    surprise     0.3125    0.1429    0.1961        35\n",
      "       anger     0.2857    0.1143    0.1633        35\n",
      "   pessimism     0.2143    0.1034    0.1395        29\n",
      "        love     0.5556    0.2941    0.3846        17\n",
      "\n",
      "    accuracy                         0.4509      1191\n",
      "   macro avg     0.3856    0.3378    0.3503      1191\n",
      "weighted avg     0.4433    0.4509    0.4430      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification reports\n",
    "print('Vectorizer:\\n', model['vect'], '\\n', sep='')\n",
    "print('Classifier:\\n', model['clf'], '\\n', sep='')\n",
    "\n",
    "print('\\nTRAINSET')\n",
    "print( classification_report( y_train, y_pred_train, target_names=labels, digits=4 ) )\n",
    "\n",
    "print('DEVSET')\n",
    "print( classification_report( y_dev, y_pred_dev, target_names=labels, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "56de760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text               0\n",
      "gpt_embedding      0\n",
      "text_clean         0\n",
      "Emotion          607\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gpt_embedding</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Razia bta rahe the but wo sure nahe the</td>\n",
       "      <td>[0.00961806159466505, -0.009336333721876144, 0...</td>\n",
       "      <td>Razia bta rahe the but wo sure nahe the</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me phr kuch parh hi lun :-P</td>\n",
       "      <td>[0.004914113786071539, -0.011376334354281425, ...</td>\n",
       "      <td>Me phr kuch parh hi lun :- P</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoxtl life ma hm bht jald matur hO jaty hai</td>\n",
       "      <td>[0.006094285752624273, -0.002856901613995433, ...</td>\n",
       "      <td>Hoxtl life ma hm bht jald matur hO jaty hai</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yar A4 me seminar ha a ja..</td>\n",
       "      <td>[-0.018210574984550476, -0.021075459197163582,...</td>\n",
       "      <td>Yar A 4 me seminar ha a ja ..</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K.Quid e azam k 400 bnay hain</td>\n",
       "      <td>[-0.0046786158345639706, -0.009765759110450745...</td>\n",
       "      <td>K . Quid e azam k 400 bnay hain</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  \\\n",
       "0      Razia bta rahe the but wo sure nahe the   \n",
       "1                  Me phr kuch parh hi lun :-P   \n",
       "2  Hoxtl life ma hm bht jald matur hO jaty hai   \n",
       "3                  Yar A4 me seminar ha a ja..   \n",
       "4                K.Quid e azam k 400 bnay hain   \n",
       "\n",
       "                                       gpt_embedding  \\\n",
       "0  [0.00961806159466505, -0.009336333721876144, 0...   \n",
       "1  [0.004914113786071539, -0.011376334354281425, ...   \n",
       "2  [0.006094285752624273, -0.002856901613995433, ...   \n",
       "3  [-0.018210574984550476, -0.021075459197163582,...   \n",
       "4  [-0.0046786158345639706, -0.009765759110450745...   \n",
       "\n",
       "                                    text_clean  Emotion  \n",
       "0      Razia bta rahe the but wo sure nahe the      NaN  \n",
       "1                 Me phr kuch parh hi lun :- P      NaN  \n",
       "2  Hoxtl life ma hm bht jald matur hO jaty hai     fear  \n",
       "3                Yar A 4 me seminar ha a ja ..  neutral  \n",
       "4              K . Quid e azam k 400 bnay hain  neutral  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD THE FILE TO WHICH THE LABELS LEAKED FROM THE TRAINING SET HAVE ALREADY BEEN TRANSFERRED\n",
    "file = 'data/df_test_with_leakedData_only.pkl'\n",
    "df_test = pd.read_pickle(file)\n",
    "print(df_test.isna().sum())\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bf999b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gpt_embedding</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Razia bta rahe the but wo sure nahe the</td>\n",
       "      <td>[0.00961806159466505, -0.009336333721876144, 0...</td>\n",
       "      <td>Razia bta rahe the but wo sure nahe the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me phr kuch parh hi lun :-P</td>\n",
       "      <td>[0.004914113786071539, -0.011376334354281425, ...</td>\n",
       "      <td>Me phr kuch parh hi lun :- P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoxtl life ma hm bht jald matur hO jaty hai</td>\n",
       "      <td>[0.006094285752624273, -0.002856901613995433, ...</td>\n",
       "      <td>Hoxtl life ma hm bht jald matur hO jaty hai</td>\n",
       "      <td>fear</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yar A4 me seminar ha a ja..</td>\n",
       "      <td>[-0.018210574984550476, -0.021075459197163582,...</td>\n",
       "      <td>Yar A 4 me seminar ha a ja ..</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K.Quid e azam k 400 bnay hain</td>\n",
       "      <td>[-0.0046786158345639706, -0.009765759110450745...</td>\n",
       "      <td>K . Quid e azam k 400 bnay hain</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hm sb b a jate nd aur b bare bnde b a jate! :-D</td>\n",
       "      <td>[-0.009173348546028137, -0.0036938106641173363...</td>\n",
       "      <td>Hm sb b a jate nd aur b bare bnde b a jate ! :- D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yr apnay senior ka method ya hota ha k wo all ...</td>\n",
       "      <td>[0.012461582198739052, 0.004763346165418625, 0...</td>\n",
       "      <td>Yr apnay senior ka method ya hota ha k wo all ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thek ha sir i will be at your office at 1</td>\n",
       "      <td>[-0.005030790343880653, -0.015968872234225273,...</td>\n",
       "      <td>thek ha sir i will be at your office at 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hahahaha tuj ma agr itni wafa ha to tu he sab ...</td>\n",
       "      <td>[-0.003401822643354535, -0.008568023331463337,...</td>\n",
       "      <td>Hahahaha tuj ma agr itni wafa ha to tu he sab ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Well you didnt told me before that the meeting...</td>\n",
       "      <td>[-0.005642556585371494, -0.004453025758266449,...</td>\n",
       "      <td>Well you didnt told me before that the meeting...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yr abu ne mana kr dya, tm log chale jana</td>\n",
       "      <td>[0.022901535034179688, -0.01324721984565258, -...</td>\n",
       "      <td>Yr abu ne mana kr dya , tm log chale jana</td>\n",
       "      <td>fear</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abi soya utha hn yr rplyy na aye to banda call...</td>\n",
       "      <td>[-0.014637974090874195, -0.018663082271814346,...</td>\n",
       "      <td>abi soya utha hn yr rplyy na aye to banda call...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Menu ki pta dhaka lgate rhe us manhus start hi...</td>\n",
       "      <td>[0.011292432434856892, -0.013326681219041348, ...</td>\n",
       "      <td>Menu ki pta dhaka lgate rhe us manhus start hi...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hawn naa...\\nMain umt hun tw unka campus  duba...</td>\n",
       "      <td>[-0.014636051841080189, -0.02131051756441593, ...</td>\n",
       "      <td>hawn naa ... Main umt hun tw unka campus dubai...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Paisai easy paisa sai aai gai</td>\n",
       "      <td>[0.006469042040407658, 0.010846740566194057, 0...</td>\n",
       "      <td>Paisai easy paisa sai aai gai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yr bnda Atleast rat ko txt kr dta merey apney ...</td>\n",
       "      <td>[0.001289149047806859, 0.0016061661299318075, ...</td>\n",
       "      <td>Yr bnda Atleast rat ko txt kr dta merey apney ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Yr das v itne dair ho gaye ha.</td>\n",
       "      <td>[-0.012117898091673851, -0.019422827288508415,...</td>\n",
       "      <td>Yr das v itne dair ho gaye ha .</td>\n",
       "      <td>disgust</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ghnty tk nai ati..to btaein mje</td>\n",
       "      <td>[-0.01757991686463356, -0.010236240923404694, ...</td>\n",
       "      <td>Ghnty tk nai ati .. to btaein mje</td>\n",
       "      <td>NaN</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sania api plz bt krna dae. Na hme.</td>\n",
       "      <td>[-0.00949001219123602, -0.013047930784523487, ...</td>\n",
       "      <td>Sania api plz bt krna dae . Na hme .</td>\n",
       "      <td>trust</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Or do haftay bhi to rehna he</td>\n",
       "      <td>[0.0023986701853573322, 0.0036331857554614544,...</td>\n",
       "      <td>Or do haftay bhi to rehna he</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Yhn sy ban nhi rhy license</td>\n",
       "      <td>[0.0029569766484200954, 0.0034017448779195547,...</td>\n",
       "      <td>Yhn sy ban nhi rhy license</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pessimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Yar muja ya pochana tha laptop kis compny ka a...</td>\n",
       "      <td>[0.0021210378035902977, 0.0004604990826919675,...</td>\n",
       "      <td>Yar muja ya pochana tha laptop kis compny ka a...</td>\n",
       "      <td>trust</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mry lye kia ly kr ai ho?</td>\n",
       "      <td>[0.019194647669792175, -0.0009029482607729733,...</td>\n",
       "      <td>Mry lye kia ly kr ai ho ?</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ok wo msg ma kr dunga.. Ap attndnc lga dena n ...</td>\n",
       "      <td>[-0.011079670861363411, 0.003995199222117662, ...</td>\n",
       "      <td>Ok wo msg ma kr dunga .. Ap attndnc lga dena n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Oye yar kya kr rha ha, aj dophr ko kya khaya t...</td>\n",
       "      <td>[0.01290382444858551, -0.025229576975107193, -...</td>\n",
       "      <td>Oye yar kya kr rha ha , aj dophr ko kya khaya ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0             Razia bta rahe the but wo sure nahe the   \n",
       "1                         Me phr kuch parh hi lun :-P   \n",
       "2         Hoxtl life ma hm bht jald matur hO jaty hai   \n",
       "3                         Yar A4 me seminar ha a ja..   \n",
       "4                       K.Quid e azam k 400 bnay hain   \n",
       "5    Hm sb b a jate nd aur b bare bnde b a jate! :-D    \n",
       "6   Yr apnay senior ka method ya hota ha k wo all ...   \n",
       "7           thek ha sir i will be at your office at 1   \n",
       "8   Hahahaha tuj ma agr itni wafa ha to tu he sab ...   \n",
       "9   Well you didnt told me before that the meeting...   \n",
       "10           Yr abu ne mana kr dya, tm log chale jana   \n",
       "11  abi soya utha hn yr rplyy na aye to banda call...   \n",
       "12  Menu ki pta dhaka lgate rhe us manhus start hi...   \n",
       "13  hawn naa...\\nMain umt hun tw unka campus  duba...   \n",
       "14                      Paisai easy paisa sai aai gai   \n",
       "15  Yr bnda Atleast rat ko txt kr dta merey apney ...   \n",
       "16                     Yr das v itne dair ho gaye ha.   \n",
       "17                    Ghnty tk nai ati..to btaein mje   \n",
       "18                 Sania api plz bt krna dae. Na hme.   \n",
       "19                       Or do haftay bhi to rehna he   \n",
       "20                         Yhn sy ban nhi rhy license   \n",
       "21  Yar muja ya pochana tha laptop kis compny ka a...   \n",
       "22                           Mry lye kia ly kr ai ho?   \n",
       "23  Ok wo msg ma kr dunga.. Ap attndnc lga dena n ...   \n",
       "24  Oye yar kya kr rha ha, aj dophr ko kya khaya t...   \n",
       "\n",
       "                                        gpt_embedding  \\\n",
       "0   [0.00961806159466505, -0.009336333721876144, 0...   \n",
       "1   [0.004914113786071539, -0.011376334354281425, ...   \n",
       "2   [0.006094285752624273, -0.002856901613995433, ...   \n",
       "3   [-0.018210574984550476, -0.021075459197163582,...   \n",
       "4   [-0.0046786158345639706, -0.009765759110450745...   \n",
       "5   [-0.009173348546028137, -0.0036938106641173363...   \n",
       "6   [0.012461582198739052, 0.004763346165418625, 0...   \n",
       "7   [-0.005030790343880653, -0.015968872234225273,...   \n",
       "8   [-0.003401822643354535, -0.008568023331463337,...   \n",
       "9   [-0.005642556585371494, -0.004453025758266449,...   \n",
       "10  [0.022901535034179688, -0.01324721984565258, -...   \n",
       "11  [-0.014637974090874195, -0.018663082271814346,...   \n",
       "12  [0.011292432434856892, -0.013326681219041348, ...   \n",
       "13  [-0.014636051841080189, -0.02131051756441593, ...   \n",
       "14  [0.006469042040407658, 0.010846740566194057, 0...   \n",
       "15  [0.001289149047806859, 0.0016061661299318075, ...   \n",
       "16  [-0.012117898091673851, -0.019422827288508415,...   \n",
       "17  [-0.01757991686463356, -0.010236240923404694, ...   \n",
       "18  [-0.00949001219123602, -0.013047930784523487, ...   \n",
       "19  [0.0023986701853573322, 0.0036331857554614544,...   \n",
       "20  [0.0029569766484200954, 0.0034017448779195547,...   \n",
       "21  [0.0021210378035902977, 0.0004604990826919675,...   \n",
       "22  [0.019194647669792175, -0.0009029482607729733,...   \n",
       "23  [-0.011079670861363411, 0.003995199222117662, ...   \n",
       "24  [0.01290382444858551, -0.025229576975107193, -...   \n",
       "\n",
       "                                           text_clean       Emotion  \\\n",
       "0             Razia bta rahe the but wo sure nahe the           NaN   \n",
       "1                        Me phr kuch parh hi lun :- P           NaN   \n",
       "2         Hoxtl life ma hm bht jald matur hO jaty hai          fear   \n",
       "3                       Yar A 4 me seminar ha a ja ..       neutral   \n",
       "4                     K . Quid e azam k 400 bnay hain       neutral   \n",
       "5   Hm sb b a jate nd aur b bare bnde b a jate ! :- D           NaN   \n",
       "6   Yr apnay senior ka method ya hota ha k wo all ...       neutral   \n",
       "7           thek ha sir i will be at your office at 1           NaN   \n",
       "8   Hahahaha tuj ma agr itni wafa ha to tu he sab ...           NaN   \n",
       "9   Well you didnt told me before that the meeting...           NaN   \n",
       "10          Yr abu ne mana kr dya , tm log chale jana          fear   \n",
       "11  abi soya utha hn yr rplyy na aye to banda call...           NaN   \n",
       "12  Menu ki pta dhaka lgate rhe us manhus start hi...       disgust   \n",
       "13  hawn naa ... Main umt hun tw unka campus dubai...       neutral   \n",
       "14                      Paisai easy paisa sai aai gai           NaN   \n",
       "15  Yr bnda Atleast rat ko txt kr dta merey apney ...          fear   \n",
       "16                    Yr das v itne dair ho gaye ha .       disgust   \n",
       "17                  Ghnty tk nai ati .. to btaein mje           NaN   \n",
       "18               Sania api plz bt krna dae . Na hme .         trust   \n",
       "19                       Or do haftay bhi to rehna he       neutral   \n",
       "20                         Yhn sy ban nhi rhy license       neutral   \n",
       "21  Yar muja ya pochana tha laptop kis compny ka a...         trust   \n",
       "22                          Mry lye kia ly kr ai ho ?  anticipation   \n",
       "23  Ok wo msg ma kr dunga .. Ap attndnc lga dena n...           NaN   \n",
       "24  Oye yar kya kr rha ha , aj dophr ko kya khaya ...           NaN   \n",
       "\n",
       "            pred  \n",
       "0        disgust  \n",
       "1            joy  \n",
       "2        disgust  \n",
       "3        neutral  \n",
       "4        neutral  \n",
       "5            joy  \n",
       "6          trust  \n",
       "7       optimism  \n",
       "8            joy  \n",
       "9       optimism  \n",
       "10         trust  \n",
       "11       disgust  \n",
       "12       disgust  \n",
       "13       neutral  \n",
       "14       neutral  \n",
       "15       sadness  \n",
       "16       neutral  \n",
       "17       disgust  \n",
       "18         trust  \n",
       "19       neutral  \n",
       "20     pessimism  \n",
       "21         trust  \n",
       "22  anticipation  \n",
       "23         trust  \n",
       "24       disgust  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction\n",
    "preds = model.predict(df_test['text'].values)\n",
    "df_test['pred'] = preds\n",
    "df_test['pred'] = df_test['pred'].map( key2label )\n",
    "df_test.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a6b7d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text             0\n",
      "gpt_embedding    0\n",
      "text_clean       0\n",
      "Emotion          0\n",
      "pred             0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neutral         401\n",
       "trust           127\n",
       "optimism        123\n",
       "joy             121\n",
       "anticipation    112\n",
       "disgust          98\n",
       "sadness          72\n",
       "fear             57\n",
       "pessimism        22\n",
       "anger            20\n",
       "surprise         20\n",
       "love             18\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfer only the unknown labels \n",
    "def transfer_label(row):\n",
    "    if not isinstance(row['Emotion'], str) and pd.isnull(row['Emotion']):\n",
    "        row['Emotion'] = row['pred']\n",
    "    return row\n",
    "\n",
    "\n",
    "df_test = df_test.apply( transfer_label, axis=1 )\n",
    "print(df_test.isna().sum())\n",
    "df_test['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8ec15fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for official submission\n",
    "file = 'data/predictions_MCEC.csv'\n",
    "df_test['Emotion'].to_csv( file, index=False, encoding='utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0f3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25197e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b0933a9",
   "metadata": {},
   "source": [
    "# HP fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2999c721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 1.0 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8907    0.8546    0.8722      2097\n",
      "         joy     0.9744    0.9800    0.9772      2097\n",
      "       trust     0.9641    0.9361    0.9499      2097\n",
      "     disgust     0.9741    0.9695    0.9718      2097\n",
      "    optimism     0.9495    0.9676    0.9584      2097\n",
      "anticipation     0.9262    0.9461    0.9361      2097\n",
      "     sadness     0.9867    0.9905    0.9886      2097\n",
      "        fear     0.9915    0.9990    0.9952      2097\n",
      "    surprise     0.9957    1.0000    0.9979      2097\n",
      "       anger     0.9976    1.0000    0.9988      2097\n",
      "   pessimism     0.9938    1.0000    0.9969      2097\n",
      "        love     0.9962    1.0000    0.9981      2097\n",
      "\n",
      "    accuracy                         0.9703     25164\n",
      "   macro avg     0.9700    0.9703    0.9701     25164\n",
      "weighted avg     0.9700    0.9703    0.9701     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5255    0.5851    0.5537       388\n",
      "         joy     0.7273    0.7328    0.7300       131\n",
      "       trust     0.3898    0.3680    0.3786       125\n",
      "     disgust     0.2778    0.2655    0.2715       113\n",
      "    optimism     0.4393    0.4273    0.4332       110\n",
      "anticipation     0.3223    0.4149    0.3628        94\n",
      "     sadness     0.3492    0.3548    0.3520        62\n",
      "        fear     0.2281    0.2500    0.2385        52\n",
      "    surprise     0.3125    0.1429    0.1961        35\n",
      "       anger     0.2857    0.1143    0.1633        35\n",
      "   pessimism     0.2143    0.1034    0.1395        29\n",
      "        love     0.5556    0.2941    0.3846        17\n",
      "\n",
      "    accuracy                         0.4509      1191\n",
      "   macro avg     0.3856    0.3378    0.3503      1191\n",
      "weighted avg     0.4433    0.4509    0.4430      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.95 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8918    0.8450    0.8678      2097\n",
      "         joy     0.9720    0.9762    0.9741      2097\n",
      "       trust     0.9599    0.9356    0.9476      2097\n",
      "     disgust     0.9723    0.9714    0.9719      2097\n",
      "    optimism     0.9367    0.9671    0.9517      2097\n",
      "anticipation     0.9222    0.9385    0.9303      2097\n",
      "     sadness     0.9890    0.9881    0.9885      2097\n",
      "        fear     0.9910    0.9990    0.9950      2097\n",
      "    surprise     0.9943    1.0000    0.9971      2097\n",
      "       anger     0.9976    1.0000    0.9988      2097\n",
      "   pessimism     0.9938    1.0000    0.9969      2097\n",
      "        love     0.9971    1.0000    0.9986      2097\n",
      "\n",
      "    accuracy                         0.9684     25164\n",
      "   macro avg     0.9682    0.9684    0.9682     25164\n",
      "weighted avg     0.9682    0.9684    0.9682     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4906    0.5387    0.5135       388\n",
      "         joy     0.6861    0.7176    0.7015       131\n",
      "       trust     0.4144    0.3680    0.3898       125\n",
      "     disgust     0.2523    0.2478    0.2500       113\n",
      "    optimism     0.4393    0.4273    0.4332       110\n",
      "anticipation     0.3089    0.4043    0.3502        94\n",
      "     sadness     0.3387    0.3387    0.3387        62\n",
      "        fear     0.2308    0.2308    0.2308        52\n",
      "    surprise     0.1333    0.0571    0.0800        35\n",
      "       anger     0.1333    0.0571    0.0800        35\n",
      "   pessimism     0.2000    0.1379    0.1633        29\n",
      "        love     0.5000    0.3529    0.4138        17\n",
      "\n",
      "    accuracy                         0.4274      1191\n",
      "   macro avg     0.3440    0.3232    0.3287      1191\n",
      "weighted avg     0.4152    0.4274    0.4190      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.9 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8757    0.8431    0.8591      2097\n",
      "         joy     0.9707    0.9785    0.9746      2097\n",
      "       trust     0.9611    0.9318    0.9462      2097\n",
      "     disgust     0.9702    0.9628    0.9665      2097\n",
      "    optimism     0.9473    0.9680    0.9575      2097\n",
      "anticipation     0.9196    0.9385    0.9290      2097\n",
      "     sadness     0.9862    0.9914    0.9888      2097\n",
      "        fear     0.9957    0.9957    0.9957      2097\n",
      "    surprise     0.9943    1.0000    0.9971      2097\n",
      "       anger     0.9967    1.0000    0.9983      2097\n",
      "   pessimism     0.9934    1.0000    0.9967      2097\n",
      "        love     0.9962    1.0000    0.9981      2097\n",
      "\n",
      "    accuracy                         0.9675     25164\n",
      "   macro avg     0.9673    0.9675    0.9673     25164\n",
      "weighted avg     0.9673    0.9675    0.9673     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5123    0.5361    0.5239       388\n",
      "         joy     0.6786    0.7252    0.7011       131\n",
      "       trust     0.3814    0.3600    0.3704       125\n",
      "     disgust     0.2339    0.2566    0.2447       113\n",
      "    optimism     0.4434    0.4273    0.4352       110\n",
      "anticipation     0.3089    0.4043    0.3502        94\n",
      "     sadness     0.2917    0.3387    0.3134        62\n",
      "        fear     0.2400    0.2308    0.2353        52\n",
      "    surprise     0.2727    0.0857    0.1304        35\n",
      "       anger     0.2143    0.0857    0.1224        35\n",
      "   pessimism     0.1111    0.0690    0.0851        29\n",
      "        love     0.6667    0.3529    0.4615        17\n",
      "\n",
      "    accuracy                         0.4274      1191\n",
      "   macro avg     0.3629    0.3227    0.3312      1191\n",
      "weighted avg     0.4213    0.4274    0.4204      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.85 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8876    0.8512    0.8690      2097\n",
      "         joy     0.9710    0.9747    0.9729      2097\n",
      "       trust     0.9479    0.9361    0.9419      2097\n",
      "     disgust     0.9713    0.9690    0.9702      2097\n",
      "    optimism     0.9486    0.9595    0.9540      2097\n",
      "anticipation     0.9270    0.9390    0.9330      2097\n",
      "     sadness     0.9830    0.9914    0.9872      2097\n",
      "        fear     0.9948    0.9943    0.9945      2097\n",
      "    surprise     0.9953    1.0000    0.9976      2097\n",
      "       anger     0.9971    1.0000    0.9986      2097\n",
      "   pessimism     0.9943    1.0000    0.9971      2097\n",
      "        love     0.9938    1.0000    0.9969      2097\n",
      "\n",
      "    accuracy                         0.9679     25164\n",
      "   macro avg     0.9676    0.9679    0.9677     25164\n",
      "weighted avg     0.9676    0.9679    0.9677     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4930    0.5464    0.5183       388\n",
      "         joy     0.6985    0.7252    0.7116       131\n",
      "       trust     0.3475    0.3280    0.3374       125\n",
      "     disgust     0.2479    0.2566    0.2522       113\n",
      "    optimism     0.4386    0.4545    0.4464       110\n",
      "anticipation     0.3306    0.4255    0.3721        94\n",
      "     sadness     0.3182    0.3387    0.3281        62\n",
      "        fear     0.3415    0.2692    0.3011        52\n",
      "    surprise     0.2308    0.0857    0.1250        35\n",
      "       anger     0.2500    0.1143    0.1569        35\n",
      "   pessimism     0.1000    0.0345    0.0513        29\n",
      "        love     0.5556    0.2941    0.3846        17\n",
      "\n",
      "    accuracy                         0.4324      1191\n",
      "   macro avg     0.3627    0.3227    0.3321      1191\n",
      "weighted avg     0.4200    0.4324    0.4223      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.8 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8772    0.8450    0.8608      2097\n",
      "         joy     0.9697    0.9766    0.9732      2097\n",
      "       trust     0.9610    0.9399    0.9503      2097\n",
      "     disgust     0.9717    0.9661    0.9689      2097\n",
      "    optimism     0.9425    0.9619    0.9521      2097\n",
      "anticipation     0.9213    0.9380    0.9296      2097\n",
      "     sadness     0.9871    0.9871    0.9871      2097\n",
      "        fear     0.9948    0.9948    0.9948      2097\n",
      "    surprise     0.9962    1.0000    0.9981      2097\n",
      "       anger     0.9967    1.0000    0.9983      2097\n",
      "   pessimism     0.9929    1.0000    0.9964      2097\n",
      "        love     0.9957    1.0000    0.9979      2097\n",
      "\n",
      "    accuracy                         0.9675     25164\n",
      "   macro avg     0.9672    0.9675    0.9673     25164\n",
      "weighted avg     0.9672    0.9675    0.9673     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5117    0.5619    0.5356       388\n",
      "         joy     0.6857    0.7328    0.7085       131\n",
      "       trust     0.4034    0.3840    0.3934       125\n",
      "     disgust     0.2479    0.2566    0.2522       113\n",
      "    optimism     0.4386    0.4545    0.4464       110\n",
      "anticipation     0.3130    0.3830    0.3445        94\n",
      "     sadness     0.3382    0.3710    0.3538        62\n",
      "        fear     0.2857    0.2308    0.2553        52\n",
      "    surprise     0.2222    0.1143    0.1509        35\n",
      "       anger     0.2727    0.0857    0.1304        35\n",
      "   pessimism     0.1000    0.0345    0.0513        29\n",
      "        love     0.4545    0.2941    0.3571        17\n",
      "\n",
      "    accuracy                         0.4408      1191\n",
      "   macro avg     0.3561    0.3253    0.3316      1191\n",
      "weighted avg     0.4268    0.4408    0.4302      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.75 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8840    0.8398    0.8613      2097\n",
      "         joy     0.9785    0.9771    0.9778      2097\n",
      "       trust     0.9568    0.9394    0.9480      2097\n",
      "     disgust     0.9699    0.9666    0.9682      2097\n",
      "    optimism     0.9429    0.9690    0.9558      2097\n",
      "anticipation     0.9269    0.9428    0.9348      2097\n",
      "     sadness     0.9886    0.9924    0.9905      2097\n",
      "        fear     0.9929    0.9962    0.9945      2097\n",
      "    surprise     0.9934    1.0000    0.9967      2097\n",
      "       anger     0.9967    1.0000    0.9983      2097\n",
      "   pessimism     0.9901    1.0000    0.9950      2097\n",
      "        love     0.9990    1.0000    0.9995      2097\n",
      "\n",
      "    accuracy                         0.9686     25164\n",
      "   macro avg     0.9683    0.9686    0.9684     25164\n",
      "weighted avg     0.9683    0.9686    0.9684     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4953    0.5490    0.5208       388\n",
      "         joy     0.6831    0.7405    0.7106       131\n",
      "       trust     0.4196    0.3760    0.3966       125\n",
      "     disgust     0.2353    0.2124    0.2233       113\n",
      "    optimism     0.4160    0.4727    0.4426       110\n",
      "anticipation     0.3246    0.3936    0.3558        94\n",
      "     sadness     0.3030    0.3226    0.3125        62\n",
      "        fear     0.2778    0.2885    0.2830        52\n",
      "    surprise     0.2857    0.1143    0.1633        35\n",
      "       anger     0.2727    0.0857    0.1304        35\n",
      "   pessimism     0.2143    0.1034    0.1395        29\n",
      "        love     0.5714    0.2353    0.3333        17\n",
      "\n",
      "    accuracy                         0.4358      1191\n",
      "   macro avg     0.3749    0.3245    0.3343      1191\n",
      "weighted avg     0.4246    0.4358    0.4250      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.7 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8850    0.8441    0.8640      2097\n",
      "         joy     0.9715    0.9747    0.9731      2097\n",
      "       trust     0.9565    0.9428    0.9496      2097\n",
      "     disgust     0.9698    0.9642    0.9670      2097\n",
      "    optimism     0.9442    0.9762    0.9599      2097\n",
      "anticipation     0.9210    0.9285    0.9247      2097\n",
      "     sadness     0.9862    0.9862    0.9862      2097\n",
      "        fear     0.9919    0.9962    0.9941      2097\n",
      "    surprise     0.9962    1.0000    0.9981      2097\n",
      "       anger     0.9971    1.0000    0.9986      2097\n",
      "   pessimism     0.9934    1.0000    0.9967      2097\n",
      "        love     0.9967    1.0000    0.9983      2097\n",
      "\n",
      "    accuracy                         0.9677     25164\n",
      "   macro avg     0.9675    0.9677    0.9675     25164\n",
      "weighted avg     0.9675    0.9677    0.9675     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5012    0.5464    0.5228       388\n",
      "         joy     0.6985    0.7252    0.7116       131\n",
      "       trust     0.3950    0.3760    0.3852       125\n",
      "     disgust     0.2321    0.2301    0.2311       113\n",
      "    optimism     0.4348    0.4545    0.4444       110\n",
      "anticipation     0.3248    0.4043    0.3602        94\n",
      "     sadness     0.3514    0.4194    0.3824        62\n",
      "        fear     0.2174    0.1923    0.2041        52\n",
      "    surprise     0.1333    0.0571    0.0800        35\n",
      "       anger     0.2308    0.0857    0.1250        35\n",
      "   pessimism     0.1667    0.0690    0.0976        29\n",
      "        love     0.4444    0.2353    0.3077        17\n",
      "\n",
      "    accuracy                         0.4324      1191\n",
      "   macro avg     0.3442    0.3163    0.3210      1191\n",
      "weighted avg     0.4183    0.4324    0.4220      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.65 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8852    0.8455    0.8649      2097\n",
      "         joy     0.9707    0.9809    0.9758      2097\n",
      "       trust     0.9643    0.9390    0.9514      2097\n",
      "     disgust     0.9715    0.9747    0.9731      2097\n",
      "    optimism     0.9439    0.9700    0.9567      2097\n",
      "anticipation     0.9269    0.9309    0.9289      2097\n",
      "     sadness     0.9876    0.9909    0.9893      2097\n",
      "        fear     0.9933    0.9957    0.9945      2097\n",
      "    surprise     0.9934    1.0000    0.9967      2097\n",
      "       anger     0.9976    1.0000    0.9988      2097\n",
      "   pessimism     0.9915    1.0000    0.9957      2097\n",
      "        love     0.9981    1.0000    0.9990      2097\n",
      "\n",
      "    accuracy                         0.9690     25164\n",
      "   macro avg     0.9687    0.9690    0.9687     25164\n",
      "weighted avg     0.9687    0.9690    0.9687     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5157    0.5515    0.5330       388\n",
      "         joy     0.6786    0.7252    0.7011       131\n",
      "       trust     0.3689    0.3600    0.3644       125\n",
      "     disgust     0.2476    0.2301    0.2385       113\n",
      "    optimism     0.4324    0.4364    0.4344       110\n",
      "anticipation     0.3197    0.4149    0.3611        94\n",
      "     sadness     0.3279    0.3226    0.3252        62\n",
      "        fear     0.2679    0.2885    0.2778        52\n",
      "    surprise     0.2143    0.0857    0.1224        35\n",
      "       anger     0.2308    0.0857    0.1250        35\n",
      "   pessimism     0.1364    0.1034    0.1176        29\n",
      "        love     0.5000    0.2941    0.3704        17\n",
      "\n",
      "    accuracy                         0.4332      1191\n",
      "   macro avg     0.3533    0.3248    0.3309      1191\n",
      "weighted avg     0.4223    0.4332    0.4247      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.6 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8705    0.8431    0.8566      2097\n",
      "         joy     0.9701    0.9742    0.9722      2097\n",
      "       trust     0.9602    0.9437    0.9519      2097\n",
      "     disgust     0.9685    0.9661    0.9673      2097\n",
      "    optimism     0.9450    0.9590    0.9520      2097\n",
      "anticipation     0.9239    0.9380    0.9309      2097\n",
      "     sadness     0.9857    0.9866    0.9862      2097\n",
      "        fear     0.9943    0.9919    0.9931      2097\n",
      "    surprise     0.9938    1.0000    0.9969      2097\n",
      "       anger     0.9971    1.0000    0.9986      2097\n",
      "   pessimism     0.9953    1.0000    0.9976      2097\n",
      "        love     0.9957    1.0000    0.9979      2097\n",
      "\n",
      "    accuracy                         0.9669     25164\n",
      "   macro avg     0.9667    0.9669    0.9668     25164\n",
      "weighted avg     0.9667    0.9669    0.9668     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4929    0.5361    0.5136       388\n",
      "         joy     0.6950    0.7481    0.7206       131\n",
      "       trust     0.3282    0.3440    0.3359       125\n",
      "     disgust     0.2162    0.2124    0.2143       113\n",
      "    optimism     0.4324    0.4364    0.4344       110\n",
      "anticipation     0.3363    0.4043    0.3671        94\n",
      "     sadness     0.2857    0.3226    0.3030        62\n",
      "        fear     0.1951    0.1538    0.1720        52\n",
      "    surprise     0.1250    0.0286    0.0465        35\n",
      "       anger     0.1875    0.0857    0.1176        35\n",
      "   pessimism     0.1875    0.1034    0.1333        29\n",
      "        love     0.4545    0.2941    0.3571        17\n",
      "\n",
      "    accuracy                         0.4190      1191\n",
      "   macro avg     0.3280    0.3058    0.3096      1191\n",
      "weighted avg     0.4021    0.4190    0.4077      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.55 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8665    0.8355    0.8507      2097\n",
      "         joy     0.9681    0.9695    0.9688      2097\n",
      "       trust     0.9631    0.9323    0.9474      2097\n",
      "     disgust     0.9682    0.9590    0.9636      2097\n",
      "    optimism     0.9350    0.9599    0.9473      2097\n",
      "anticipation     0.9162    0.9380    0.9270      2097\n",
      "     sadness     0.9866    0.9862    0.9864      2097\n",
      "        fear     0.9929    0.9981    0.9955      2097\n",
      "    surprise     0.9938    1.0000    0.9969      2097\n",
      "       anger     0.9967    1.0000    0.9983      2097\n",
      "   pessimism     0.9920    1.0000    0.9960      2097\n",
      "        love     0.9971    1.0000    0.9986      2097\n",
      "\n",
      "    accuracy                         0.9649     25164\n",
      "   macro avg     0.9647    0.9649    0.9647     25164\n",
      "weighted avg     0.9647    0.9649    0.9647     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4965    0.5490    0.5214       388\n",
      "         joy     0.6866    0.7023    0.6943       131\n",
      "       trust     0.3534    0.3280    0.3402       125\n",
      "     disgust     0.2727    0.2920    0.2821       113\n",
      "    optimism     0.4234    0.4273    0.4253       110\n",
      "anticipation     0.3276    0.4043    0.3619        94\n",
      "     sadness     0.3667    0.3548    0.3607        62\n",
      "        fear     0.2241    0.2500    0.2364        52\n",
      "    surprise     0.0000    0.0000    0.0000        35\n",
      "       anger     0.2500    0.0857    0.1277        35\n",
      "   pessimism     0.2308    0.1034    0.1429        29\n",
      "        love     0.4167    0.2941    0.3448        17\n",
      "\n",
      "    accuracy                         0.4282      1191\n",
      "   macro avg     0.3374    0.3159    0.3198      1191\n",
      "weighted avg     0.4130    0.4282    0.4178      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.5 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8663    0.8278    0.8466      2097\n",
      "         joy     0.9720    0.9766    0.9743      2097\n",
      "       trust     0.9575    0.9351    0.9462      2097\n",
      "     disgust     0.9644    0.9571    0.9607      2097\n",
      "    optimism     0.9432    0.9657    0.9543      2097\n",
      "anticipation     0.9197    0.9399    0.9297      2097\n",
      "     sadness     0.9871    0.9881    0.9876      2097\n",
      "        fear     0.9933    0.9962    0.9948      2097\n",
      "    surprise     0.9948    1.0000    0.9974      2097\n",
      "       anger     0.9962    1.0000    0.9981      2097\n",
      "   pessimism     0.9901    1.0000    0.9950      2097\n",
      "        love     0.9981    1.0000    0.9990      2097\n",
      "\n",
      "    accuracy                         0.9655     25164\n",
      "   macro avg     0.9652    0.9655    0.9653     25164\n",
      "weighted avg     0.9652    0.9655    0.9653     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4904    0.5284    0.5087       388\n",
      "         joy     0.7077    0.7023    0.7050       131\n",
      "       trust     0.3451    0.3120    0.3277       125\n",
      "     disgust     0.2353    0.2478    0.2414       113\n",
      "    optimism     0.4118    0.4455    0.4279       110\n",
      "anticipation     0.3025    0.3830    0.3380        94\n",
      "     sadness     0.3548    0.3548    0.3548        62\n",
      "        fear     0.2041    0.1923    0.1980        52\n",
      "    surprise     0.0714    0.0286    0.0408        35\n",
      "       anger     0.2105    0.1143    0.1481        35\n",
      "   pessimism     0.1500    0.1034    0.1224        29\n",
      "        love     0.4444    0.2353    0.3077        17\n",
      "\n",
      "    accuracy                         0.4139      1191\n",
      "   macro avg     0.3273    0.3040    0.3101      1191\n",
      "weighted avg     0.4037    0.4139    0.4068      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.45 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8802    0.8412    0.8603      2097\n",
      "         joy     0.9684    0.9781    0.9732      2097\n",
      "       trust     0.9527    0.9404    0.9465      2097\n",
      "     disgust     0.9631    0.9590    0.9611      2097\n",
      "    optimism     0.9461    0.9619    0.9539      2097\n",
      "anticipation     0.9251    0.9418    0.9334      2097\n",
      "     sadness     0.9914    0.9895    0.9905      2097\n",
      "        fear     0.9924    0.9948    0.9936      2097\n",
      "    surprise     0.9943    1.0000    0.9971      2097\n",
      "       anger     0.9953    1.0000    0.9976      2097\n",
      "   pessimism     0.9957    1.0000    0.9979      2097\n",
      "        love     0.9986    1.0000    0.9993      2097\n",
      "\n",
      "    accuracy                         0.9672     25164\n",
      "   macro avg     0.9669    0.9672    0.9670     25164\n",
      "weighted avg     0.9669    0.9672    0.9670     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4807    0.5464    0.5115       388\n",
      "         joy     0.7197    0.7252    0.7224       131\n",
      "       trust     0.3697    0.3520    0.3607       125\n",
      "     disgust     0.2581    0.2832    0.2700       113\n",
      "    optimism     0.4579    0.4455    0.4516       110\n",
      "anticipation     0.3190    0.3936    0.3524        94\n",
      "     sadness     0.3548    0.3548    0.3548        62\n",
      "        fear     0.2195    0.1731    0.1935        52\n",
      "    surprise     0.1250    0.0286    0.0465        35\n",
      "       anger     0.2353    0.1143    0.1538        35\n",
      "   pessimism     0.1538    0.0690    0.0952        29\n",
      "        love     0.4545    0.2941    0.3571        17\n",
      "\n",
      "    accuracy                         0.4299      1191\n",
      "   macro avg     0.3457    0.3150    0.3225      1191\n",
      "weighted avg     0.4154    0.4299    0.4193      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.4 \n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.8681    0.8379    0.8527      2097\n",
      "         joy     0.9700    0.9719    0.9709      2097\n",
      "       trust     0.9598    0.9342    0.9468      2097\n",
      "     disgust     0.9664    0.9595    0.9629      2097\n",
      "    optimism     0.9336    0.9661    0.9496      2097\n",
      "anticipation     0.9206    0.9285    0.9245      2097\n",
      "     sadness     0.9872    0.9900    0.9886      2097\n",
      "        fear     0.9886    0.9924    0.9905      2097\n",
      "    surprise     0.9953    1.0000    0.9976      2097\n",
      "       anger     0.9981    1.0000    0.9990      2097\n",
      "   pessimism     0.9938    1.0000    0.9969      2097\n",
      "        love     0.9962    1.0000    0.9981      2097\n",
      "\n",
      "    accuracy                         0.9650     25164\n",
      "   macro avg     0.9648    0.9650    0.9649     25164\n",
      "weighted avg     0.9648    0.9650    0.9649     25164\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4942    0.5515    0.5213       388\n",
      "         joy     0.7164    0.7328    0.7245       131\n",
      "       trust     0.3719    0.3600    0.3659       125\n",
      "     disgust     0.2364    0.2301    0.2332       113\n",
      "    optimism     0.4464    0.4545    0.4505       110\n",
      "anticipation     0.3226    0.4255    0.3670        94\n",
      "     sadness     0.3651    0.3710    0.3680        62\n",
      "        fear     0.2326    0.1923    0.2105        52\n",
      "    surprise     0.0000    0.0000    0.0000        35\n",
      "       anger     0.1538    0.0571    0.0833        35\n",
      "   pessimism     0.1905    0.1379    0.1600        29\n",
      "        love     0.6667    0.3529    0.4615        17\n",
      "\n",
      "    accuracy                         0.4332      1191\n",
      "   macro avg     0.3497    0.3222    0.3288      1191\n",
      "weighted avg     0.4158    0.4332    0.4219      1191\n",
      "\n",
      "\n",
      " ============================================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# search for one optimal hepyparameter\n",
    "labels = list(label2key.keys())\n",
    "res = []\n",
    "params = list(range(40, 101, 5))\n",
    "for param in reversed(params):\n",
    "\n",
    "    clf_params_xgb2 = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 6,                 # maybe 4 w/gamma=0.55 (train accu=0.9 vs. 0.97 for depth 6)\n",
    "        'learning_rate': 0.3,                                 # eta  \n",
    "        'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 1.5,                                     # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': param/100,                               # 0-1    \n",
    "        'colsample_bylevel': 1.0,                             # 0-1\n",
    "        'colsample_bynode': 1.0,                              # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                              # 0-1  \n",
    "        'seed': 2,\n",
    "        'num_class': 12,\n",
    "        #'use_label_encoder': False,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "    }\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 1.0,\n",
    "        'min_df': 8,            # best 8\n",
    "        'analyzer': 'char',\n",
    "        'ngram_range': (1,4),   # best (1,4)\n",
    "        'binary': True,\n",
    "        'stop_words': 'english',\n",
    "    }\n",
    "\n",
    "    vectorizer = CountVectorizer( **vect_params2 )\n",
    "    clf        = XGBClassifier( **clf_params_xgb2 )\n",
    "    model      = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev   = model.predict(X_dev)\n",
    "\n",
    "    # print classification reports\n",
    "    print('Parameter:', param/100, '\\n')\n",
    "    print('\\nTRAINSET')\n",
    "    print( classification_report( y_train, y_pred_train, target_names=labels, digits=4 ) )\n",
    "    clf_rep1 = classification_report( y_train, y_pred_train, target_names=labels, output_dict=True )\n",
    "\n",
    "    print('DEVSET')\n",
    "    print( classification_report( y_dev, y_pred_dev, target_names=labels, digits=4 ) )\n",
    "    clf_rep2 = classification_report( y_dev, y_pred_dev, target_names=labels, output_dict=True )\n",
    "    \n",
    "    res.append([ clf_rep1['macro avg']['f1-score'], clf_rep1['accuracy'],\n",
    "                 clf_rep2['macro avg']['f1-score'], clf_rep2['accuracy'], param, ])\n",
    "    \n",
    "    print('\\n', '='*78, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f2ed8eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9700934669687205, 0.970274996026069, 0.3503155943511486, 0.4508816120906801, 100]\n",
      "[0.9683748207692101, 0.9686059450007948, 0.3343079157672156, 0.4357682619647355, 75]\n",
      "[0.967742438920736, 0.96793037672866, 0.3320877010346391, 0.4324097397145256, 85]\n",
      "[0.9672918461644503, 0.9674535050071531, 0.33163539380150825, 0.44080604534005036, 80]\n",
      "[0.9673085085663319, 0.9674932443172787, 0.331150312582271, 0.42737195633921077, 90]\n",
      "[0.968739745640386, 0.968963598791925, 0.33091337717551106, 0.4332493702770781, 65]\n",
      "[0.9648524688070638, 0.9650294070894929, 0.32880852700348967, 0.4332493702770781, 40]\n",
      "[0.968186148724008, 0.9684072484501669, 0.32873200127580676, 0.42737195633921077, 95]\n",
      "[0.9670162290710995, 0.9672150691463997, 0.3224758890213447, 0.42989084802686817, 45]\n",
      "[0.967520145105364, 0.9677316801780321, 0.32100839322477714, 0.4324097397145256, 70]\n",
      "[0.9646998331946658, 0.9648704498489906, 0.31980562516758676, 0.4282115869017632, 55]\n",
      "[0.9653166395000299, 0.9655460181211254, 0.3100596791754279, 0.4139378673383711, 50]\n",
      "[0.9667516227123089, 0.966897154665395, 0.30963656551739166, 0.418975650713686, 60]\n"
     ]
    }
   ],
   "source": [
    "# review results\n",
    "res = sorted(res, key=lambda x: x[2], reverse=True)\n",
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838d7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0026756a",
   "metadata": {},
   "source": [
    "## Log of some best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7da44c",
   "metadata": {},
   "source": [
    "__Obsertvations__:\n",
    "* tfidf is more overfit, runs longer, macro-F1 is lower than Countverctorizer\n",
    "* Counvectorizer (True) is least overfit (0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e5a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST HYPERPARAMETERS\n",
    "clf_params_xgb = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,                 # maybe 4 w/gamma=0.55 (train accu=0.9 vs. 0.97 for depth 6)\n",
    "    'learning_rate': 0.3,                                 # eta  \n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 1.5,                                     # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,                             # 0-1\n",
    "    'colsample_bynode': 1.0,                              # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 12,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 8,            # best 8\n",
    "    'analyzer': 'char',\n",
    "    'ngram_range': (1,4),   # best (1,4)\n",
    "    'binary': True,\n",
    "    'stop_words': 'english',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e2338",
   "metadata": {},
   "source": [
    "__Best results with the best hyperparameters__:\n",
    "```\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     neutral     0.8907    0.8546    0.8722      2097\n",
    "         joy     0.9744    0.9800    0.9772      2097\n",
    "       trust     0.9641    0.9361    0.9499      2097\n",
    "     disgust     0.9741    0.9695    0.9718      2097\n",
    "    optimism     0.9495    0.9676    0.9584      2097\n",
    "anticipation     0.9262    0.9461    0.9361      2097\n",
    "     sadness     0.9867    0.9905    0.9886      2097\n",
    "        fear     0.9915    0.9990    0.9952      2097\n",
    "    surprise     0.9957    1.0000    0.9979      2097\n",
    "       anger     0.9976    1.0000    0.9988      2097\n",
    "   pessimism     0.9938    1.0000    0.9969      2097\n",
    "        love     0.9962    1.0000    0.9981      2097\n",
    "\n",
    "    accuracy                         0.9703     25164\n",
    "   macro avg     0.9700    0.9703    0.9701     25164\n",
    "weighted avg     0.9700    0.9703    0.9701     25164\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     neutral     0.5255    0.5851    0.5537       388\n",
    "         joy     0.7273    0.7328    0.7300       131\n",
    "       trust     0.3898    0.3680    0.3786       125\n",
    "     disgust     0.2778    0.2655    0.2715       113\n",
    "    optimism     0.4393    0.4273    0.4332       110\n",
    "anticipation     0.3223    0.4149    0.3628        94\n",
    "     sadness     0.3492    0.3548    0.3520        62\n",
    "        fear     0.2281    0.2500    0.2385        52\n",
    "    surprise     0.3125    0.1429    0.1961        35\n",
    "       anger     0.2857    0.1143    0.1633        35\n",
    "   pessimism     0.2143    0.1034    0.1395        29\n",
    "        love     0.5556    0.2941    0.3846        17\n",
    "\n",
    "    accuracy                         0.4509      1191\n",
    "   macro avg     0.3856    0.3378    0.3503      1191\n",
    "weighted avg     0.4433    0.4509    0.4430      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4825d",
   "metadata": {},
   "source": [
    "__Another good results__  \n",
    "analyzer='char', Counvectorizer, binary=True, ngram_range=(1,4), LR=0.3\n",
    "```\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "                            \n",
    "    accuracy                         0.9768     25164\n",
    "   macro avg     0.9767    0.9768    0.9767     25164\n",
    "weighted avg     0.9767    0.9768    0.9767     25164\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     neutral     0.4989    0.5825    0.5375       388\n",
    "         joy     0.6828    0.7557    0.7174       131\n",
    "       trust     0.4000    0.3840    0.3918       125\n",
    "     disgust     0.2315    0.2212    0.2262       113\n",
    "    optimism     0.4706    0.4364    0.4528       110\n",
    "anticipation     0.3063    0.3617    0.3317        94\n",
    "     sadness     0.3509    0.3226    0.3361        62\n",
    "        fear     0.2917    0.2692    0.2800        52\n",
    "    surprise     0.3750    0.0857    0.1395        35\n",
    "       anger     0.3333    0.1143    0.1702        35\n",
    "   pessimism     0.1667    0.1034    0.1277        29\n",
    "        love     0.6667    0.3529    0.4615        17\n",
    "\n",
    "    accuracy                         0.4450      1191\n",
    "   macro avg     0.3979    0.3325    0.3477      1191\n",
    "weighted avg     0.4346    0.4450    0.4331      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27995476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
