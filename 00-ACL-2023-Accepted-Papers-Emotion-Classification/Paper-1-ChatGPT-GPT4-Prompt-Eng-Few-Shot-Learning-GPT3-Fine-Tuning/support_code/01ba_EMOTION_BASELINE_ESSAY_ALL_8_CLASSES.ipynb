{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "# EMO BASELINE\n",
    "## ACL 2023 Conference\n",
    "## WASSA 2023 Shared Task on Empathy, Emotion, and Personality Detection in Interactions\n",
    "More details [here](https://codalab.lisn.upsaclay.fr/competitions/11167#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import ftfy\n",
    "import pycld2 as cld2\n",
    "import time\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2ef3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeClassifierCVProba(RidgeClassifierCV):\n",
    "    '''\n",
    "        This lets RidgeClassifierCV() output probabilities with predict_proba()\n",
    "    '''\n",
    "    def predict_proba(self, X):\n",
    "        d = self.decision_function(X)\n",
    "        return np.exp(d) / np.sum(np.exp(d))\n",
    "    \n",
    "class RidgeClassifierProba(RidgeClassifier):\n",
    "    '''\n",
    "        This lets RidgeClassifier() output probabilities with predict_proba()\n",
    "    '''\n",
    "    def predict_proba(self, X):\n",
    "        d = self.decision_function(X)\n",
    "        return np.exp(d) / np.sum(np.exp(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d6f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "    \n",
    "# model = Pipeline( steps=[('vect', vectorizer), ('to_dense', DenseTransformer()), ('clf', clf)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08dd2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_spaces = re.compile('\\s{2,}')\n",
    "\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    for char in ['�', '•']:\n",
    "        if char in s:\n",
    "            s = s.replace(char, ' ')\n",
    "    s = ftfy.fix_text(s)\n",
    "    \n",
    "    #s = clean.sub(' ', s.lower())\n",
    "    s = multi_spaces.sub(' ', s)\n",
    "        \n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0933b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang( t ):\n",
    "    '''\n",
    "        Return the language(s) in string s.\n",
    "        Naive Bayes classifier under the hood -\n",
    "        results are less certain for strings that are too short.\n",
    "        Returns up to three languages with confidence scores.\n",
    "        More on usage: https://pypi.org/project/pycld2/\n",
    "    '''\n",
    "    _, _, details = cld2.detect( ftfy.fix_text( t ) )\n",
    "    return details[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a7ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(emotions: List[str])->List[int]:\n",
    "    '''\n",
    "        Convert list of strings with categories into list of 0s and 1s with length 8 because there are 8 categories;\n",
    "        1 in the i-th position means that this essay belongs to the i-th category as in key2label[i]\n",
    "    '''\n",
    "    res  = [0]*8\n",
    "    idxs = [label2key[e] for e in emotions]    \n",
    "    for idx in idxs:\n",
    "        res[idx] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbb2ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Anger', 1: 'Disgust', 2: 'Fear', 3: 'Hope', 4: 'Joy', 5: 'Neutral', 6: 'Sadness', 7: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "# target variables\n",
    "label2key = {   \n",
    "    'Anger':    0,\n",
    "    'Disgust':  1,\n",
    "    'Fear':     2,\n",
    "    'Hope':     3,    \n",
    "    'Joy':      4,\n",
    "    'Neutral':  5,\n",
    "    'Sadness':  6,\n",
    "    'Surprise': 7,\n",
    "}\n",
    "key2label = {v: k for k,v in label2key.items()}\n",
    "print(key2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56940aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new new version (Dec 2022)\n",
    "def upsample_all( df_, labels_col='target', random_state=47 ):\n",
    "    '''\n",
    "        Upsample each class in column labels_col of pandas dataframe df_\n",
    "        to the number of data points in majority class\n",
    "    '''\n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_[labels_col].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_[labels_col] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "\n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "\n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "\n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0a230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_8cats =      [ \"'s\", 'a', 'about', 'after', 'again', 'all', 'am', 'america', 'an', 'and', 'animal', 'animals',\n",
    "                    'are', 'around', 'as', 'at', 'bad', 'be', 'because', 'but', 'by', 'can', 'children',\n",
    "                    'crazy', 'death', 'do', 'even', 'find', 'for', 'from', 'get', 'go', 'had', 'has', 'have',\n",
    "                    'having', 'he', 'his', 'horrible', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'just',\n",
    "                    'kill', 'killed', 'know', 'like', 'live', 'life', 'lives', 'lived', 'm', 'make', 'makes',\n",
    "                    'man', 'me', 'mind', 'more', 'most', 'much', 'my', 'need', 'never', 'no', 'not', 'now',\n",
    "                    'of', 'on', 'one', 'or', 'other', 'out', 'people', 'place', 'put', 'really', 'sad', 'see',\n",
    "                    'seems', 'situation', 'so', 'some', 'something', 'species', 'stop', 'story',\n",
    "                    'such', 't', 'take', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'thing',\n",
    "                    'things', 'think', 'this', 'time', 'to', 'type', 'up', 'us', 'very', 'war', 'was', 'way',\n",
    "                    'we', 'were', 'what', 'when', 'with', 'worse', 'would', 'you',\n",
    "                   ]\n",
    "\n",
    "words_7cats      = [ 'age', 'air', 'also', 'always', 'any', 'article', 'attack', 'away', 'back', 'been', 'before',\n",
    "                     'being', 'believe', 'both', 'cause', 'child', 'could', 'country', 'day', 'deal', 'did', 'die',\n",
    "                     'disease', 'done', 'down', 'during', 'dying', 'each', 'either', 'end', 'facing', 'feel',\n",
    "                     'felt', 'first', 'food', 'future', 'girl', 'glad', 'going', 'good', 'government', 'great',\n",
    "                     'guess', 'happened', 'happening', 'hard', 'harm', 'hate', 'her', 'high', 'him', 'humans',\n",
    "                     'imagine', 'instead', 'interesting', 'job', 'jobs', 'keep', 'kids', 'leave', 'left', 'let',\n",
    "                     'life', 'living', 'lost', 'lot', 'make', 'many', 'needs', 'new', 'normal', 'often', 'oil',\n",
    "                     'only', 'over', 'pain', 'person', 'places', 'poor', 'population', 'probably', 'problem',\n",
    "                     'protect', 'read', 'reading', 'real', 'same', 'say', 'she', 'should', 'show', 'sick',\n",
    "                     'society', 'someone', 'sounds', 'start', 'still', 'suffering', 'sure', 'terrible',\n",
    "                     'thinking', 'those', 'though', 'thought', 'twice', 'under', 'water', 'were', 'where',\n",
    "                     'which', 'who', 'whole', 'why', 'wildlife', 'will', 'woman', 'wonder', 'world', 'worried',\n",
    "                     'years', 'your', ]\n",
    "\n",
    "experimental_sw = words_7cats + words_8cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dafb5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['enough', 'often', 'con', 'whence', 'them', 'been', 'anyhow', 'even', 'whither', 'everywhere', 'about', 'themselves', 'somehow', 'others', 'eleven', 'fill', 'under', 'forty', 'same', 'seemed', 'between', 'however', 'whereupon', 'has', 'besides', 'latterly', 'which', 'else', 'am', 'part', 'the', 'made', 'nothing', 'not', 'since', 'take', 'least', 'becoming', 'everyone', 'own', 'empty', 'sometime', 'those', 'many', 'seeming', 'further', 'hereby', 'up', 'both', 'cant', 'ours', 'another', 'three', 'until', 'become', 'who', 'seems', 'two', 'could', 'none', 'her', 'on', 'what', 'de', 'beside', 'and', 'sincere', 'our', 'us', 'wherever', 'un', 'sometimes', 'is', 'when', 'move', 'although', 're', 'a', 'elsewhere', 'toward', 'much', 'per', 'together', 'also', 'few', 'never', 'that', 'therefore', 'inc', 'therein', 'down', 'whoever', 'next', 'perhaps', 'always', 'whether', 'amount', 'describe', 'nobody', 'either', 'anyone', 'fifteen', 'thin', 'wherein', 'should', 'everything', 'get', 'thence', 'some', 'namely', 'someone', 'due', 'again', 'fifty', 'might', 'amoungst', 'nor', 'several', 'behind', 'hasnt', 'herself', 'full', 'their', 'five', 'his', 'whose', 'twenty', 'whole', 'fire', 'your', 'moreover', 'now', 'over', 'ie', 'one', 'because', 'for', 'such', 'afterwards', 'to', 'sixty', 'third', 'side', 'four', 'this', 'upon', 'yours', 'me', 'hundred', 'show', 'first', 'during', 'him', 'neither', 'other', 'how', 'be', 'would', 'hereupon', 'yet', 'all', 'hers', 'alone', 'indeed', 'thereafter', 'whenever', 'meanwhile', 'whereas', 'anything', 'front', 'nine', 'hereafter', 'any', 'amongst', 'once', 'across', 'as', 'towards', 'done', 'less', 'give', 'though', 'before', 'via', 'found', 'so', 'interest', 'serious', 'if', 'in', 'yourselves', 'with', 'why', 'please', 'among', 'keep', 'of', 'or', 'were', 'no', 'very', 'mill', 'itself', 'have', 'still', 'my', 'eight', 'against', 'beforehand', 'somewhere', 'after', 'off', 'only', 'they', 'here', 'thereby', 'beyond', 'cry', 'thus', 'by', 'bill', 'each', 'former', 'eg', 'twelve', 'myself', 'bottom', 'already', 'i', 'system', 'ten', 'anyway', 'these', 'rather', 'detail', 'along', 'ourselves', 'may', 'see', 'thru', 'mostly', 'seem', 'through', 'put', 'most', 'etc', 'couldnt', 'every', 'name', 'almost', 'above', 'mine', 'into', 'herein', 'are', 'was', 'you', 'himself', 'thick', 'became', 'then', 'than', 'whereafter', 'thereupon', 'too', 'becomes', 'will', 'below', 'without', 'ltd', 'at', 'anywhere', 'do', 'back', 'last', 'she', 'yourself', 'something', 'formerly', 'an', 'find', 'six', 'but', 'latter', 'can', 'while', 'being', 'there', 'well', 'whatever', 'whom', 'throughout', 'ever', 'co', 'go', 'otherwise', 'its', 'where', 'noone', 'more', 'nowhere', 'we', 'around', 'within', 'onto', 'top', 'call', 'whereby', 'he', 'out', 'hence', 'must', 'from', 'had', 'cannot', 'it', 'except', 'nevertheless']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "154\n",
      "['i', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['enough', 'unable', 'often', 'con', 'wherewith', 'whence', 'them', 'thenceforth', 'been', 'anyhow', 'even', 'whither', 'everywhere', \"how's\", 'about', 'themselves', 'somehow', 'others', 'thereon', 'thou', \"shouldn't\", 'eleven', 'fill', 'somebody', 'under', 'forty', 'same', 'seemed', 'between', \"why's\", 'however', 'whereupon', 'has', 'besides', 'latterly', 'which', 'upwards', 'else', 'hardly', 'am', 'part', 'km', '+', 'the', 'made', 'nothing', 'not', 'since', 'take', 'least', 'didn', 'howsoever', 'becoming', 'albeit', 'doing', 'everyone', 'yippee', 'own', 'empty', 'sometime', 'those', 'ain', 'nonetheless', \"mightn't\", 'many', 'spoke', 'seeming', \"she's\", 'like', 'quite', 'further', 'hereby', 'up', 'av', 'worse', 'both', 'having', \"wasn't\", 'cant', 'ours', 'another', 'three', 'ought', 'until', 'become', \"that's\", 'who', 'seems', 'two', 'could', 'slunk', 'none', 'thee', 'her', 'on', 'everybody', 'what', 'de', 'beside', 'and', 'sake', 'sincere', 'whichever', 'our', 'double', 'us', 'isn', 'wherever', 'un', '*', 'hath', 'sometimes', 'is', 'supposing', 'when', 'move', 'although', 'round', 're', 'a', 'elsewhere', 'toward', 'much', 'per', 'inside', 'halves', 'indoors', 'together', 'thereof', 'also', 'few', 'never', 'nowadays', 'whereon', 'that', 'therefore', 'use', \"they'll\", 'inc', 'therein', 'down', \"hadn't\", 'whoever', 'next', 'perhaps', 'farthest', 'always', 'hadn', '?', 'whether', '.', 'contrariwise', 'amount', 'describe', 'hindmost', 'needn', 'hither', 'nobody', 'either', 'furthest', 'anyone', 'fifteen', 'thin', 'thereto', 'vs', 'wherein', 'should', 'everything', 'notwithstanding', 'don', 'get', 'thence', 'ugh', 'some', 'mustn', \"we've\", \"you've\", 'namely', 'someone', 'due', 'again', 'day', 'fifty', 'might', 'sprang', 'doth', 'kind', 'did', 'amoungst', \"aren't\", 'nor', 'several', 'behind', 'hasnt', 'herself', 'full', 'their', 'canst', \"haven't\", 'five', \"here's\", 'his', 'inasmuch', 'whose', 'twenty', 'whew', 'anybody', 'whole', \"she'll\", '-', 'ms', 'just', \"don't\", 'fire', 'your', 'saw', \"shan't\", 'include', 'nope', 'moreover', \"wouldn't\", \"when's\", 'now', 'over', 'ie', 'worst', 'unlike', 'one', 'because', 'for', \"it's\", 'such', 'afterwards', 'to', 'sixty', \"you'd\", 'third', \"they've\", 'side', 'four', 'this', 'upon', 'yours', 'cu', 'me', 'hundred', 'show', 'need', \"couldn't\", 'first', 'during', 'him', 'neither', 'stave', \"she'd\", 'other', 'haven', 'somewhat', 'including', 'couldn', 'forth', 'provide', 'slew', 'how', 'be', 'would', 'll', 'hereupon', \"i'm\", 'yet', \"they're\", 'all', '\"', 'hers', 'alone', 'indeed', 'thereafter', 'thereabout', 'whoa', 'whenever', 'meanwhile', \"he'd\", 'whereas', 'whosoever', 'anything', 't', 'front', 'nine', 'staves', 'shalt', '/', 'exception', 'hereafter', 'inwards', 'any', 'amongst', 'hereto', 'ff', 'once', 'across', 'cf', 'thrice', \"he'll\", 'cos', 'as', 'maybe', 'towards', 'done', 'less', 'give', 'though', 'before', 'via', 'found', 'so', 'sprung', 'interest', 'whereunto', 'meantime', \"we'll\", 'serious', 'if', 'hitherto', 'in', \"weren't\", 'sideways', \"won't\", 'seldom', 'included', 'yourselves', 'whereto', 'with', \"they'd\", 'henceforth', 'insomuch', \"hasn't\", 'choose', 'shouldn', 'why', 'weren', 'please', 'among', 'selves', 'keep', 'of', 'or', 'were', 'no', 'very', 'mill', 'itself', 'wasn', 'hereabouts', 'mrs', 'little', 'underneath', 'have', 'doesn', 'whatsoever', 'still', 've', 'year', 'my', 'eight', 'against', 'd', 's', 'shown', 'beforehand', 'instead', 'somewhere', 'after', 'off', 'only', 'they', \"doesn't\", \"needn't\", 'used', \"didn't\", 'kg', 'here', 'thereby', 'beyond', 'cry', 'thus', 'lest', \"there's\", 'by', 'using', 'bill', 'furthermore', \"let's\", 'seen', 'save', 'want', 'week', 'each', 'shan', 'smote', 'former', 'whereof', 'whomever', 'eg', 'twelve', 'myself', 'bottom', 'mightn', 'already', 'whensoever', \"i'd\", 'm', 'farther', 'i', 'system', \"we'd\", 'ten', 'exclude', 'far', 'does', 'anyway', 'these', 'forward', 'ok', 'whilst', 'rather', 'detail', 'wherefrom', '``', 'along', 'ourselves', '`', 'may', 'see', 'exclusive', \"should've\", ',', \"''\", 'wouldn', 'sang', 'thru', 'mostly', 'mr', 'seem', 'through', 'spake', 'put', 'spat', \"that'll\", 'most', 'etc', 'couldnt', \"i'll\", 'every', 'sent', 'name', 'almost', 'above', 'dost', 'aren', \"what's\", 'really', 'mine', 'into', 'herein', 'thyself', 'are', 'was', 'you', 'himself', \"we're\", 'thick', 'became', 'then', 'et', 'said', 'than', 'whichsoever', 'whereafter', 'hast', 'thereupon', 'according', 'too', 'becomes', 'will', 'wherefore', \"he's\", 'excepting', 'hasn', 'whereabouts', 'seeing', 'below', 'spoken', 'without', \"isn't\", 'ltd', 'inward', 'at', 'anywhere', \"mustn't\", 'ma', 'do', 'back', 'last', \"where's\", 'she', 'certain', 'yourself', \"you'll\", 'something', 'formerly', \"you're\", 'an', 'find', 'slung', 'six', 'thereabouts', 'but', 'latter', 'excluding', 'can', 'whereat', '!', 'excepted', 'while', 'dual', 'let', 'being', 'there', 'well', 'whatever', 'whom', 'throughout', 'ever', \"who's\", 'co', 'go', 'won', 'whomsoever', 'otherwise', 'slept', 'apart', 'thy', 'its', 'where', 'upward', 'noone', 'more', 'y', 'nowhere', 'we', 'around', 'within', \"i've\", 'o', 'onto', \"'\", 'top', 'wilt', 'plenty', 'call', 'ye', 'whereby', 'he', 'out', 'hence', 'outside', 'must', 'from', 'wheresoever', 'wow', 'theirs', 'had', 'whereinto', 'cannot', 'it', 'unless', 'till', 'except', 'nevertheless']\n"
     ]
    }
   ],
   "source": [
    "# COMMON STOPWORDS\n",
    "from sklearn.feature_extraction import _stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(_stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('data/lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"i\", \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4652e9",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 41) (208, 54) (165, 3)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/df_train.pkl'\n",
    "df_train = pd.read_pickle(file1)\n",
    "\n",
    "file2    = 'data/df_dev.pkl'\n",
    "df_dev   = pd.read_pickle(file2)\n",
    "\n",
    "file3    = 'data/df_augmented.pkl'\n",
    "df_aug   = pd.read_pickle(file3)\n",
    "df_aug['emotion']        = df_aug['emotion'].apply( lambda x: [x] if isinstance(x,str) else x)\n",
    "df_aug['target_encoded'] = df_aug['emotion'].apply( get_target )\n",
    "\n",
    "print(df_train.shape, df_dev.shape, df_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d746488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The rising number of drug addiction cases is a...</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The single mother worked multiple jobs to prov...</td>\n",
       "      <td>[Joy]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just came across an article about a recent o...</td>\n",
       "      <td>[Fear]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She was diagnosed with a severe illness, but s...</td>\n",
       "      <td>[Joy]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay     emotion  \\\n",
       "0  The rising number of drug addiction cases is a...      [Hope]   \n",
       "1  I'm flabbergasted by this new medical treatmen...  [Surprise]   \n",
       "2  The single mother worked multiple jobs to prov...       [Joy]   \n",
       "3  I just came across an article about a recent o...      [Fear]   \n",
       "4  She was diagnosed with a severe illness, but s...       [Joy]   \n",
       "\n",
       "             target_encoded  \n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dc8d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare additional text columns (tsk = title, summary, keywords)\n",
    "df_train['essay_clean_spellchecked_tsk'] = df_train['gpt35_title'] + '. ' + df_train['gpt35_summary'] + ' ' +\\\n",
    "                                           df_train['gpt35_keywords'] + ' ' +\\\n",
    "                                           df_train['essay_clean_spellchecked']\n",
    "df_train['essay_clean_spellchecked_ts']  = df_train['gpt35_title'] + '. ' + df_train['gpt35_summary'] + ' ' +\\\n",
    "                                           df_train['essay_clean_spellchecked']\n",
    "df_train['title_summary_keywords']       = df_train['gpt35_title'] + '. ' + df_train['gpt35_summary'] + ' ' +\\\n",
    "                                           df_train['gpt35_keywords']\n",
    "df_train['title_summary']                = df_train['gpt35_title'] + '. ' + df_train['gpt35_summary']\n",
    "\n",
    "\n",
    "\n",
    "df_dev['essay_clean_spellchecked_tsk'] = df_dev['gpt35_title'] + '. ' + df_dev['gpt35_summary'] + ' ' +\\\n",
    "                                         df_dev['gpt35_keywords'] + ' ' +\\\n",
    "                                         df_dev['essay_clean_spellchecked']\n",
    "df_dev['essay_clean_spellchecked_ts']  = df_dev['gpt35_title'] + '. ' + df_dev['gpt35_summary'] + ' ' +\\\n",
    "                                         df_dev['essay_clean_spellchecked']\n",
    "df_dev['title_summary_keywords']       = df_dev['gpt35_title'] + '. ' + df_dev['gpt35_summary'] + ' ' +\\\n",
    "                                         df_dev['gpt35_keywords']\n",
    "df_dev['title_summary']                = df_dev['gpt35_title'] + '. ' + df_dev['gpt35_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c8de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.\n",
      "The Heartbreaking Reality of Natural Disasters. The author expresses sadness at the living conditions of people affected by a storm and hopes that aid reaches those who need it the most. They acknowledge the difficulty of coping with such a situation and empathize with those who are struggling. heartbreak, people, living conditions, aid, island, needy, family, depression, hopelessness, storm, meal. It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.\n",
      "The Heartbreaking Reality of Natural Disasters. The author expresses sadness at the living conditions of people affected by a storm and hopes that aid reaches those who need it the most. They acknowledge the difficulty of coping with such a situation and empathize with those who are struggling. It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.\n",
      "The Heartbreaking Reality of Natural Disasters. The author expresses sadness at the living conditions of people affected by a storm and hopes that aid reaches those who need it the most. They acknowledge the difficulty of coping with such a situation and empathize with those who are struggling. heartbreak, people, living conditions, aid, island, needy, family, depression, hopelessness, storm, meal.\n",
      "The Heartbreaking Reality of Natural Disasters. The author expresses sadness at the living conditions of people affected by a storm and hopes that aid reaches those who need it the most. They acknowledge the difficulty of coping with such a situation and empathize with those who are struggling.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\n",
      "Finding Solutions to Help Haiti's Hungry and Thirsty Population. The author wonders why more people aren't helping those in Haiti, despite the country's poverty and corruption. They acknowledge that looting of supplies is understandable due to hunger, but suggest finding alternative ways to distribute food and water. people, help, Haiti, country, supplies, looted, crowds, hungry, food, water, survive, distribute I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\n",
      "Finding Solutions to Help Haiti's Hungry and Thirsty Population. The author wonders why more people aren't helping those in Haiti, despite the country's poverty and corruption. They acknowledge that looting of supplies is understandable due to hunger, but suggest finding alternative ways to distribute food and water. I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\n",
      "Finding Solutions to Help Haiti's Hungry and Thirsty Population. The author wonders why more people aren't helping those in Haiti, despite the country's poverty and corruption. They acknowledge that looting of supplies is understandable due to hunger, but suggest finding alternative ways to distribute food and water. people, help, Haiti, country, supplies, looted, crowds, hungry, food, water, survive, distribute\n",
      "Finding Solutions to Help Haiti's Hungry and Thirsty Population. The author wonders why more people aren't helping those in Haiti, despite the country's poverty and corruption. They acknowledge that looting of supplies is understandable due to hunger, but suggest finding alternative ways to distribute food and water.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "After reading the article, you can't help but feel really sad and terrible for the people that were affected by the hurricane. It was a situation that they did not deserve and one that they most likely did not cause but mother nature has other plans for us. I feel bad for all the children as well as animals that are there as well with no shelter or food.\n",
      "The Devastating Impact of Hurricane on People and Animals. The text expresses sympathy for the people and animals affected by a hurricane, acknowledging that it was a situation they did not deserve and likely did not cause. The author specifically mentions feeling bad for the children and animals without shelter or food. article, sad, terrible, people, affected, hurricane, situation, mother nature, children, animals, shelter, food. After reading the article, you can't help but feel really sad and terrible for the people that were affected by the hurricane. It was a situation that they did not deserve and one that they most likely did not cause but mother nature has other plans for us. I feel bad for all the children as well as animals that are there as well with no shelter or food.\n",
      "The Devastating Impact of Hurricane on People and Animals. The text expresses sympathy for the people and animals affected by a hurricane, acknowledging that it was a situation they did not deserve and likely did not cause. The author specifically mentions feeling bad for the children and animals without shelter or food. After reading the article, you can't help but feel really sad and terrible for the people that were affected by the hurricane. It was a situation that they did not deserve and one that they most likely did not cause but mother nature has other plans for us. I feel bad for all the children as well as animals that are there as well with no shelter or food.\n",
      "The Devastating Impact of Hurricane on People and Animals. The text expresses sympathy for the people and animals affected by a hurricane, acknowledging that it was a situation they did not deserve and likely did not cause. The author specifically mentions feeling bad for the children and animals without shelter or food. article, sad, terrible, people, affected, hurricane, situation, mother nature, children, animals, shelter, food.\n",
      "The Devastating Impact of Hurricane on People and Animals. The text expresses sympathy for the people and animals affected by a hurricane, acknowledging that it was a situation they did not deserve and likely did not cause. The author specifically mentions feeling bad for the children and animals without shelter or food.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "It is so sad that someone who had such an amazing story died in such a freak accident. His life was filled with amazing triumphs only for him to die in such a sad way. It is truly heart breaking to think about. He came from nothing and truly got the american dream. He died in such a rare and crazy way that it is so sad.\n",
      "Tragic Death of a Triumphed Life. The text expresses sadness over the death of someone with an amazing life story who achieved the American dream, only to die in a rare and tragic accident. sad, amazing story, died, freak accident, life, triumphs, heart breaking, nothing, american dream, rare, crazy way. It is so sad that someone who had such an amazing story died in such a freak accident. His life was filled with amazing triumphs only for him to die in such a sad way. It is truly heart breaking to think about. He came from nothing and truly got the american dream. He died in such a rare and crazy way that it is so sad.\n",
      "Tragic Death of a Triumphed Life. The text expresses sadness over the death of someone with an amazing life story who achieved the American dream, only to die in a rare and tragic accident. It is so sad that someone who had such an amazing story died in such a freak accident. His life was filled with amazing triumphs only for him to die in such a sad way. It is truly heart breaking to think about. He came from nothing and truly got the american dream. He died in such a rare and crazy way that it is so sad.\n",
      "Tragic Death of a Triumphed Life. The text expresses sadness over the death of someone with an amazing life story who achieved the American dream, only to die in a rare and tragic accident. sad, amazing story, died, freak accident, life, triumphs, heart breaking, nothing, american dream, rare, crazy way.\n",
      "Tragic Death of a Triumphed Life. The text expresses sadness over the death of someone with an amazing life story who achieved the American dream, only to die in a rare and tragic accident.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "From reading the article, it looks like the world lost a kindhearted and generous person. If no drugs or alcohol were involved in the accident. I wonder what happen to make them crash. I wonder if it was common to be on the boat with no life jacket. The life jacket may not even mattered because of the speed and the rocks.\n",
      "Speculating on the Tragic Accident of a Kindhearted Person. The article discusses the loss of a kind and generous person and questions the cause of the accident, including whether drugs or alcohol were involved and if it was common to be on the boat without a life jacket. The effectiveness of a life jacket is also questioned due to the speed and rocks involved in the crash. world, kindhearted, generous, drugs, alcohol, accident, crash, boat, life jacket, speed, rocks. From reading the article, it looks like the world lost a kindhearted and generous person. If no drugs or alcohol were involved in the accident. I wonder what happen to make them crash. I wonder if it was common to be on the boat with no life jacket. The life jacket may not even mattered because of the speed and the rocks.\n",
      "Speculating on the Tragic Accident of a Kindhearted Person. The article discusses the loss of a kind and generous person and questions the cause of the accident, including whether drugs or alcohol were involved and if it was common to be on the boat without a life jacket. The effectiveness of a life jacket is also questioned due to the speed and rocks involved in the crash. From reading the article, it looks like the world lost a kindhearted and generous person. If no drugs or alcohol were involved in the accident. I wonder what happen to make them crash. I wonder if it was common to be on the boat with no life jacket. The life jacket may not even mattered because of the speed and the rocks.\n",
      "Speculating on the Tragic Accident of a Kindhearted Person. The article discusses the loss of a kind and generous person and questions the cause of the accident, including whether drugs or alcohol were involved and if it was common to be on the boat without a life jacket. The effectiveness of a life jacket is also questioned due to the speed and rocks involved in the crash. world, kindhearted, generous, drugs, alcohol, accident, crash, boat, life jacket, speed, rocks.\n",
      "Speculating on the Tragic Accident of a Kindhearted Person. The article discusses the loss of a kind and generous person and questions the cause of the accident, including whether drugs or alcohol were involved and if it was common to be on the boat without a life jacket. The effectiveness of a life jacket is also questioned due to the speed and rocks involved in the crash.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "That's sad. Regardless of what they find out happened, who was controlling what or if they had drugs in their system or whatever, it's sad. I don't know that they will find out anything, i just feel like lots of people will turn this into something it's not. It's unfortunate anytime a young person like this, with the world at their fingertips, loses their life in something that was controllable.\n",
      "The Tragic Loss of a Young Life. The author expresses sadness over the death of a young person and doubts that the investigation will reveal anything significant. They believe that people may try to make more of the situation than it actually is. The author finds it unfortunate that a young person with potential lost their life in a preventable situation. sad, find out, happened, controlling, drugs, system, people, unfortunate, young person, world, fingertips, loses life, controllable. That's sad. Regardless of what they find out happened, who was controlling what or if they had drugs in their system or whatever, it's sad. I don't know that they will find out anything, i just feel like lots of people will turn this into something it's not. It's unfortunate anytime a young person like this, with the world at their fingertips, loses their life in something that was controllable.\n",
      "The Tragic Loss of a Young Life. The author expresses sadness over the death of a young person and doubts that the investigation will reveal anything significant. They believe that people may try to make more of the situation than it actually is. The author finds it unfortunate that a young person with potential lost their life in a preventable situation. That's sad. Regardless of what they find out happened, who was controlling what or if they had drugs in their system or whatever, it's sad. I don't know that they will find out anything, i just feel like lots of people will turn this into something it's not. It's unfortunate anytime a young person like this, with the world at their fingertips, loses their life in something that was controllable.\n",
      "The Tragic Loss of a Young Life. The author expresses sadness over the death of a young person and doubts that the investigation will reveal anything significant. They believe that people may try to make more of the situation than it actually is. The author finds it unfortunate that a young person with potential lost their life in a preventable situation. sad, find out, happened, controlling, drugs, system, people, unfortunate, young person, world, fingertips, loses life, controllable.\n",
      "The Tragic Loss of a Young Life. The author expresses sadness over the death of a young person and doubts that the investigation will reveal anything significant. They believe that people may try to make more of the situation than it actually is. The author finds it unfortunate that a young person with potential lost their life in a preventable situation.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "After reading the article, my reaction is that it is very sad that boys that young have to be put behind bars. I think that children should be able to experience their childhood and have fun at that age. They should not be facing hardships at all. They should be playing with friends and be in school at that age and not locked up behind a cell.\n",
      "The Sad Reality of Children Behind Bars. The text expresses sadness that young boys have to be put behind bars and believes that children should be able to enjoy their childhood without facing hardships or being locked up. boys, young, put behind bars, children, experience, childhood, fun, age, facing hardships, playing with friends, school, locked up, cell. After reading the article, my reaction is that it is very sad that boys that young have to be put behind bars. I think that children should be able to experience their childhood and have fun at that age. They should not be facing hardships at all. They should be playing with friends and be in school at that age and not locked up behind a cell.\n",
      "The Sad Reality of Children Behind Bars. The text expresses sadness that young boys have to be put behind bars and believes that children should be able to enjoy their childhood without facing hardships or being locked up. After reading the article, my reaction is that it is very sad that boys that young have to be put behind bars. I think that children should be able to experience their childhood and have fun at that age. They should not be facing hardships at all. They should be playing with friends and be in school at that age and not locked up behind a cell.\n",
      "The Sad Reality of Children Behind Bars. The text expresses sadness that young boys have to be put behind bars and believes that children should be able to enjoy their childhood without facing hardships or being locked up. boys, young, put behind bars, children, experience, childhood, fun, age, facing hardships, playing with friends, school, locked up, cell.\n",
      "The Sad Reality of Children Behind Bars. The text expresses sadness that young boys have to be put behind bars and believes that children should be able to enjoy their childhood without facing hardships or being locked up.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "It sounds like these boys had a really rough life. I do think we all have personal responsibility for our choices at the end of the day though. Even though you might have it rough, ultimately it is up to you to decide to break the law or use drugs or not. So they had free will too and could have exercised that. Regardless, it is still sad that they went through a rough childhood. Nobody should have to endure that and kids are the saddest victims.\n",
      "The Responsibility of Personal Choices Despite a Rough Childhood. The text discusses the idea of personal responsibility and free will in relation to individuals who have had a rough life. While acknowledging the sad reality of a difficult childhood, the author emphasizes the importance of making responsible choices. boys, rough life, personal responsibility, choices, free will, break the law, use drugs, childhood, sad, endure, kids, victims. It sounds like these boys had a really rough life. I do think we all have personal responsibility for our choices at the end of the day though. Even though you might have it rough, ultimately it is up to you to decide to break the law or use drugs or not. So they had free will too and could have exercised that. Regardless, it is still sad that they went through a rough childhood. Nobody should have to endure that and kids are the saddest victims.\n",
      "The Responsibility of Personal Choices Despite a Rough Childhood. The text discusses the idea of personal responsibility and free will in relation to individuals who have had a rough life. While acknowledging the sad reality of a difficult childhood, the author emphasizes the importance of making responsible choices. It sounds like these boys had a really rough life. I do think we all have personal responsibility for our choices at the end of the day though. Even though you might have it rough, ultimately it is up to you to decide to break the law or use drugs or not. So they had free will too and could have exercised that. Regardless, it is still sad that they went through a rough childhood. Nobody should have to endure that and kids are the saddest victims.\n",
      "The Responsibility of Personal Choices Despite a Rough Childhood. The text discusses the idea of personal responsibility and free will in relation to individuals who have had a rough life. While acknowledging the sad reality of a difficult childhood, the author emphasizes the importance of making responsible choices. boys, rough life, personal responsibility, choices, free will, break the law, use drugs, childhood, sad, endure, kids, victims.\n",
      "The Responsibility of Personal Choices Despite a Rough Childhood. The text discusses the idea of personal responsibility and free will in relation to individuals who have had a rough life. While acknowledging the sad reality of a difficult childhood, the author emphasizes the importance of making responsible choices.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "This is a tragic and sad story about how some children can experience the foster care system. Shelton bounced from one home to another, getting into trouble along the way, before beginning a life of crime and going to prison as a young adult and then as an adult. Given the way he was raised, it is almost impossible to imagine an alternate ending.\n",
      "The Tragic Journey of a Foster Child. The text is a sad story about a child named Shelton who experienced the foster care system, bounced from one home to another, got into trouble, and eventually ended up in a life of crime and prison. The author suggests that given his upbringing, it was unlikely for Shelton to have a different outcome. foster care system, children, Shelton, trouble, life of crime, prison, young adult, adult, raised, alternate ending. This is a tragic and sad story about how some children can experience the foster care system. Shelton bounced from one home to another, getting into trouble along the way, before beginning a life of crime and going to prison as a young adult and then as an adult. Given the way he was raised, it is almost impossible to imagine an alternate ending.\n",
      "The Tragic Journey of a Foster Child. The text is a sad story about a child named Shelton who experienced the foster care system, bounced from one home to another, got into trouble, and eventually ended up in a life of crime and prison. The author suggests that given his upbringing, it was unlikely for Shelton to have a different outcome. This is a tragic and sad story about how some children can experience the foster care system. Shelton bounced from one home to another, getting into trouble along the way, before beginning a life of crime and going to prison as a young adult and then as an adult. Given the way he was raised, it is almost impossible to imagine an alternate ending.\n",
      "The Tragic Journey of a Foster Child. The text is a sad story about a child named Shelton who experienced the foster care system, bounced from one home to another, got into trouble, and eventually ended up in a life of crime and prison. The author suggests that given his upbringing, it was unlikely for Shelton to have a different outcome. foster care system, children, Shelton, trouble, life of crime, prison, young adult, adult, raised, alternate ending.\n",
      "The Tragic Journey of a Foster Child. The text is a sad story about a child named Shelton who experienced the foster care system, bounced from one home to another, got into trouble, and eventually ended up in a life of crime and prison. The author suggests that given his upbringing, it was unlikely for Shelton to have a different outcome.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "Hello. I feel really terrible about the current coal mining situation in India. Many lives have been lost, properties destroyed, so many losses for the people of that country. It's really pathetic how the government has failed in their duty to protect the citizens. It's even more worrisome that they have also failed to compensate the victims. It's a pity\n",
      "The Tragic Consequences of Failed Government Protection in Indian Coal Mining. The author expresses sadness and concern about the coal mining situation in India, where many lives have been lost and properties destroyed. They criticize the government for failing to protect citizens and compensate victims. coal mining, India, lives lost, properties destroyed, losses, government, duty, protect citizens, compensate victims, pity. Hello. I feel really terrible about the current coal mining situation in India. Many lives have been lost, properties destroyed, so many losses for the people of that country. It's really pathetic how the government has failed in their duty to protect the citizens. It's even more worrisome that they have also failed to compensate the victims. It's a pity\n",
      "The Tragic Consequences of Failed Government Protection in Indian Coal Mining. The author expresses sadness and concern about the coal mining situation in India, where many lives have been lost and properties destroyed. They criticize the government for failing to protect citizens and compensate victims. Hello. I feel really terrible about the current coal mining situation in India. Many lives have been lost, properties destroyed, so many losses for the people of that country. It's really pathetic how the government has failed in their duty to protect the citizens. It's even more worrisome that they have also failed to compensate the victims. It's a pity\n",
      "The Tragic Consequences of Failed Government Protection in Indian Coal Mining. The author expresses sadness and concern about the coal mining situation in India, where many lives have been lost and properties destroyed. They criticize the government for failing to protect citizens and compensate victims. coal mining, India, lives lost, properties destroyed, losses, government, duty, protect citizens, compensate victims, pity.\n",
      "The Tragic Consequences of Failed Government Protection in Indian Coal Mining. The author expresses sadness and concern about the coal mining situation in India, where many lives have been lost and properties destroyed. They criticize the government for failing to protect citizens and compensate victims.\n",
      "\n",
      " ============================================================================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify prepared text columns\n",
    "temp = df_train[['essay_clean_spellchecked', 'essay_clean_spellchecked_tsk',\n",
    "       'essay_clean_spellchecked_ts', 'title_summary_keywords',\n",
    "       'title_summary']]\n",
    "for a, b, c, d, e in temp.values[:10]:\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(e)\n",
    "    print('\\n', '='*77, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc06cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 45)\n",
      "(611, 45)\n"
     ]
    }
   ],
   "source": [
    "# downsample sadness to neutral\n",
    "to_drop     = 181\n",
    "emotion_col = 'emotion_no_2nd_neut'\n",
    "mask = df_train[emotion_col].apply( lambda x: 'Sadness' in x and 'Joy' not in x and 'Surprise' not in x and\\\n",
    "                                              'Fear' not in x and 'Hope' not in x)\n",
    "idx_to_drop = df_train[mask].sample(n=to_drop, random_state=random_state).index\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train = df_train.drop( idx_to_drop )\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be3d988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column: essay_clean\n"
     ]
    }
   ],
   "source": [
    "candidate_cols = [ 'essay_clean', 'essay_clean_spellchecked_tsk',\n",
    "                   'essay_clean_spellchecked_ts', 'title_summary_keywords',\n",
    "                   'title_summary', ]\n",
    "text_col    = candidate_cols[0]\n",
    "print('Text column:', text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0153532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled column names: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target_encoded</th>\n",
       "      <th>essay_clean</th>\n",
       "      <th>essay_clean_spellchecked</th>\n",
       "      <th>emotion_no_2nd_neut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The rising number of drug addiction cases is a...</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>The rising number of drug addiction cases is a...</td>\n",
       "      <td>The rising number of drug addiction cases is a...</td>\n",
       "      <td>[Hope]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>[Surprise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The single mother worked multiple jobs to prov...</td>\n",
       "      <td>[Joy]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>The single mother worked multiple jobs to prov...</td>\n",
       "      <td>The single mother worked multiple jobs to prov...</td>\n",
       "      <td>[Joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just came across an article about a recent o...</td>\n",
       "      <td>[Fear]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>I just came across an article about a recent o...</td>\n",
       "      <td>I just came across an article about a recent o...</td>\n",
       "      <td>[Fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She was diagnosed with a severe illness, but s...</td>\n",
       "      <td>[Joy]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>She was diagnosed with a severe illness, but s...</td>\n",
       "      <td>She was diagnosed with a severe illness, but s...</td>\n",
       "      <td>[Joy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay     emotion  \\\n",
       "0  The rising number of drug addiction cases is a...      [Hope]   \n",
       "1  I'm flabbergasted by this new medical treatmen...  [Surprise]   \n",
       "2  The single mother worked multiple jobs to prov...       [Joy]   \n",
       "3  I just came across an article about a recent o...      [Fear]   \n",
       "4  She was diagnosed with a severe illness, but s...       [Joy]   \n",
       "\n",
       "             target_encoded  \\\n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0]   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 1, 0, 0, 0]   \n",
       "\n",
       "                                         essay_clean  \\\n",
       "0  The rising number of drug addiction cases is a...   \n",
       "1  I'm flabbergasted by this new medical treatmen...   \n",
       "2  The single mother worked multiple jobs to prov...   \n",
       "3  I just came across an article about a recent o...   \n",
       "4  She was diagnosed with a severe illness, but s...   \n",
       "\n",
       "                            essay_clean_spellchecked emotion_no_2nd_neut  \n",
       "0  The rising number of drug addiction cases is a...              [Hope]  \n",
       "1  I'm flabbergasted by this new medical treatmen...          [Surprise]  \n",
       "2  The single mother worked multiple jobs to prov...               [Joy]  \n",
       "3  I just came across an article about a recent o...              [Fear]  \n",
       "4  She was diagnosed with a severe illness, but s...               [Joy]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add augmented data\n",
    "df_aug['essay_clean']              = df_aug['essay'].values\n",
    "df_aug['essay_clean_spellchecked'] = df_aug['essay'].values\n",
    "df_aug['emotion_no_2nd_neut']      = df_aug['emotion'].values\n",
    "print('Misspelled column names:', [c for c in df_aug.columns if c not in df_train.columns])\n",
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0530e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before concatenation: (611, 45)\n",
      "After concatenation: (776, 45)\n",
      "article_id                       165\n",
      "conversation_id                  165\n",
      "speaker_number                   165\n",
      "essay_id                         165\n",
      "speaker_id                       165\n",
      "essay                              0\n",
      "essay_clean                        0\n",
      "split                            165\n",
      "gender                           165\n",
      "education                        165\n",
      "race                             165\n",
      "age                              165\n",
      "income                           165\n",
      "emotion                            0\n",
      "target_encoded                     0\n",
      "emotion_count                    165\n",
      "article                          165\n",
      "article_clean                    165\n",
      "essay_clean_spellchecked           0\n",
      "article_clean_spellchecked       165\n",
      "gpt_embedding                    165\n",
      "emotion_no_2nd_neut                0\n",
      "gpt35_keywords                   165\n",
      "gpt35_title                      165\n",
      "gpt35_summary                    165\n",
      "gpt4_summary                     165\n",
      "gpt4_title                       165\n",
      "gpt4_keywords                    165\n",
      "empathy                          165\n",
      "distress                         165\n",
      "personality_conscientiousness    165\n",
      "personality_openess              165\n",
      "personality_extraversion         165\n",
      "personality_agreeableness        165\n",
      "personality_stability            165\n",
      "iri_perspective_taking           165\n",
      "iri_personal_distress            165\n",
      "iri_fantasy                      165\n",
      "iri_empathatic_concern           165\n",
      "empathy_label                    165\n",
      "distress_label                   165\n",
      "essay_clean_spellchecked_tsk     165\n",
      "essay_clean_spellchecked_ts      165\n",
      "title_summary_keywords           165\n",
      "title_summary                    165\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>speaker_number</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>essay_clean</th>\n",
       "      <th>split</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>emotion</th>\n",
       "      <th>target_encoded</th>\n",
       "      <th>emotion_count</th>\n",
       "      <th>article</th>\n",
       "      <th>article_clean</th>\n",
       "      <th>essay_clean_spellchecked</th>\n",
       "      <th>article_clean_spellchecked</th>\n",
       "      <th>gpt_embedding</th>\n",
       "      <th>emotion_no_2nd_neut</th>\n",
       "      <th>gpt35_keywords</th>\n",
       "      <th>gpt35_title</th>\n",
       "      <th>gpt35_summary</th>\n",
       "      <th>gpt4_summary</th>\n",
       "      <th>gpt4_title</th>\n",
       "      <th>gpt4_keywords</th>\n",
       "      <th>empathy</th>\n",
       "      <th>distress</th>\n",
       "      <th>personality_conscientiousness</th>\n",
       "      <th>personality_openess</th>\n",
       "      <th>personality_extraversion</th>\n",
       "      <th>personality_agreeableness</th>\n",
       "      <th>personality_stability</th>\n",
       "      <th>iri_perspective_taking</th>\n",
       "      <th>iri_personal_distress</th>\n",
       "      <th>iri_fantasy</th>\n",
       "      <th>iri_empathatic_concern</th>\n",
       "      <th>empathy_label</th>\n",
       "      <th>distress_label</th>\n",
       "      <th>essay_clean_spellchecked_tsk</th>\n",
       "      <th>essay_clean_spellchecked_ts</th>\n",
       "      <th>title_summary_keywords</th>\n",
       "      <th>title_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>339.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Why is the rest of the world just sitting arou...</td>\n",
       "      <td>Why is the rest of the world just sitting arou...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>35000</td>\n",
       "      <td>[Disgust]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The treatment of Calais camp children isn’t ju...</td>\n",
       "      <td>The treatment of Calais camp children isn't ju...</td>\n",
       "      <td>Why is the rest of the world just sitting arou...</td>\n",
       "      <td>The treatment of Calais camp children isn't ju...</td>\n",
       "      <td>[-0.00291491043753922, -0.019601348787546158, ...</td>\n",
       "      <td>[Disgust]</td>\n",
       "      <td>rest of the world, sitting around, watching, d...</td>\n",
       "      <td>The World's Responsibility to Protect Abused C...</td>\n",
       "      <td>The author is expressing disgust at the fact t...</td>\n",
       "      <td>The text expresses frustration at the inaction...</td>\n",
       "      <td>Global Inaction on Child Abuse: A Call for Col...</td>\n",
       "      <td>rest of the world, sitting around, watching, d...</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.714</td>\n",
       "      <td>132</td>\n",
       "      <td>140</td>\n",
       "      <td>The World's Responsibility to Protect Abused C...</td>\n",
       "      <td>The World's Responsibility to Protect Abused C...</td>\n",
       "      <td>The World's Responsibility to Protect Abused C...</td>\n",
       "      <td>The World's Responsibility to Protect Abused C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>313.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>I feel the asylum process is very, very bad in...</td>\n",
       "      <td>I feel the asylum process is very, very bad in...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>35000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Syrians and Iraqis granted asylum in Germany f...</td>\n",
       "      <td>Syrians and Iraqis granted asylum in Germany f...</td>\n",
       "      <td>I feel the asylum process is very, very bad in...</td>\n",
       "      <td>Syrians and Iraqis granted asylum in Germany f...</td>\n",
       "      <td>[0.002763778902590275, -0.0021175541914999485,...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>asylum process, bad, countries, Germany, USA, ...</td>\n",
       "      <td>The Need for Improvement in the Asylum Process...</td>\n",
       "      <td>The author believes that the asylum process is...</td>\n",
       "      <td>The asylum process is considered poor in many ...</td>\n",
       "      <td>Inadequacies in the Asylum Process: A Global P...</td>\n",
       "      <td>asylum process, bad, countries, Germany, USA, ...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>6.125</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.714</td>\n",
       "      <td>129</td>\n",
       "      <td>141</td>\n",
       "      <td>The Need for Improvement in the Asylum Process...</td>\n",
       "      <td>The Need for Improvement in the Asylum Process...</td>\n",
       "      <td>The Need for Improvement in the Asylum Process...</td>\n",
       "      <td>The Need for Improvement in the Asylum Process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>395.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>This is so sad and tragic. The most selfish th...</td>\n",
       "      <td>This is so sad and tragic. The most selfish th...</td>\n",
       "      <td>train</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[Anger, Sadness]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wife Who Died Alongside Husband, Children in M...</td>\n",
       "      <td>Wife Who Died Alongside Husband, Children in M...</td>\n",
       "      <td>This is so sad and tragic. The most selfish th...</td>\n",
       "      <td>Wife Who Died Alongside Husband, Children in M...</td>\n",
       "      <td>[-0.01548206340521574, -0.0008526873425580561,...</td>\n",
       "      <td>[Anger, Sadness]</td>\n",
       "      <td>sad, tragic, selfish, innocent lives, own wife...</td>\n",
       "      <td>The Tragic Consequences of Selfishness</td>\n",
       "      <td>The text discusses a tragic event where a man ...</td>\n",
       "      <td>The text expresses sadness and anger over a tr...</td>\n",
       "      <td>\"Tragic Loss of Innocent Lives: Anger and Pity...</td>\n",
       "      <td>sad, tragic, selfish, innocent lives, wife, ki...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>136</td>\n",
       "      <td>148</td>\n",
       "      <td>The Tragic Consequences of Selfishness. The te...</td>\n",
       "      <td>The Tragic Consequences of Selfishness. The te...</td>\n",
       "      <td>The Tragic Consequences of Selfishness. The te...</td>\n",
       "      <td>The Tragic Consequences of Selfishness. The te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>148.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>This is pretty sad for the elephant.  It obvio...</td>\n",
       "      <td>This is pretty sad for the elephant. It obviou...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>42000</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Help Demand That Nosey The Elephant Be Rescued...</td>\n",
       "      <td>Help Demand That Nosey The Elephant Be Rescued...</td>\n",
       "      <td>This is pretty sad for the elephant. It obviou...</td>\n",
       "      <td>Help Demand That Nosey The Elephant Be Rescued...</td>\n",
       "      <td>[-0.016700077801942825, -0.02291671186685562, ...</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>elephant, captivity, circuses, shows, inhumane...</td>\n",
       "      <td>The Inhumane Treatment of Elephants in Circuses</td>\n",
       "      <td>The text expresses sadness for elephants being...</td>\n",
       "      <td>The text expresses sadness for elephants being...</td>\n",
       "      <td>The Inhumane Treatment of Elephants in Circuse...</td>\n",
       "      <td>sad, elephant, no say, matter, not okay, intel...</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.714</td>\n",
       "      <td>2.571</td>\n",
       "      <td>3.857</td>\n",
       "      <td>108</td>\n",
       "      <td>100</td>\n",
       "      <td>The Inhumane Treatment of Elephants in Circuse...</td>\n",
       "      <td>The Inhumane Treatment of Elephants in Circuse...</td>\n",
       "      <td>The Inhumane Treatment of Elephants in Circuse...</td>\n",
       "      <td>The Inhumane Treatment of Elephants in Circuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>92.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>This little boy died in a well while he was he...</td>\n",
       "      <td>This little boy died in a well while he was he...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>29000</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>China: Boy trapped in well found dead after 4 ...</td>\n",
       "      <td>China: Boy trapped in well found dead after 4 ...</td>\n",
       "      <td>This little boy died in a well while he was he...</td>\n",
       "      <td>China: Boy trapped in well found dead after 4 ...</td>\n",
       "      <td>[0.013278329744935036, -0.004298476502299309, ...</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>little boy, died, well, father, harvest vegeta...</td>\n",
       "      <td>Tragic Death of a Little Boy in a Well</td>\n",
       "      <td>A little boy died in a well while helping his ...</td>\n",
       "      <td>A young boy tragically died in a well while he...</td>\n",
       "      <td>Tragic Death of Little Boy in a Well During Ve...</td>\n",
       "      <td>little boy, died, well, helping, father, harve...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>5.000</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.571</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>4.429</td>\n",
       "      <td>134</td>\n",
       "      <td>132</td>\n",
       "      <td>Tragic Death of a Little Boy in a Well. A litt...</td>\n",
       "      <td>Tragic Death of a Little Boy in a Well. A litt...</td>\n",
       "      <td>Tragic Death of a Little Boy in a Well. A litt...</td>\n",
       "      <td>Tragic Death of a Little Boy in a Well. A litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is astonishing. I had no idea that some i...</td>\n",
       "      <td>This is astonishing. I had no idea that some i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is astonishing. I had no idea that some i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>116.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>This is just crazy that people are still dying...</td>\n",
       "      <td>This is just crazy that people are still dying...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>25000</td>\n",
       "      <td>[Anger, Sadness]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Everyone agrees we need to fight cholera. No o...</td>\n",
       "      <td>Everyone agrees we need to fight cholera. No o...</td>\n",
       "      <td>This is just crazy that people are still dying...</td>\n",
       "      <td>Everyone agrees we need to fight cholera. No o...</td>\n",
       "      <td>[0.01276214700192213, 0.007139384746551514, -0...</td>\n",
       "      <td>[Anger, Sadness]</td>\n",
       "      <td>medicine, save lives, epidemics, senseless, di...</td>\n",
       "      <td>The Need for Better Epidemic Preparedness and ...</td>\n",
       "      <td>The author expresses frustration that people a...</td>\n",
       "      <td>The text expresses frustration over people sti...</td>\n",
       "      <td>Inadequate Epidemic Preparedness and the Need ...</td>\n",
       "      <td>crazy, people, dying, medicine, save lives, en...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.286</td>\n",
       "      <td>3.571</td>\n",
       "      <td>3.714</td>\n",
       "      <td>130</td>\n",
       "      <td>140</td>\n",
       "      <td>The Need for Better Epidemic Preparedness and ...</td>\n",
       "      <td>The Need for Better Epidemic Preparedness and ...</td>\n",
       "      <td>The Need for Better Epidemic Preparedness and ...</td>\n",
       "      <td>The Need for Better Epidemic Preparedness and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>66.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>It was a shame that there was a fire but at le...</td>\n",
       "      <td>It was a shame that there was a fire but at le...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>36000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>At least 10 hurt after massive Arizona apartme...</td>\n",
       "      <td>At least 10 hurt after massive Arizona apartme...</td>\n",
       "      <td>It was a shame that there was a fire but at le...</td>\n",
       "      <td>At least 10 hurt after massive Arizona apartme...</td>\n",
       "      <td>[0.000277692946838215, -0.018580766394734383, ...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>fire, firefighters, preventing loss of life, s...</td>\n",
       "      <td>Gratitude for Life-Saving Efforts in a Fire In...</td>\n",
       "      <td>The text discusses a fire that occurred but th...</td>\n",
       "      <td>A fire occurred, but fortunately, there were n...</td>\n",
       "      <td>\"Firefighters Prevent Loss of Life in Tragic B...</td>\n",
       "      <td>shame, fire, did not die, much worse, firefigh...</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.875</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.714</td>\n",
       "      <td>2.857</td>\n",
       "      <td>3.286</td>\n",
       "      <td>2.9285</td>\n",
       "      <td>119</td>\n",
       "      <td>123</td>\n",
       "      <td>Gratitude for Life-Saving Efforts in a Fire In...</td>\n",
       "      <td>Gratitude for Life-Saving Efforts in a Fire In...</td>\n",
       "      <td>Gratitude for Life-Saving Efforts in a Fire In...</td>\n",
       "      <td>Gratitude for Life-Saving Efforts in a Fire In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>242.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>The fact that civilians have to be hurt in the...</td>\n",
       "      <td>The fact that civilians have to be hurt in the...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>35000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Operation killed Afghan civilians, US military...</td>\n",
       "      <td>Operation killed Afghan civilians, US military...</td>\n",
       "      <td>The fact that civilians have to be hurt in the...</td>\n",
       "      <td>Operation killed Afghan civilians, US military...</td>\n",
       "      <td>[-0.01901341788470745, 0.007733711041510105, 0...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>civilians, hurt, process, get rid, Taliban, in...</td>\n",
       "      <td>Minimizing Civilian Casualties in the Fight Ag...</td>\n",
       "      <td>The author is concerned about the harm caused ...</td>\n",
       "      <td>The text expresses concern about civilian harm...</td>\n",
       "      <td>Minimizing Civilian Casualties in the Fight Ag...</td>\n",
       "      <td>civilians, hurt, process, Taliban, mind boggli...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>6.250</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.714</td>\n",
       "      <td>131</td>\n",
       "      <td>142</td>\n",
       "      <td>Minimizing Civilian Casualties in the Fight Ag...</td>\n",
       "      <td>Minimizing Civilian Casualties in the Fight Ag...</td>\n",
       "      <td>Minimizing Civilian Casualties in the Fight Ag...</td>\n",
       "      <td>Minimizing Civilian Casualties in the Fight Ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>113.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>I don't have a major problem with limiting the...</td>\n",
       "      <td>I don't have a major problem with limiting the...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>28000</td>\n",
       "      <td>[Anger]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Environmental health officers call for smoking...</td>\n",
       "      <td>Environmental health officers call for smoking...</td>\n",
       "      <td>I don't have a major problem with limiting the...</td>\n",
       "      <td>Environmental health officers call for smoking...</td>\n",
       "      <td>[0.013939978554844856, -0.012559131719172001, ...</td>\n",
       "      <td>[Anger]</td>\n",
       "      <td>limiting, areas, smoke, children, second-hand ...</td>\n",
       "      <td>The Absurdity of Banning Smoking in Public View</td>\n",
       "      <td>The author disagrees with limiting smoking are...</td>\n",
       "      <td>The author expresses their understanding of li...</td>\n",
       "      <td>\"Opposing Overreach in Public Smoking Restrict...</td>\n",
       "      <td>major problem, limiting areas, people can smok...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.286</td>\n",
       "      <td>4.429</td>\n",
       "      <td>2.714</td>\n",
       "      <td>2.571</td>\n",
       "      <td>100</td>\n",
       "      <td>117</td>\n",
       "      <td>The Absurdity of Banning Smoking in Public Vie...</td>\n",
       "      <td>The Absurdity of Banning Smoking in Public Vie...</td>\n",
       "      <td>The Absurdity of Banning Smoking in Public Vie...</td>\n",
       "      <td>The Absurdity of Banning Smoking in Public Vie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>336.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Hello. I feel really terrible about the curren...</td>\n",
       "      <td>Hello. I feel really terrible about the curren...</td>\n",
       "      <td>train</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[Disgust, Sadness]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The human and animal costs of India's unregula...</td>\n",
       "      <td>The human and animal costs of India's unregula...</td>\n",
       "      <td>Hello. I feel really terrible about the curren...</td>\n",
       "      <td>The human and animal costs of India's unregula...</td>\n",
       "      <td>[-0.007167597766965628, -0.031361475586891174,...</td>\n",
       "      <td>[Disgust, Sadness]</td>\n",
       "      <td>coal mining, India, lives lost, properties des...</td>\n",
       "      <td>The Tragic Consequences of Failed Government P...</td>\n",
       "      <td>The author expresses sadness and concern about...</td>\n",
       "      <td>The text expresses concern over the coal minin...</td>\n",
       "      <td>The Coal Mining Crisis in India: Government's ...</td>\n",
       "      <td>coal mining, India, lives lost, properties des...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>136</td>\n",
       "      <td>100</td>\n",
       "      <td>The Tragic Consequences of Failed Government P...</td>\n",
       "      <td>The Tragic Consequences of Failed Government P...</td>\n",
       "      <td>The Tragic Consequences of Failed Government P...</td>\n",
       "      <td>The Tragic Consequences of Failed Government P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>139.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>This story is sad because of all the frogs bec...</td>\n",
       "      <td>This story is sad because of all the frogs bec...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>110000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Garden ponds 'playing role' in frog disease sp...</td>\n",
       "      <td>Garden ponds 'playing role' in frog disease sp...</td>\n",
       "      <td>This story is sad because of all the frogs bec...</td>\n",
       "      <td>Garden ponds 'playing role' in frog disease sp...</td>\n",
       "      <td>[0.02508121356368065, -0.007409779820591211, 0...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>sad, frogs, sick, spreading, educate, people, ...</td>\n",
       "      <td>The Spread of Frog Disease: A Call for Education</td>\n",
       "      <td>The text discusses the sadness of the spread o...</td>\n",
       "      <td>The text expresses sadness over the rapid spre...</td>\n",
       "      <td>\"Frog Epidemic: Raising Awareness and Preventi...</td>\n",
       "      <td>story, sad, frogs, sick, quickly, spreading, a...</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>3.500</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.143</td>\n",
       "      <td>3.571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135</td>\n",
       "      <td>120</td>\n",
       "      <td>The Spread of Frog Disease: A Call for Educati...</td>\n",
       "      <td>The Spread of Frog Disease: A Call for Educati...</td>\n",
       "      <td>The Spread of Frog Disease: A Call for Educati...</td>\n",
       "      <td>The Spread of Frog Disease: A Call for Educati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>13.0</td>\n",
       "      <td>435.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Troops killed in Afghanistan always angers bec...</td>\n",
       "      <td>Troops killed in Afghanistan always angers bec...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>35000</td>\n",
       "      <td>[Anger]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2 U.S. troops killed fighting Taliban in Afgha...</td>\n",
       "      <td>2 U.S. troops killed fighting Taliban in Afgha...</td>\n",
       "      <td>Troops killed in Afghanistan always angers bec...</td>\n",
       "      <td>2 U.S. troops killed fighting Taliban in Afgha...</td>\n",
       "      <td>[-0.0020872794557362795, -0.023513058200478554...</td>\n",
       "      <td>[Anger]</td>\n",
       "      <td>Troops, killed, Afghanistan, angers, no longer...</td>\n",
       "      <td>The Need to Withdraw Troops from Afghanistan</td>\n",
       "      <td>The author believes that the continued presenc...</td>\n",
       "      <td>The text expresses frustration over troops sti...</td>\n",
       "      <td>The Frustration of Continued Military Presence...</td>\n",
       "      <td>Troops, killed, Afghanistan, angers, no longer...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>5.875</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.714</td>\n",
       "      <td>129</td>\n",
       "      <td>139</td>\n",
       "      <td>The Need to Withdraw Troops from Afghanistan. ...</td>\n",
       "      <td>The Need to Withdraw Troops from Afghanistan. ...</td>\n",
       "      <td>The Need to Withdraw Troops from Afghanistan. ...</td>\n",
       "      <td>The Need to Withdraw Troops from Afghanistan. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent news about the increase in hate cri...</td>\n",
       "      <td>The recent news about the increase in hate cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent news about the increase in hate cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm flabbergasted by this new medical treatmen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I recall watching documentaries about deep-sea...</td>\n",
       "      <td>I recall watching documentaries about deep-sea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I recall watching documentaries about deep-sea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Surprise]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The issue of gender inequality continues to pe...</td>\n",
       "      <td>The issue of gender inequality continues to pe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The issue of gender inequality continues to pe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stories of families being separated by war and...</td>\n",
       "      <td>Stories of families being separated by war and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stories of families being separated by war and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reading about the increasing cyber-attacks is ...</td>\n",
       "      <td>Reading about the increasing cyber-attacks is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fear]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reading about the increasing cyber-attacks is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fear]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>125.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>I guess I just feel bad for these people.  Los...</td>\n",
       "      <td>I guess I just feel bad for these people. Losi...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>82000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Family loses home and two pets in D.C. fire — ...</td>\n",
       "      <td>Family loses home and two pets in D.C. fire — ...</td>\n",
       "      <td>I guess I just feel bad for these people. Losi...</td>\n",
       "      <td>Family loses home and two pets in D.C. fire — ...</td>\n",
       "      <td>[0.0034201634116470814, -0.015805058181285858,...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>people, losing, home, fire, everything, love, ...</td>\n",
       "      <td>Sympathy for Victims of House Fires</td>\n",
       "      <td>The author expresses sympathy for people who l...</td>\n",
       "      <td>The text expresses sympathy for people who los...</td>\n",
       "      <td>Empathy for Fire Victims and the Reality of Fault</td>\n",
       "      <td>feel bad, people, losing, home, fire, suck, lo...</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>1.250</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.571</td>\n",
       "      <td>2.286</td>\n",
       "      <td>1.857</td>\n",
       "      <td>2.0</td>\n",
       "      <td>111</td>\n",
       "      <td>102</td>\n",
       "      <td>Sympathy for Victims of House Fires. The autho...</td>\n",
       "      <td>Sympathy for Victims of House Fires. The autho...</td>\n",
       "      <td>Sympathy for Victims of House Fires. The autho...</td>\n",
       "      <td>Sympathy for Victims of House Fires. The autho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>94.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Its always sad when these things happen becaus...</td>\n",
       "      <td>Its always sad when these things happen becaus...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>35000</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Civilians shot, bodies hung from poles in Mosu...</td>\n",
       "      <td>Civilians shot, bodies hung from poles in Mosu...</td>\n",
       "      <td>Its always sad when these things happen becaus...</td>\n",
       "      <td>Civilians shot, bodies hung from poles in Mosu...</td>\n",
       "      <td>[0.015544404275715351, -0.007654540706425905, ...</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>sad, humans, civilians, innocent, prevent, war...</td>\n",
       "      <td>Preventing Unnecessary Wars and Fighting</td>\n",
       "      <td>The author expresses sadness over the harm cau...</td>\n",
       "      <td>The text expresses sadness over the harm cause...</td>\n",
       "      <td>The Tragedy of Innocent Civilian Casualties an...</td>\n",
       "      <td>sad, humans, civilians, innocent, compete, unn...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>6.125</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.714</td>\n",
       "      <td>129</td>\n",
       "      <td>141</td>\n",
       "      <td>Preventing Unnecessary Wars and Fighting. The ...</td>\n",
       "      <td>Preventing Unnecessary Wars and Fighting. The ...</td>\n",
       "      <td>Preventing Unnecessary Wars and Fighting. The ...</td>\n",
       "      <td>Preventing Unnecessary Wars and Fighting. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>18.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>I think anytime somebody dies because of gunfi...</td>\n",
       "      <td>I think anytime somebody dies because of gunfi...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>35000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3 U.S. military trainers killed in gunfire by ...</td>\n",
       "      <td>3 U.S. military trainers killed in gunfire by ...</td>\n",
       "      <td>I think anytime somebody dies because of gunfi...</td>\n",
       "      <td>3 U.S. military trainers killed in gunfire by ...</td>\n",
       "      <td>[0.002146774437278509, -0.004916753154247999, ...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>gunfire, deaths, security units, gate, shot, m...</td>\n",
       "      <td>Gunfire Deaths at Security Units: Tragic Mista...</td>\n",
       "      <td>The author expresses sadness over any death ca...</td>\n",
       "      <td>The text discusses a tragic incident where peo...</td>\n",
       "      <td>Security Unit Shooting: Tragic Deaths Due to M...</td>\n",
       "      <td>anytime, somebody dies, gunfire, terrible, cas...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.429</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2.714</td>\n",
       "      <td>129</td>\n",
       "      <td>140</td>\n",
       "      <td>Gunfire Deaths at Security Units: Tragic Mista...</td>\n",
       "      <td>Gunfire Deaths at Security Units: Tragic Mista...</td>\n",
       "      <td>Gunfire Deaths at Security Units: Tragic Mista...</td>\n",
       "      <td>Gunfire Deaths at Security Units: Tragic Mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's such a sad reality that so many people in...</td>\n",
       "      <td>It's such a sad reality that so many people in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Joy]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's such a sad reality that so many people in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Joy]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>73.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>As sad as i feel for Billy Bob Thornton and An...</td>\n",
       "      <td>As sad as i feel for Billy Bob Thornton and An...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>85000</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Billy Bob Thornton says he 'never felt good en...</td>\n",
       "      <td>Billy Bob Thornton says he 'never felt good en...</td>\n",
       "      <td>As sad as i feel for Billy Bob Thornton and An...</td>\n",
       "      <td>Billy Bob Thornton says he 'never felt good en...</td>\n",
       "      <td>[-0.003696779254823923, -0.02497788332402706, ...</td>\n",
       "      <td>[Neutral]</td>\n",
       "      <td>Billy Bob Thornton, Angelina Jolie, children, ...</td>\n",
       "      <td>The Privilege of Celebrity Divorce: A Lack of ...</td>\n",
       "      <td>The author expresses little concern for the br...</td>\n",
       "      <td>The text expresses sympathy for Billy Bob Thor...</td>\n",
       "      <td>Impact of Celebrity Divorce on Children</td>\n",
       "      <td>sad, Billy Bob Thornton, Angelina Jolie, bothe...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.643</td>\n",
       "      <td>2.0715</td>\n",
       "      <td>4.143</td>\n",
       "      <td>4.643</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>The Privilege of Celebrity Divorce: A Lack of ...</td>\n",
       "      <td>The Privilege of Celebrity Divorce: A Lack of ...</td>\n",
       "      <td>The Privilege of Celebrity Divorce: A Lack of ...</td>\n",
       "      <td>The Privilege of Celebrity Divorce: A Lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>72.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>it's crazy how elephant poaching is still a th...</td>\n",
       "      <td>it's crazy how elephant poaching is still a th...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>68000</td>\n",
       "      <td>[Anger, Surprise]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bid for strongest protection for all African e...</td>\n",
       "      <td>Bid for strongest protection for all African e...</td>\n",
       "      <td>it's crazy how elephant poaching is still a th...</td>\n",
       "      <td>Bid for strongest protection for all African e...</td>\n",
       "      <td>[-0.016599148511886597, -0.012002266943454742,...</td>\n",
       "      <td>[Anger, Surprise]</td>\n",
       "      <td>elephant poaching, ivory, extinction, stop, cr...</td>\n",
       "      <td>The Urgent Need to Stop Elephant Poaching for ...</td>\n",
       "      <td>The text discusses the issue of elephant poach...</td>\n",
       "      <td>The text expresses disbelief and concern about...</td>\n",
       "      <td>The Ongoing Crisis of Elephant Poaching for Ivory</td>\n",
       "      <td>crazy, elephant poaching, still a thing, nearl...</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.750</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.143</td>\n",
       "      <td>1.714</td>\n",
       "      <td>3.857</td>\n",
       "      <td>2.429</td>\n",
       "      <td>110</td>\n",
       "      <td>106</td>\n",
       "      <td>The Urgent Need to Stop Elephant Poaching for ...</td>\n",
       "      <td>The Urgent Need to Stop Elephant Poaching for ...</td>\n",
       "      <td>The Urgent Need to Stop Elephant Poaching for ...</td>\n",
       "      <td>The Urgent Need to Stop Elephant Poaching for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id  conversation_id  speaker_number  essay_id  speaker_id  \\\n",
       "233       339.0            300.0             1.0     299.0        19.0   \n",
       "140       313.0            193.0             1.0     192.0        19.0   \n",
       "385       395.0            488.0             1.0     487.0        26.0   \n",
       "261       148.0            335.0             1.0     334.0        24.0   \n",
       "713        92.0            402.0             2.0     901.0         8.0   \n",
       "9           NaN              NaN             NaN       NaN         NaN   \n",
       "276       116.0            353.0             1.0     352.0        53.0   \n",
       "306        66.0            388.0             1.0     387.0        33.0   \n",
       "623       242.0            292.0             2.0     791.0        19.0   \n",
       "669       113.0            348.0             2.0     847.0        48.0   \n",
       "9         336.0             17.0             1.0      16.0        31.0   \n",
       "169       139.0            229.0             1.0     228.0        57.0   \n",
       "342        13.0            435.0             1.0     434.0        19.0   \n",
       "6           NaN              NaN             NaN       NaN         NaN   \n",
       "1           NaN              NaN             NaN       NaN         NaN   \n",
       "36          NaN              NaN             NaN       NaN         NaN   \n",
       "145         NaN              NaN             NaN       NaN         NaN   \n",
       "130         NaN              NaN             NaN       NaN         NaN   \n",
       "123         NaN              NaN             NaN       NaN         NaN   \n",
       "108       125.0            154.0             1.0     153.0        49.0   \n",
       "498        94.0            145.0             2.0     644.0        19.0   \n",
       "759        18.0            463.0             2.0     962.0        19.0   \n",
       "10          NaN              NaN             NaN       NaN         NaN   \n",
       "490        73.0            132.0             2.0     631.0        17.0   \n",
       "436        72.0             58.0             2.0     557.0        46.0   \n",
       "\n",
       "                                                 essay  \\\n",
       "233  Why is the rest of the world just sitting arou...   \n",
       "140  I feel the asylum process is very, very bad in...   \n",
       "385  This is so sad and tragic. The most selfish th...   \n",
       "261  This is pretty sad for the elephant.  It obvio...   \n",
       "713  This little boy died in a well while he was he...   \n",
       "9    This is astonishing. I had no idea that some i...   \n",
       "276  This is just crazy that people are still dying...   \n",
       "306  It was a shame that there was a fire but at le...   \n",
       "623  The fact that civilians have to be hurt in the...   \n",
       "669  I don't have a major problem with limiting the...   \n",
       "9    Hello. I feel really terrible about the curren...   \n",
       "169  This story is sad because of all the frogs bec...   \n",
       "342  Troops killed in Afghanistan always angers bec...   \n",
       "6    The recent news about the increase in hate cri...   \n",
       "1    I'm flabbergasted by this new medical treatmen...   \n",
       "36   I recall watching documentaries about deep-sea...   \n",
       "145  The issue of gender inequality continues to pe...   \n",
       "130  Stories of families being separated by war and...   \n",
       "123  Reading about the increasing cyber-attacks is ...   \n",
       "108  I guess I just feel bad for these people.  Los...   \n",
       "498  Its always sad when these things happen becaus...   \n",
       "759  I think anytime somebody dies because of gunfi...   \n",
       "10   It's such a sad reality that so many people in...   \n",
       "490  As sad as i feel for Billy Bob Thornton and An...   \n",
       "436  it's crazy how elephant poaching is still a th...   \n",
       "\n",
       "                                           essay_clean  split   gender  \\\n",
       "233  Why is the rest of the world just sitting arou...  train        1   \n",
       "140  I feel the asylum process is very, very bad in...  train        1   \n",
       "385  This is so sad and tragic. The most selfish th...  train  unknown   \n",
       "261  This is pretty sad for the elephant. It obviou...  train        2   \n",
       "713  This little boy died in a well while he was he...  train        2   \n",
       "9    This is astonishing. I had no idea that some i...    NaN      NaN   \n",
       "276  This is just crazy that people are still dying...  train        2   \n",
       "306  It was a shame that there was a fire but at le...  train        1   \n",
       "623  The fact that civilians have to be hurt in the...  train        1   \n",
       "669  I don't have a major problem with limiting the...  train        1   \n",
       "9    Hello. I feel really terrible about the curren...  train  unknown   \n",
       "169  This story is sad because of all the frogs bec...  train        2   \n",
       "342  Troops killed in Afghanistan always angers bec...  train        1   \n",
       "6    The recent news about the increase in hate cri...    NaN      NaN   \n",
       "1    I'm flabbergasted by this new medical treatmen...    NaN      NaN   \n",
       "36   I recall watching documentaries about deep-sea...    NaN      NaN   \n",
       "145  The issue of gender inequality continues to pe...    NaN      NaN   \n",
       "130  Stories of families being separated by war and...    NaN      NaN   \n",
       "123  Reading about the increasing cyber-attacks is ...    NaN      NaN   \n",
       "108  I guess I just feel bad for these people. Losi...  train        1   \n",
       "498  Its always sad when these things happen becaus...  train        1   \n",
       "759  I think anytime somebody dies because of gunfi...  train        1   \n",
       "10   It's such a sad reality that so many people in...    NaN      NaN   \n",
       "490  As sad as i feel for Billy Bob Thornton and An...  train        1   \n",
       "436  it's crazy how elephant poaching is still a th...  train        1   \n",
       "\n",
       "    education     race      age   income             emotion  \\\n",
       "233         6        2       32    35000           [Disgust]   \n",
       "140         6        2       32    35000           [Neutral]   \n",
       "385   unknown  unknown  unknown  unknown    [Anger, Sadness]   \n",
       "261         7        1       38    42000           [Sadness]   \n",
       "713         6        1       62    29000           [Sadness]   \n",
       "9         NaN      NaN      NaN      NaN          [Surprise]   \n",
       "276         3        1       27    25000    [Anger, Sadness]   \n",
       "306         4        5       33    36000           [Neutral]   \n",
       "623         6        2       32    35000           [Neutral]   \n",
       "669         6        1       41    28000             [Anger]   \n",
       "9     unknown  unknown  unknown  unknown  [Disgust, Sadness]   \n",
       "169         6        1       47   110000           [Neutral]   \n",
       "342         6        2       32    35000             [Anger]   \n",
       "6         NaN      NaN      NaN      NaN              [Hope]   \n",
       "1         NaN      NaN      NaN      NaN          [Surprise]   \n",
       "36        NaN      NaN      NaN      NaN          [Surprise]   \n",
       "145       NaN      NaN      NaN      NaN              [Hope]   \n",
       "130       NaN      NaN      NaN      NaN              [Hope]   \n",
       "123       NaN      NaN      NaN      NaN              [Fear]   \n",
       "108         5        1       31    82000           [Neutral]   \n",
       "498         6        2       32    35000           [Sadness]   \n",
       "759         6        2       32    35000           [Neutral]   \n",
       "10        NaN      NaN      NaN      NaN               [Joy]   \n",
       "490         6        1       29    85000           [Neutral]   \n",
       "436         6        1       34    68000   [Anger, Surprise]   \n",
       "\n",
       "               target_encoded  emotion_count  \\\n",
       "233  [0, 1, 0, 0, 0, 0, 0, 0]            1.0   \n",
       "140  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "385  [1, 0, 0, 0, 0, 0, 1, 0]            2.0   \n",
       "261  [0, 0, 0, 0, 0, 0, 1, 0]            1.0   \n",
       "713  [0, 0, 0, 0, 0, 0, 1, 0]            1.0   \n",
       "9    [0, 0, 0, 0, 0, 0, 0, 1]            NaN   \n",
       "276  [1, 0, 0, 0, 0, 0, 1, 0]            2.0   \n",
       "306  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "623  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "669  [1, 0, 0, 0, 0, 0, 0, 0]            1.0   \n",
       "9    [0, 1, 0, 0, 0, 0, 1, 0]            2.0   \n",
       "169  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "342  [1, 0, 0, 0, 0, 0, 0, 0]            1.0   \n",
       "6    [0, 0, 0, 1, 0, 0, 0, 0]            NaN   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 1]            NaN   \n",
       "36   [0, 0, 0, 0, 0, 0, 0, 1]            NaN   \n",
       "145  [0, 0, 0, 1, 0, 0, 0, 0]            NaN   \n",
       "130  [0, 0, 0, 1, 0, 0, 0, 0]            NaN   \n",
       "123  [0, 0, 1, 0, 0, 0, 0, 0]            NaN   \n",
       "108  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "498  [0, 0, 0, 0, 0, 0, 1, 0]            1.0   \n",
       "759  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "10   [0, 0, 0, 0, 1, 0, 0, 0]            NaN   \n",
       "490  [0, 0, 0, 0, 0, 1, 0, 0]            1.0   \n",
       "436  [1, 0, 0, 0, 0, 0, 0, 1]            2.0   \n",
       "\n",
       "                                               article  \\\n",
       "233  The treatment of Calais camp children isn’t ju...   \n",
       "140  Syrians and Iraqis granted asylum in Germany f...   \n",
       "385  Wife Who Died Alongside Husband, Children in M...   \n",
       "261  Help Demand That Nosey The Elephant Be Rescued...   \n",
       "713  China: Boy trapped in well found dead after 4 ...   \n",
       "9                                                  NaN   \n",
       "276  Everyone agrees we need to fight cholera. No o...   \n",
       "306  At least 10 hurt after massive Arizona apartme...   \n",
       "623  Operation killed Afghan civilians, US military...   \n",
       "669  Environmental health officers call for smoking...   \n",
       "9    The human and animal costs of India's unregula...   \n",
       "169  Garden ponds 'playing role' in frog disease sp...   \n",
       "342  2 U.S. troops killed fighting Taliban in Afgha...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Family loses home and two pets in D.C. fire — ...   \n",
       "498  Civilians shot, bodies hung from poles in Mosu...   \n",
       "759  3 U.S. military trainers killed in gunfire by ...   \n",
       "10                                                 NaN   \n",
       "490  Billy Bob Thornton says he 'never felt good en...   \n",
       "436  Bid for strongest protection for all African e...   \n",
       "\n",
       "                                         article_clean  \\\n",
       "233  The treatment of Calais camp children isn't ju...   \n",
       "140  Syrians and Iraqis granted asylum in Germany f...   \n",
       "385  Wife Who Died Alongside Husband, Children in M...   \n",
       "261  Help Demand That Nosey The Elephant Be Rescued...   \n",
       "713  China: Boy trapped in well found dead after 4 ...   \n",
       "9                                                  NaN   \n",
       "276  Everyone agrees we need to fight cholera. No o...   \n",
       "306  At least 10 hurt after massive Arizona apartme...   \n",
       "623  Operation killed Afghan civilians, US military...   \n",
       "669  Environmental health officers call for smoking...   \n",
       "9    The human and animal costs of India's unregula...   \n",
       "169  Garden ponds 'playing role' in frog disease sp...   \n",
       "342  2 U.S. troops killed fighting Taliban in Afgha...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Family loses home and two pets in D.C. fire — ...   \n",
       "498  Civilians shot, bodies hung from poles in Mosu...   \n",
       "759  3 U.S. military trainers killed in gunfire by ...   \n",
       "10                                                 NaN   \n",
       "490  Billy Bob Thornton says he 'never felt good en...   \n",
       "436  Bid for strongest protection for all African e...   \n",
       "\n",
       "                              essay_clean_spellchecked  \\\n",
       "233  Why is the rest of the world just sitting arou...   \n",
       "140  I feel the asylum process is very, very bad in...   \n",
       "385  This is so sad and tragic. The most selfish th...   \n",
       "261  This is pretty sad for the elephant. It obviou...   \n",
       "713  This little boy died in a well while he was he...   \n",
       "9    This is astonishing. I had no idea that some i...   \n",
       "276  This is just crazy that people are still dying...   \n",
       "306  It was a shame that there was a fire but at le...   \n",
       "623  The fact that civilians have to be hurt in the...   \n",
       "669  I don't have a major problem with limiting the...   \n",
       "9    Hello. I feel really terrible about the curren...   \n",
       "169  This story is sad because of all the frogs bec...   \n",
       "342  Troops killed in Afghanistan always angers bec...   \n",
       "6    The recent news about the increase in hate cri...   \n",
       "1    I'm flabbergasted by this new medical treatmen...   \n",
       "36   I recall watching documentaries about deep-sea...   \n",
       "145  The issue of gender inequality continues to pe...   \n",
       "130  Stories of families being separated by war and...   \n",
       "123  Reading about the increasing cyber-attacks is ...   \n",
       "108  I guess I just feel bad for these people. Losi...   \n",
       "498  Its always sad when these things happen becaus...   \n",
       "759  I think anytime somebody dies because of gunfi...   \n",
       "10   It's such a sad reality that so many people in...   \n",
       "490  As sad as i feel for Billy Bob Thornton and An...   \n",
       "436  it's crazy how elephant poaching is still a th...   \n",
       "\n",
       "                            article_clean_spellchecked  \\\n",
       "233  The treatment of Calais camp children isn't ju...   \n",
       "140  Syrians and Iraqis granted asylum in Germany f...   \n",
       "385  Wife Who Died Alongside Husband, Children in M...   \n",
       "261  Help Demand That Nosey The Elephant Be Rescued...   \n",
       "713  China: Boy trapped in well found dead after 4 ...   \n",
       "9                                                  NaN   \n",
       "276  Everyone agrees we need to fight cholera. No o...   \n",
       "306  At least 10 hurt after massive Arizona apartme...   \n",
       "623  Operation killed Afghan civilians, US military...   \n",
       "669  Environmental health officers call for smoking...   \n",
       "9    The human and animal costs of India's unregula...   \n",
       "169  Garden ponds 'playing role' in frog disease sp...   \n",
       "342  2 U.S. troops killed fighting Taliban in Afgha...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Family loses home and two pets in D.C. fire — ...   \n",
       "498  Civilians shot, bodies hung from poles in Mosu...   \n",
       "759  3 U.S. military trainers killed in gunfire by ...   \n",
       "10                                                 NaN   \n",
       "490  Billy Bob Thornton says he 'never felt good en...   \n",
       "436  Bid for strongest protection for all African e...   \n",
       "\n",
       "                                         gpt_embedding emotion_no_2nd_neut  \\\n",
       "233  [-0.00291491043753922, -0.019601348787546158, ...           [Disgust]   \n",
       "140  [0.002763778902590275, -0.0021175541914999485,...           [Neutral]   \n",
       "385  [-0.01548206340521574, -0.0008526873425580561,...    [Anger, Sadness]   \n",
       "261  [-0.016700077801942825, -0.02291671186685562, ...           [Sadness]   \n",
       "713  [0.013278329744935036, -0.004298476502299309, ...           [Sadness]   \n",
       "9                                                  NaN          [Surprise]   \n",
       "276  [0.01276214700192213, 0.007139384746551514, -0...    [Anger, Sadness]   \n",
       "306  [0.000277692946838215, -0.018580766394734383, ...           [Neutral]   \n",
       "623  [-0.01901341788470745, 0.007733711041510105, 0...           [Neutral]   \n",
       "669  [0.013939978554844856, -0.012559131719172001, ...             [Anger]   \n",
       "9    [-0.007167597766965628, -0.031361475586891174,...  [Disgust, Sadness]   \n",
       "169  [0.02508121356368065, -0.007409779820591211, 0...           [Neutral]   \n",
       "342  [-0.0020872794557362795, -0.023513058200478554...             [Anger]   \n",
       "6                                                  NaN              [Hope]   \n",
       "1                                                  NaN          [Surprise]   \n",
       "36                                                 NaN          [Surprise]   \n",
       "145                                                NaN              [Hope]   \n",
       "130                                                NaN              [Hope]   \n",
       "123                                                NaN              [Fear]   \n",
       "108  [0.0034201634116470814, -0.015805058181285858,...           [Neutral]   \n",
       "498  [0.015544404275715351, -0.007654540706425905, ...           [Sadness]   \n",
       "759  [0.002146774437278509, -0.004916753154247999, ...           [Neutral]   \n",
       "10                                                 NaN               [Joy]   \n",
       "490  [-0.003696779254823923, -0.02497788332402706, ...           [Neutral]   \n",
       "436  [-0.016599148511886597, -0.012002266943454742,...   [Anger, Surprise]   \n",
       "\n",
       "                                        gpt35_keywords  \\\n",
       "233  rest of the world, sitting around, watching, d...   \n",
       "140  asylum process, bad, countries, Germany, USA, ...   \n",
       "385  sad, tragic, selfish, innocent lives, own wife...   \n",
       "261  elephant, captivity, circuses, shows, inhumane...   \n",
       "713  little boy, died, well, father, harvest vegeta...   \n",
       "9                                                  NaN   \n",
       "276  medicine, save lives, epidemics, senseless, di...   \n",
       "306  fire, firefighters, preventing loss of life, s...   \n",
       "623  civilians, hurt, process, get rid, Taliban, in...   \n",
       "669  limiting, areas, smoke, children, second-hand ...   \n",
       "9    coal mining, India, lives lost, properties des...   \n",
       "169  sad, frogs, sick, spreading, educate, people, ...   \n",
       "342  Troops, killed, Afghanistan, angers, no longer...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  people, losing, home, fire, everything, love, ...   \n",
       "498  sad, humans, civilians, innocent, prevent, war...   \n",
       "759  gunfire, deaths, security units, gate, shot, m...   \n",
       "10                                                 NaN   \n",
       "490  Billy Bob Thornton, Angelina Jolie, children, ...   \n",
       "436  elephant poaching, ivory, extinction, stop, cr...   \n",
       "\n",
       "                                           gpt35_title  \\\n",
       "233  The World's Responsibility to Protect Abused C...   \n",
       "140  The Need for Improvement in the Asylum Process...   \n",
       "385             The Tragic Consequences of Selfishness   \n",
       "261    The Inhumane Treatment of Elephants in Circuses   \n",
       "713             Tragic Death of a Little Boy in a Well   \n",
       "9                                                  NaN   \n",
       "276  The Need for Better Epidemic Preparedness and ...   \n",
       "306  Gratitude for Life-Saving Efforts in a Fire In...   \n",
       "623  Minimizing Civilian Casualties in the Fight Ag...   \n",
       "669    The Absurdity of Banning Smoking in Public View   \n",
       "9    The Tragic Consequences of Failed Government P...   \n",
       "169   The Spread of Frog Disease: A Call for Education   \n",
       "342       The Need to Withdraw Troops from Afghanistan   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108                Sympathy for Victims of House Fires   \n",
       "498           Preventing Unnecessary Wars and Fighting   \n",
       "759  Gunfire Deaths at Security Units: Tragic Mista...   \n",
       "10                                                 NaN   \n",
       "490  The Privilege of Celebrity Divorce: A Lack of ...   \n",
       "436  The Urgent Need to Stop Elephant Poaching for ...   \n",
       "\n",
       "                                         gpt35_summary  \\\n",
       "233  The author is expressing disgust at the fact t...   \n",
       "140  The author believes that the asylum process is...   \n",
       "385  The text discusses a tragic event where a man ...   \n",
       "261  The text expresses sadness for elephants being...   \n",
       "713  A little boy died in a well while helping his ...   \n",
       "9                                                  NaN   \n",
       "276  The author expresses frustration that people a...   \n",
       "306  The text discusses a fire that occurred but th...   \n",
       "623  The author is concerned about the harm caused ...   \n",
       "669  The author disagrees with limiting smoking are...   \n",
       "9    The author expresses sadness and concern about...   \n",
       "169  The text discusses the sadness of the spread o...   \n",
       "342  The author believes that the continued presenc...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  The author expresses sympathy for people who l...   \n",
       "498  The author expresses sadness over the harm cau...   \n",
       "759  The author expresses sadness over any death ca...   \n",
       "10                                                 NaN   \n",
       "490  The author expresses little concern for the br...   \n",
       "436  The text discusses the issue of elephant poach...   \n",
       "\n",
       "                                          gpt4_summary  \\\n",
       "233  The text expresses frustration at the inaction...   \n",
       "140  The asylum process is considered poor in many ...   \n",
       "385  The text expresses sadness and anger over a tr...   \n",
       "261  The text expresses sadness for elephants being...   \n",
       "713  A young boy tragically died in a well while he...   \n",
       "9                                                  NaN   \n",
       "276  The text expresses frustration over people sti...   \n",
       "306  A fire occurred, but fortunately, there were n...   \n",
       "623  The text expresses concern about civilian harm...   \n",
       "669  The author expresses their understanding of li...   \n",
       "9    The text expresses concern over the coal minin...   \n",
       "169  The text expresses sadness over the rapid spre...   \n",
       "342  The text expresses frustration over troops sti...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  The text expresses sympathy for people who los...   \n",
       "498  The text expresses sadness over the harm cause...   \n",
       "759  The text discusses a tragic incident where peo...   \n",
       "10                                                 NaN   \n",
       "490  The text expresses sympathy for Billy Bob Thor...   \n",
       "436  The text expresses disbelief and concern about...   \n",
       "\n",
       "                                            gpt4_title  \\\n",
       "233  Global Inaction on Child Abuse: A Call for Col...   \n",
       "140  Inadequacies in the Asylum Process: A Global P...   \n",
       "385  \"Tragic Loss of Innocent Lives: Anger and Pity...   \n",
       "261  The Inhumane Treatment of Elephants in Circuse...   \n",
       "713  Tragic Death of Little Boy in a Well During Ve...   \n",
       "9                                                  NaN   \n",
       "276  Inadequate Epidemic Preparedness and the Need ...   \n",
       "306  \"Firefighters Prevent Loss of Life in Tragic B...   \n",
       "623  Minimizing Civilian Casualties in the Fight Ag...   \n",
       "669  \"Opposing Overreach in Public Smoking Restrict...   \n",
       "9    The Coal Mining Crisis in India: Government's ...   \n",
       "169  \"Frog Epidemic: Raising Awareness and Preventi...   \n",
       "342  The Frustration of Continued Military Presence...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Empathy for Fire Victims and the Reality of Fault   \n",
       "498  The Tragedy of Innocent Civilian Casualties an...   \n",
       "759  Security Unit Shooting: Tragic Deaths Due to M...   \n",
       "10                                                 NaN   \n",
       "490            Impact of Celebrity Divorce on Children   \n",
       "436  The Ongoing Crisis of Elephant Poaching for Ivory   \n",
       "\n",
       "                                         gpt4_keywords   empathy  distress  \\\n",
       "233  rest of the world, sitting around, watching, d...  6.333333     6.000   \n",
       "140  asylum process, bad, countries, Germany, USA, ...  5.833333     6.125   \n",
       "385  sad, tragic, selfish, innocent lives, wife, ki...  7.000000     7.000   \n",
       "261  sad, elephant, no say, matter, not okay, intel...  2.333333     1.000   \n",
       "713  little boy, died, well, helping, father, harve...  6.666667     5.000   \n",
       "9                                                  NaN       NaN       NaN   \n",
       "276  crazy, people, dying, medicine, save lives, en...  6.000000     6.000   \n",
       "306  shame, fire, did not die, much worse, firefigh...  4.166667     3.875   \n",
       "623  civilians, hurt, process, Taliban, mind boggli...  6.166667     6.250   \n",
       "669  major problem, limiting areas, people can smok...  1.000000     3.125   \n",
       "9    coal mining, India, lives lost, properties des...  7.000000     1.000   \n",
       "169  story, sad, frogs, sick, quickly, spreading, a...  6.833333     3.500   \n",
       "342  Troops, killed, Afghanistan, angers, no longer...  5.833333     5.875   \n",
       "6                                                  NaN       NaN       NaN   \n",
       "1                                                  NaN       NaN       NaN   \n",
       "36                                                 NaN       NaN       NaN   \n",
       "145                                                NaN       NaN       NaN   \n",
       "130                                                NaN       NaN       NaN   \n",
       "123                                                NaN       NaN       NaN   \n",
       "108  feel bad, people, losing, home, fire, suck, lo...  2.833333     1.250   \n",
       "498  sad, humans, civilians, innocent, compete, unn...  5.833333     6.125   \n",
       "759  anytime, somebody dies, gunfire, terrible, cas...  5.833333     6.000   \n",
       "10                                                 NaN       NaN       NaN   \n",
       "490  sad, Billy Bob Thornton, Angelina Jolie, bothe...  1.000000     1.000   \n",
       "436  crazy, elephant poaching, still a thing, nearl...  2.666667     1.750   \n",
       "\n",
       "    personality_conscientiousness personality_openess  \\\n",
       "233                           5.5                 5.0   \n",
       "140                           5.5                 5.0   \n",
       "385                       unknown             unknown   \n",
       "261                           7.0                 3.5   \n",
       "713                           6.5                 2.5   \n",
       "9                             NaN                 NaN   \n",
       "276                           6.0                 6.0   \n",
       "306                          4.25                4.25   \n",
       "623                           5.5                 5.0   \n",
       "669                           2.5                 2.0   \n",
       "9                         unknown             unknown   \n",
       "169                           7.0                 6.5   \n",
       "342                           5.5                 5.0   \n",
       "6                             NaN                 NaN   \n",
       "1                             NaN                 NaN   \n",
       "36                            NaN                 NaN   \n",
       "145                           NaN                 NaN   \n",
       "130                           NaN                 NaN   \n",
       "123                           NaN                 NaN   \n",
       "108                           3.5                 2.5   \n",
       "498                           5.5                 5.0   \n",
       "759                           5.5                 5.0   \n",
       "10                            NaN                 NaN   \n",
       "490                          6.75                6.75   \n",
       "436                           5.5                 6.0   \n",
       "\n",
       "    personality_extraversion personality_agreeableness personality_stability  \\\n",
       "233                      2.0                       5.5                   4.5   \n",
       "140                      2.0                       5.5                   4.5   \n",
       "385                  unknown                   unknown               unknown   \n",
       "261                      6.5                       5.5                   6.5   \n",
       "713                      1.0                       6.5                   2.0   \n",
       "9                        NaN                       NaN                   NaN   \n",
       "276                      6.0                       6.0                   6.0   \n",
       "306                      4.5                      4.25                  3.75   \n",
       "623                      2.0                       5.5                   4.5   \n",
       "669                      1.5                       2.0                   1.0   \n",
       "9                    unknown                   unknown               unknown   \n",
       "169                      4.5                       5.5                   7.0   \n",
       "342                      2.0                       5.5                   4.5   \n",
       "6                        NaN                       NaN                   NaN   \n",
       "1                        NaN                       NaN                   NaN   \n",
       "36                       NaN                       NaN                   NaN   \n",
       "145                      NaN                       NaN                   NaN   \n",
       "130                      NaN                       NaN                   NaN   \n",
       "123                      NaN                       NaN                   NaN   \n",
       "108                      2.0                       4.5                   5.0   \n",
       "498                      2.0                       5.5                   4.5   \n",
       "759                      2.0                       5.5                   4.5   \n",
       "10                       NaN                       NaN                   NaN   \n",
       "490                     6.75                      6.75                   7.0   \n",
       "436                      5.5                       5.0                   6.5   \n",
       "\n",
       "    iri_perspective_taking iri_personal_distress iri_fantasy  \\\n",
       "233                  3.429                 2.857       2.857   \n",
       "140                  3.429                 2.857       2.857   \n",
       "385                unknown               unknown     unknown   \n",
       "261                  3.429                 2.714       2.571   \n",
       "713                  3.571                   4.0       2.143   \n",
       "9                      NaN                   NaN         NaN   \n",
       "276                    4.0                 2.286       3.571   \n",
       "306                  2.714                 2.857       3.286   \n",
       "623                  3.429                 2.857       2.857   \n",
       "669                  3.286                 4.429       2.714   \n",
       "9                  unknown               unknown     unknown   \n",
       "169                    5.0                 2.143       3.571   \n",
       "342                  3.429                 2.857       2.857   \n",
       "6                      NaN                   NaN         NaN   \n",
       "1                      NaN                   NaN         NaN   \n",
       "36                     NaN                   NaN         NaN   \n",
       "145                    NaN                   NaN         NaN   \n",
       "130                    NaN                   NaN         NaN   \n",
       "123                    NaN                   NaN         NaN   \n",
       "108                  3.571                 2.286       1.857   \n",
       "498                  3.429                 2.857       2.857   \n",
       "759                  3.429                 2.857       2.857   \n",
       "10                     NaN                   NaN         NaN   \n",
       "490                  4.643                2.0715       4.143   \n",
       "436                  3.143                 1.714       3.857   \n",
       "\n",
       "    iri_empathatic_concern empathy_label distress_label  \\\n",
       "233                  2.714           132            140   \n",
       "140                  2.714           129            141   \n",
       "385                unknown           136            148   \n",
       "261                  3.857           108            100   \n",
       "713                  4.429           134            132   \n",
       "9                      NaN           NaN            NaN   \n",
       "276                  3.714           130            140   \n",
       "306                 2.9285           119            123   \n",
       "623                  2.714           131            142   \n",
       "669                  2.571           100            117   \n",
       "9                  unknown           136            100   \n",
       "169                    5.0           135            120   \n",
       "342                  2.714           129            139   \n",
       "6                      NaN           NaN            NaN   \n",
       "1                      NaN           NaN            NaN   \n",
       "36                     NaN           NaN            NaN   \n",
       "145                    NaN           NaN            NaN   \n",
       "130                    NaN           NaN            NaN   \n",
       "123                    NaN           NaN            NaN   \n",
       "108                    2.0           111            102   \n",
       "498                  2.714           129            141   \n",
       "759                  2.714           129            140   \n",
       "10                     NaN           NaN            NaN   \n",
       "490                  4.643           100            100   \n",
       "436                  2.429           110            106   \n",
       "\n",
       "                          essay_clean_spellchecked_tsk  \\\n",
       "233  The World's Responsibility to Protect Abused C...   \n",
       "140  The Need for Improvement in the Asylum Process...   \n",
       "385  The Tragic Consequences of Selfishness. The te...   \n",
       "261  The Inhumane Treatment of Elephants in Circuse...   \n",
       "713  Tragic Death of a Little Boy in a Well. A litt...   \n",
       "9                                                  NaN   \n",
       "276  The Need for Better Epidemic Preparedness and ...   \n",
       "306  Gratitude for Life-Saving Efforts in a Fire In...   \n",
       "623  Minimizing Civilian Casualties in the Fight Ag...   \n",
       "669  The Absurdity of Banning Smoking in Public Vie...   \n",
       "9    The Tragic Consequences of Failed Government P...   \n",
       "169  The Spread of Frog Disease: A Call for Educati...   \n",
       "342  The Need to Withdraw Troops from Afghanistan. ...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Sympathy for Victims of House Fires. The autho...   \n",
       "498  Preventing Unnecessary Wars and Fighting. The ...   \n",
       "759  Gunfire Deaths at Security Units: Tragic Mista...   \n",
       "10                                                 NaN   \n",
       "490  The Privilege of Celebrity Divorce: A Lack of ...   \n",
       "436  The Urgent Need to Stop Elephant Poaching for ...   \n",
       "\n",
       "                           essay_clean_spellchecked_ts  \\\n",
       "233  The World's Responsibility to Protect Abused C...   \n",
       "140  The Need for Improvement in the Asylum Process...   \n",
       "385  The Tragic Consequences of Selfishness. The te...   \n",
       "261  The Inhumane Treatment of Elephants in Circuse...   \n",
       "713  Tragic Death of a Little Boy in a Well. A litt...   \n",
       "9                                                  NaN   \n",
       "276  The Need for Better Epidemic Preparedness and ...   \n",
       "306  Gratitude for Life-Saving Efforts in a Fire In...   \n",
       "623  Minimizing Civilian Casualties in the Fight Ag...   \n",
       "669  The Absurdity of Banning Smoking in Public Vie...   \n",
       "9    The Tragic Consequences of Failed Government P...   \n",
       "169  The Spread of Frog Disease: A Call for Educati...   \n",
       "342  The Need to Withdraw Troops from Afghanistan. ...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Sympathy for Victims of House Fires. The autho...   \n",
       "498  Preventing Unnecessary Wars and Fighting. The ...   \n",
       "759  Gunfire Deaths at Security Units: Tragic Mista...   \n",
       "10                                                 NaN   \n",
       "490  The Privilege of Celebrity Divorce: A Lack of ...   \n",
       "436  The Urgent Need to Stop Elephant Poaching for ...   \n",
       "\n",
       "                                title_summary_keywords  \\\n",
       "233  The World's Responsibility to Protect Abused C...   \n",
       "140  The Need for Improvement in the Asylum Process...   \n",
       "385  The Tragic Consequences of Selfishness. The te...   \n",
       "261  The Inhumane Treatment of Elephants in Circuse...   \n",
       "713  Tragic Death of a Little Boy in a Well. A litt...   \n",
       "9                                                  NaN   \n",
       "276  The Need for Better Epidemic Preparedness and ...   \n",
       "306  Gratitude for Life-Saving Efforts in a Fire In...   \n",
       "623  Minimizing Civilian Casualties in the Fight Ag...   \n",
       "669  The Absurdity of Banning Smoking in Public Vie...   \n",
       "9    The Tragic Consequences of Failed Government P...   \n",
       "169  The Spread of Frog Disease: A Call for Educati...   \n",
       "342  The Need to Withdraw Troops from Afghanistan. ...   \n",
       "6                                                  NaN   \n",
       "1                                                  NaN   \n",
       "36                                                 NaN   \n",
       "145                                                NaN   \n",
       "130                                                NaN   \n",
       "123                                                NaN   \n",
       "108  Sympathy for Victims of House Fires. The autho...   \n",
       "498  Preventing Unnecessary Wars and Fighting. The ...   \n",
       "759  Gunfire Deaths at Security Units: Tragic Mista...   \n",
       "10                                                 NaN   \n",
       "490  The Privilege of Celebrity Divorce: A Lack of ...   \n",
       "436  The Urgent Need to Stop Elephant Poaching for ...   \n",
       "\n",
       "                                         title_summary  \n",
       "233  The World's Responsibility to Protect Abused C...  \n",
       "140  The Need for Improvement in the Asylum Process...  \n",
       "385  The Tragic Consequences of Selfishness. The te...  \n",
       "261  The Inhumane Treatment of Elephants in Circuse...  \n",
       "713  Tragic Death of a Little Boy in a Well. A litt...  \n",
       "9                                                  NaN  \n",
       "276  The Need for Better Epidemic Preparedness and ...  \n",
       "306  Gratitude for Life-Saving Efforts in a Fire In...  \n",
       "623  Minimizing Civilian Casualties in the Fight Ag...  \n",
       "669  The Absurdity of Banning Smoking in Public Vie...  \n",
       "9    The Tragic Consequences of Failed Government P...  \n",
       "169  The Spread of Frog Disease: A Call for Educati...  \n",
       "342  The Need to Withdraw Troops from Afghanistan. ...  \n",
       "6                                                  NaN  \n",
       "1                                                  NaN  \n",
       "36                                                 NaN  \n",
       "145                                                NaN  \n",
       "130                                                NaN  \n",
       "123                                                NaN  \n",
       "108  Sympathy for Victims of House Fires. The autho...  \n",
       "498  Preventing Unnecessary Wars and Fighting. The ...  \n",
       "759  Gunfire Deaths at Security Units: Tragic Mista...  \n",
       "10                                                 NaN  \n",
       "490  The Privilege of Celebrity Divorce: A Lack of ...  \n",
       "436  The Urgent Need to Stop Elephant Poaching for ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Before concatenation:', df_train.shape)\n",
    "df_train = pd.concat([ df_train.copy(), df_aug.copy() ]).sample(frac=1, random_state=random_state)\n",
    "print('After concatenation:', df_train.shape)\n",
    "print(df_train.isna().sum())\n",
    "df_train.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set b4 upsampling:\n",
      "Neutral     202\n",
      "Sadness     202\n",
      "Anger       118\n",
      "Disgust      92\n",
      "Joy          70\n",
      "Surprise     68\n",
      "Hope         63\n",
      "Fear         58\n",
      "Name: emotion_no_2nd_neut, dtype: int64\n",
      "\n",
      "Train set after upsampling:\n",
      "Hope        202\n",
      "Surprise    202\n",
      "Joy         202\n",
      "Fear        202\n",
      "Sadness     202\n",
      "Disgust     202\n",
      "Anger       202\n",
      "Neutral     202\n",
      "Name: emotion_no_2nd_neut, dtype: int64\n",
      "\n",
      "3    202\n",
      "7    202\n",
      "4    202\n",
      "2    202\n",
      "6    202\n",
      "1    202\n",
      "0    202\n",
      "5    202\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# X, y for training\n",
    "df_train_exploded = df_train.explode(emotion_col).copy()\n",
    "df_train_exploded['target'] = df_train_exploded[emotion_col].map( label2key )\n",
    "\n",
    "print('\\nTrain set b4 upsampling:\\n', df_train_exploded[emotion_col].value_counts(), sep='')\n",
    "df_train_exploded = upsample_all( df_train_exploded.copy(), labels_col='target', random_state=random_state )\n",
    "print('\\nTrain set after upsampling:\\n', df_train_exploded[emotion_col].value_counts(), '\\n\\n', \n",
    "       df_train_exploded['target'].value_counts(), sep='')\n",
    "\n",
    "X_train_exploded = df_train_exploded[text_col].values\n",
    "y_train_exploded = df_train_exploded['target'].values\n",
    "#df_train_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (1616,) (1616,) (776,) (776, 8) (208,) (208, 8)\n"
     ]
    }
   ],
   "source": [
    "# for testing on training set\n",
    "X_train         = df_train[text_col].values\n",
    "y_train_encoded = np.array( df_train['target_encoded'].values.tolist() )\n",
    "\n",
    "# for testing on test set\n",
    "X_dev          = df_dev[text_col].values\n",
    "y_dev_encoded  = np.array( df_dev['target_encoded'].values.tolist() )\n",
    "\n",
    "X_train_exploded, y_train_exploded = sklearn.utils.shuffle( X_train_exploded, y_train_exploded,\n",
    "                                                            random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train_exploded.shape, y_train_exploded.shape, X_train.shape, y_train_encoded.shape,\n",
    "                              X_dev.shape, y_dev_encoded.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68e01147",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_nb = {\n",
    "    'alpha': 1.0,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "\n",
    "clf_params_rf = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'entropy',                         # “gini”, “entropy”\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'auto',                      # “auto”, “sqrt”, “log2”\n",
    "    'class_weight': None,                        # dict, 'balanced', 'balanced_subsample', None\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "\n",
    "clf_params_knn = {    \n",
    "    'n_neighbors': 5,\n",
    "    'weights': 'uniform',     # default=’uniform’, {‘uniform’, ‘distance’}\n",
    "    'algorithm': 'auto',      # default=’auto’, {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}\n",
    "    'metric': 'minkowski',    # default=’minkowski’{ 'euclidean', 'cosine', } + sklearn.neighbors.VALID_METRICS['brute']\n",
    "    'p': 2,                   # default=2, p for minkowski distance\n",
    "    'n_jobs': -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3541e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_svm = {\n",
    "    \n",
    "    'C': 1.0,                      # default=1.0\n",
    "    'kernel': 'rbf',               # default=’rbf’, {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}\n",
    "    'degree': 3,                   # default=3, degree for polynomial f(x)\n",
    "    'tol': 1e-3,                   # stopping criteria, default=1e-3\n",
    "    'gamma': 'scale',               # default=’scale’, kernel coeff for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "                                   # 'scale' => 1 / (n_features * X.var()), ‘auto’ => 1 / n_features\n",
    "    'coef0': 0.0,                  # default=0.0, independent term in kernel function in ‘poly’ and ‘sigmoid’\n",
    "    'shrinking': True,             # default=True'\n",
    "    'cache_size': 200,             # default=200,   size of the kernel cache (in MB)\n",
    "    'decision_function_shape': 'ovr',    # default=’ovr’, {‘ovo’, ‘ovr’}, multiclass => always 'ovo'\n",
    "    'break_ties': False,           # default=False, for decision_function_shape='ovr' and num classes>2 (longer)\n",
    "    'max_iter': -1,                # default=-1,    limit on iterations\n",
    "    'class_weight': 'balanced',          # default=None,  dict or ‘balanced'\n",
    "    'probability': True,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ddf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full LR\n",
    "clf_params_lr = {\n",
    "\n",
    "    'C': 1.0,                      # default 0.1, inverse regularization strength, smaller => stronger regularization\n",
    "    \n",
    "    'solver': 'liblinear',         # default=’lbfgs’ {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "    # small dataset => ‘liblinear’ big dataset => ‘sag’ and ‘saga’ (faster);\n",
    "    # multiclass => ‘newton-cg’, ‘sag’, ‘saga’, ‘lbfgs’; ‘liblinear’ only for one-versus-rest\n",
    "    # supported penalties by solver: ‘newton-cg’, ‘lbfgs’, ‘sag’ - [‘l2’, ‘none’], ‘liblinear’ - [‘l1’, ‘l2’],\n",
    "    # ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]\n",
    "\n",
    "    'max_iter': 200,               # default=100, iters for solvers to converge    \n",
    "    'penalty': 'l2',               # ‘l1’, ‘l2’, ‘elasticnet’ (both), ‘none’, default=’l2’ (not for al solvers)\n",
    "    'dual': True,                 # default=False (dual formulation only for l2  with liblinear solver\n",
    "                                   # Prefer dual=False when n_samples > n_features\n",
    "\n",
    "    'tol': 1e-4,                   # stopping criteria, default=1e-4\n",
    "    'fit_intercept': True,          # default True; whether to fit bias / interceptbe added to the decision function\n",
    "    'intercept_scaling': 1,        # default=1, for solver ‘liblinear’ and self.fit_intercept=True (additional term)\n",
    "    'class_weight': None,          # default=None, dict or ‘balanced'\n",
    "        \n",
    "    'multi_class': 'auto',         #  default=’auto’, {‘auto’, ‘ovr’, ‘multinomial’},\n",
    "    # 'ovr’ => binary problem fit for each label\n",
    "    # ‘multinomial’ => multinomial loss fit across entire prob distribution\n",
    "    # ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.\n",
    "\n",
    "    'l1_ratio': None,\n",
    "    # default = None, elastic-Net mixing param, [0,1], only for penalty='elasticnet'. l1_ratio=0 => penalty='l2',\n",
    "    # l1_ratio=1 => penalty='l1', combination of L1 and L2 if in between\n",
    "    \n",
    "    'verbose': 0,\n",
    "    'warm_start': False,    \n",
    "    'n_jobs': -1,\n",
    "    'random_state': random_state,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df367a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 3,          # 3 - 0.5489\n",
    "        'learning_rate': 0.1,    #                            # eta\n",
    "        'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.9,           # 0.9  (0.5638, thres 0.21)     # 0-1    \n",
    "        'colsample_bylevel': 1.0,   #0.55 (0.5741, thres 0.25)     # 0-1\n",
    "        'colsample_bynode': 1.0,                                    # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                                    # 0-1  \n",
    "        'seed': 2,\n",
    "        'num_class': 8,\n",
    "        #'use_label_encoder': False,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "vect_params = {\n",
    "    'max_df': 0.45,    # 0.45 - 0.5285\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char_wb',\n",
    "    'ngram_range': (1,7),\n",
    "    'binary': True,\n",
    "    'stop_words': stopwords_combined,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54c3588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MultinomialNB( **clf_params_nb )\n",
    "#clf = LogisticRegression( **clf_params_lr )\n",
    "#clf = RandomForestClassifier( **clf_params_rf )\n",
    "#clf = SVC( **clf_params_svm )\n",
    "#clf = KNeighborsClassifier( **clf_params_knn )\n",
    "clf = XGBClassifier( **clf_params_xgb_word )\n",
    "#clf = RidgeClassifierCV()\n",
    "#clf = RidgeClassifierProba()\n",
    "#clf = AdaBoostClassifier()\n",
    "#clf = MLPClassifier()\n",
    "#clf = DecisionTreeClassifier()\n",
    "#clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "#clf_calib   = CalibratedClassifierCV(clf, cv=5, method='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(analyzer=&#x27;char_wb&#x27;, binary=True, max_df=0.45,\n",
       "                                 ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;enough&#x27;, &#x27;unable&#x27;, &#x27;often&#x27;, &#x27;con&#x27;,\n",
       "                                             &#x27;wherewith&#x27;, &#x27;whence&#x27;, &#x27;them&#x27;,\n",
       "                                             &#x27;thenceforth&#x27;, &#x27;been&#x27;, &#x27;anyhow&#x27;,\n",
       "                                             &#x27;even&#x27;, &#x27;whither&#x27;, &#x27;everywhere&#x27;,\n",
       "                                             &quot;how&#x27;s&quot;, &#x27;about&#x27;, &#x27;themselves&#x27;,\n",
       "                                             &#x27;somehow&#x27;, &#x27;others&#x27;, &#x27;thereon&#x27;,\n",
       "                                             &#x27;thou&#x27;, &quot;shouldn&#x27;t&quot;, &#x27;eleven&#x27;,\n",
       "                                             &#x27;fill&#x27;, &#x27;somebody&#x27;, &#x27;under&#x27;,\n",
       "                                             &#x27;fort...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=3, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=8,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(analyzer=&#x27;char_wb&#x27;, binary=True, max_df=0.45,\n",
       "                                 ngram_range=(1, 7),\n",
       "                                 stop_words=[&#x27;enough&#x27;, &#x27;unable&#x27;, &#x27;often&#x27;, &#x27;con&#x27;,\n",
       "                                             &#x27;wherewith&#x27;, &#x27;whence&#x27;, &#x27;them&#x27;,\n",
       "                                             &#x27;thenceforth&#x27;, &#x27;been&#x27;, &#x27;anyhow&#x27;,\n",
       "                                             &#x27;even&#x27;, &#x27;whither&#x27;, &#x27;everywhere&#x27;,\n",
       "                                             &quot;how&#x27;s&quot;, &#x27;about&#x27;, &#x27;themselves&#x27;,\n",
       "                                             &#x27;somehow&#x27;, &#x27;others&#x27;, &#x27;thereon&#x27;,\n",
       "                                             &#x27;thou&#x27;, &quot;shouldn&#x27;t&quot;, &#x27;eleven&#x27;,\n",
       "                                             &#x27;fill&#x27;, &#x27;somebody&#x27;, &#x27;under&#x27;,\n",
       "                                             &#x27;fort...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=3, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=8,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softmax&#x27;, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char_wb&#x27;, binary=True, max_df=0.45,\n",
       "                ngram_range=(1, 7),\n",
       "                stop_words=[&#x27;enough&#x27;, &#x27;unable&#x27;, &#x27;often&#x27;, &#x27;con&#x27;, &#x27;wherewith&#x27;,\n",
       "                            &#x27;whence&#x27;, &#x27;them&#x27;, &#x27;thenceforth&#x27;, &#x27;been&#x27;, &#x27;anyhow&#x27;,\n",
       "                            &#x27;even&#x27;, &#x27;whither&#x27;, &#x27;everywhere&#x27;, &quot;how&#x27;s&quot;, &#x27;about&#x27;,\n",
       "                            &#x27;themselves&#x27;, &#x27;somehow&#x27;, &#x27;others&#x27;, &#x27;thereon&#x27;,\n",
       "                            &#x27;thou&#x27;, &quot;shouldn&#x27;t&quot;, &#x27;eleven&#x27;, &#x27;fill&#x27;, &#x27;somebody&#x27;,\n",
       "                            &#x27;under&#x27;, &#x27;forty&#x27;, &#x27;same&#x27;, &#x27;seemed&#x27;, &#x27;between&#x27;,\n",
       "                            &quot;why&#x27;s&quot;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;merror&#x27;, feature_types=None, gamma=0, gpu_id=None,\n",
       "              grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
       "              max_depth=3, max_leaves=None, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
       "              num_class=8, num_parallel_tree=None, objective=&#x27;multi:softmax&#x27;, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(analyzer='char_wb', binary=True, max_df=0.45,\n",
       "                                 ngram_range=(1, 7),\n",
       "                                 stop_words=['enough', 'unable', 'often', 'con',\n",
       "                                             'wherewith', 'whence', 'them',\n",
       "                                             'thenceforth', 'been', 'anyhow',\n",
       "                                             'even', 'whither', 'everywhere',\n",
       "                                             \"how's\", 'about', 'themselves',\n",
       "                                             'somehow', 'others', 'thereon',\n",
       "                                             'thou', \"shouldn't\", 'eleven',\n",
       "                                             'fill', 'somebody', 'under',\n",
       "                                             'fort...\n",
       "                               feature_types=None, gamma=0, gpu_id=None,\n",
       "                               grow_policy=None, importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=1,\n",
       "                               max_depth=3, max_leaves=None, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1, num_class=8,\n",
       "                               num_parallel_tree=None,\n",
       "                               objective='multi:softmax', ...))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "model       = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train_exploded, y_train_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_probas = model.predict_proba(X_train)\n",
    "y_pred_dev_probas   = model.predict_proba(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebed7ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best train and test thresholds: 0.27, 0.21\n",
      "Labels: ['Anger', 'Disgust', 'Fear', 'Hope', 'Joy', 'Neutral', 'Sadness', 'Surprise']\n"
     ]
    }
   ],
   "source": [
    "def convert_preds(pred_probas, threshold = 0.33):\n",
    "    '''\n",
    "        Convert predicted probabilities into a one-hot encoded binary list\n",
    "        based on a threshold for the second class. First class is always argmax()\n",
    "    '''\n",
    "    y_pred_ = []\n",
    "    for probas in pred_probas:\n",
    "        sorted_idxs = np.argsort(probas)\n",
    "        proba2      = probas[sorted_idxs[-2]]\n",
    "        res_idxs    = sorted_idxs[-2:] if proba2 >= threshold else sorted_idxs[-1:]\n",
    "        res = [0]*8\n",
    "        for idx in res_idxs:\n",
    "            res[idx] = 1\n",
    "        y_pred_.append(res)\n",
    "    return np.array(y_pred_)\n",
    "\n",
    "\n",
    "def find_best_threshold(y_encoded, y_pred_probas):\n",
    "    '''Return best threshold'''\n",
    "    res_dev = []\n",
    "    for i in range(0,101):\n",
    "        threshold = i/100\n",
    "        y_pred_encoded = convert_preds(y_pred_probas, threshold=threshold)\n",
    "        clf_rep_dev = classification_report( y_encoded, y_pred_encoded, output_dict=True )\n",
    "        res_dev.append([ clf_rep_dev['macro avg']['f1-score'], threshold ])\n",
    "    return sorted(res_dev, reverse=True)[0]\n",
    "\n",
    "\n",
    "_, threshold_train = find_best_threshold(y_train_encoded, y_pred_train_probas)\n",
    "_, threshold_dev   = find_best_threshold(y_dev_encoded, y_pred_dev_probas)\n",
    "print(f'Best train and test thresholds: {threshold_train}, {threshold_dev}')\n",
    "\n",
    "y_pred_train_encoded = convert_preds(y_pred_train_probas, threshold=threshold_train)\n",
    "y_pred_dev_encoded   = convert_preds(y_pred_dev_probas, threshold=threshold_dev)\n",
    "labels = list(label2key.keys())\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfd5f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "CountVectorizer(analyzer='char_wb', binary=True, max_df=0.45,\n",
      "                ngram_range=(1, 7),\n",
      "                stop_words=['enough', 'unable', 'often', 'con', 'wherewith',\n",
      "                            'whence', 'them', 'thenceforth', 'been', 'anyhow',\n",
      "                            'even', 'whither', 'everywhere', \"how's\", 'about',\n",
      "                            'themselves', 'somehow', 'others', 'thereon',\n",
      "                            'thou', \"shouldn't\", 'eleven', 'fill', 'somebody',\n",
      "                            'under', 'forty', 'same', 'seemed', 'between',\n",
      "                            \"why's\", ...])\n",
      "\n",
      "Classifier:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1.0, colsample_bynode=1.0, colsample_bytree=1.0,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric='merror', feature_types=None, gamma=0, gpu_id=None,\n",
      "              grow_policy=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
      "              max_depth=3, max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=-1,\n",
      "              num_class=8, num_parallel_tree=None, objective='multi:softmax', ...)\n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger     0.9561    0.9237    0.9397       118\n",
      "     Disgust     0.9880    0.8913    0.9371        92\n",
      "        Fear     1.0000    1.0000    1.0000        58\n",
      "        Hope     0.9683    0.9683    0.9683        63\n",
      "         Joy     1.0000    1.0000    1.0000        70\n",
      "     Neutral     0.9621    0.8826    0.9206       230\n",
      "     Sadness     0.9385    0.8317    0.8819       202\n",
      "    Surprise     1.0000    0.9853    0.9926        68\n",
      "\n",
      "   micro avg     0.9680    0.9079    0.9370       901\n",
      "   macro avg     0.9766    0.9354    0.9550       901\n",
      "weighted avg     0.9673    0.9079    0.9362       901\n",
      " samples avg     0.9800    0.9442    0.9516       901\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger     0.4694    0.6053    0.5287        38\n",
      "     Disgust     0.3947    0.6250    0.4839        24\n",
      "        Fear     0.5000    0.6250    0.5556         8\n",
      "        Hope     0.4737    0.5625    0.5143        16\n",
      "         Joy     1.0000    0.5000    0.6667         2\n",
      "     Neutral     0.4535    0.7222    0.5571        54\n",
      "     Sadness     0.8000    0.7525    0.7755       101\n",
      "    Surprise     0.2727    1.0000    0.4286         3\n",
      "\n",
      "   micro avg     0.5534    0.6951    0.6162       246\n",
      "   macro avg     0.5455    0.6741    0.5638       246\n",
      "weighted avg     0.5975    0.6951    0.6317       246\n",
      " samples avg     0.6034    0.7163    0.6298       246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizer:\\n', model['vect'], '\\n', sep='')\n",
    "print('Classifier:\\n', model['clf'], '\\n', sep='')\n",
    "\n",
    "print('\\nTRAINSET')\n",
    "print( classification_report( y_train_encoded, y_pred_train_encoded, target_names=labels, digits=4 ) )\n",
    "clf_rep_train = classification_report( y_train_encoded, y_pred_train_encoded, target_names=labels, output_dict=True )\n",
    "\n",
    "print('DEVSET')\n",
    "print( classification_report( y_dev_encoded, y_pred_dev_encoded, target_names=labels, digits=4 ) )\n",
    "clf_rep_dev = classification_report( y_dev_encoded, y_pred_dev_encoded, target_names=labels, output_dict=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a3218",
   "metadata": {},
   "source": [
    "Initial results for text_col + ts are good (seem better than for text_col). Just ts is close (slightly lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b21a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3c932f",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fb5dbd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5637923782787123, 0.21],\n",
       " [0.5563234615633137, 0.2],\n",
       " [0.5512856529254565, 0.19],\n",
       " [0.5407239392861057, 0.26],\n",
       " [0.5398555344058867, 0.24],\n",
       " [0.537027565027565, 0.25],\n",
       " [0.5367097945620769, 0.22],\n",
       " [0.5352385614793703, 0.23],\n",
       " [0.533215532674898, 0.27],\n",
       " [0.5307408126220245, 0.18],\n",
       " [0.5302024711881123, 0.17],\n",
       " [0.5288625422846734, 0.3],\n",
       " [0.5286753256936744, 0.33],\n",
       " [0.5273190054121892, 0.29],\n",
       " [0.5267087842070023, 0.28],\n",
       " [0.5266782760849698, 0.14],\n",
       " [0.5255744970985825, 0.16],\n",
       " [0.5253721170625968, 0.32],\n",
       " [0.5247048943770255, 0.31],\n",
       " [0.5234815395096254, 0.15],\n",
       " [0.5216467671105833, 0.13],\n",
       " [0.5187339893222246, 0.34],\n",
       " [0.5180605706432189, 0.12],\n",
       " [0.5174366059461795, 0.11],\n",
       " [0.5140496682699256, 0.1],\n",
       " [0.5127879246075031, 0.36],\n",
       " [0.5127259145292336, 0.38],\n",
       " [0.5127259145292336, 0.37],\n",
       " [0.5109513308042719, 0.35],\n",
       " [0.5078373498668962, 0.09],\n",
       " [0.5066775274324594, 0.4],\n",
       " [0.5066775274324594, 0.39],\n",
       " [0.5065189140669202, 0.08],\n",
       " [0.4999494037177553, 0.07],\n",
       " [0.49872038841838806, 0.06],\n",
       " [0.4983871185650268, 0.05],\n",
       " [0.4971383334230366, 0.04],\n",
       " [0.49681604981346017, 0.03],\n",
       " [0.49681604981346017, 0.02],\n",
       " [0.49681604981346017, 0.01],\n",
       " [0.49623734610975645, 0.0],\n",
       " [0.4735440521255778, 1.0],\n",
       " [0.4735440521255778, 0.99],\n",
       " [0.4735440521255778, 0.98],\n",
       " [0.4735440521255778, 0.97],\n",
       " [0.4735440521255778, 0.96],\n",
       " [0.4735440521255778, 0.95],\n",
       " [0.4735440521255778, 0.94],\n",
       " [0.4735440521255778, 0.93],\n",
       " [0.4735440521255778, 0.92],\n",
       " [0.4735440521255778, 0.91],\n",
       " [0.4735440521255778, 0.9],\n",
       " [0.4735440521255778, 0.89],\n",
       " [0.4735440521255778, 0.88],\n",
       " [0.4735440521255778, 0.87],\n",
       " [0.4735440521255778, 0.86],\n",
       " [0.4735440521255778, 0.85],\n",
       " [0.4735440521255778, 0.84],\n",
       " [0.4735440521255778, 0.83],\n",
       " [0.4735440521255778, 0.82],\n",
       " [0.4735440521255778, 0.81],\n",
       " [0.4735440521255778, 0.8],\n",
       " [0.4735440521255778, 0.79],\n",
       " [0.4735440521255778, 0.78],\n",
       " [0.4735440521255778, 0.77],\n",
       " [0.4735440521255778, 0.76],\n",
       " [0.4735440521255778, 0.75],\n",
       " [0.4735440521255778, 0.74],\n",
       " [0.4735440521255778, 0.73],\n",
       " [0.4735440521255778, 0.72],\n",
       " [0.4735440521255778, 0.71],\n",
       " [0.4735440521255778, 0.7],\n",
       " [0.4735440521255778, 0.69],\n",
       " [0.4735440521255778, 0.68],\n",
       " [0.4735440521255778, 0.67],\n",
       " [0.4735440521255778, 0.66],\n",
       " [0.4735440521255778, 0.65],\n",
       " [0.4735440521255778, 0.64],\n",
       " [0.4735440521255778, 0.63],\n",
       " [0.4735440521255778, 0.62],\n",
       " [0.4735440521255778, 0.61],\n",
       " [0.4735440521255778, 0.6],\n",
       " [0.4735440521255778, 0.59],\n",
       " [0.4735440521255778, 0.58],\n",
       " [0.4735440521255778, 0.57],\n",
       " [0.4735440521255778, 0.56],\n",
       " [0.4735440521255778, 0.55],\n",
       " [0.4735440521255778, 0.54],\n",
       " [0.4735440521255778, 0.53],\n",
       " [0.4735440521255778, 0.52],\n",
       " [0.4735440521255778, 0.51],\n",
       " [0.4735440521255778, 0.5],\n",
       " [0.4735440521255778, 0.49],\n",
       " [0.4735440521255778, 0.48],\n",
       " [0.4735440521255778, 0.47],\n",
       " [0.4735440521255778, 0.46],\n",
       " [0.4735440521255778, 0.45],\n",
       " [0.4735440521255778, 0.44],\n",
       " [0.4735440521255778, 0.43],\n",
       " [0.4735440521255778, 0.42],\n",
       " [0.4735440521255778, 0.41]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_threshold2(y_dev_encoded, y_pred_dev_probas):\n",
    "    '''Return all thresholds (for control)'''\n",
    "    res_dev = []\n",
    "    for i in range(0,101):\n",
    "        threshold = i/100\n",
    "        y_pred_dev_encoded = convert_preds(y_pred_dev_probas, threshold=threshold)\n",
    "        clf_rep_dev = classification_report( y_dev_encoded, y_pred_dev_encoded, output_dict=True )\n",
    "        res_dev.append([ clf_rep_dev['macro avg']['f1-score'], threshold ])\n",
    "    return sorted(res_dev, reverse=True)\n",
    "\n",
    "find_best_threshold2(y_dev_encoded, y_pred_dev_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478901b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "199f3704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best train and test thresholds: 0.27, 0.21\n",
      "\n",
      "PARAM: 1\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger     0.9561    0.9237    0.9397       118\n",
      "     Disgust     0.9880    0.8913    0.9371        92\n",
      "        Fear     1.0000    1.0000    1.0000        58\n",
      "        Hope     0.9683    0.9683    0.9683        63\n",
      "         Joy     1.0000    1.0000    1.0000        70\n",
      "     Neutral     0.9621    0.8826    0.9206       230\n",
      "     Sadness     0.9385    0.8317    0.8819       202\n",
      "    Surprise     1.0000    0.9853    0.9926        68\n",
      "\n",
      "   micro avg     0.9680    0.9079    0.9370       901\n",
      "   macro avg     0.9766    0.9354    0.9550       901\n",
      "weighted avg     0.9673    0.9079    0.9362       901\n",
      " samples avg     0.9800    0.9442    0.9516       901\n",
      "\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger     0.4694    0.6053    0.5287        38\n",
      "     Disgust     0.3947    0.6250    0.4839        24\n",
      "        Fear     0.5000    0.6250    0.5556         8\n",
      "        Hope     0.4737    0.5625    0.5143        16\n",
      "         Joy     1.0000    0.5000    0.6667         2\n",
      "     Neutral     0.4535    0.7222    0.5571        54\n",
      "     Sadness     0.8000    0.7525    0.7755       101\n",
      "    Surprise     0.2727    1.0000    0.4286         3\n",
      "\n",
      "   micro avg     0.5534    0.6951    0.6162       246\n",
      "   macro avg     0.5455    0.6741    0.5638       246\n",
      "weighted avg     0.5975    0.6951    0.6317       246\n",
      " samples avg     0.6034    0.7163    0.6298       246\n",
      "\n",
      "\n",
      "Best macro F1 score: 0.5638\n",
      "\n",
      " ============================================================================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "#params = [i/100 for i in range(1,101)] #+ [i for i in range(150,501,25)]\n",
    "#params = [2,3,4,5,6,7,8,9,10,11,15]\n",
    "#params = [7,8,9,10,11,12,14,15]\n",
    "params = [1]\n",
    "\n",
    "for param in params:\n",
    "    clf_params_xgb_word2 = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 3,          # 3 - 0.5489\n",
    "        'learning_rate': 0.1,    #                            # eta\n",
    "        'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.9,           # 0.9  (0.5638, thres 0.21)     # 0-1    \n",
    "        'colsample_bylevel': 1.0,   #0.55 (0.5741, thres 0.25)     # 0-1\n",
    "        'colsample_bynode': 1.0,                                    # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                                    # 0-1  \n",
    "        'seed': 2,\n",
    "        'num_class': 8,\n",
    "        #'use_label_encoder': False,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 0.45,    # 0.45 - 0.5285\n",
    "        'min_df': 1,\n",
    "        'analyzer': 'char_wb',\n",
    "        'ngram_range': (1,7),\n",
    "        'binary': True,\n",
    "        'stop_words': stopwords_combined,\n",
    "    }\n",
    "\n",
    "    clf        = XGBClassifier( **clf_params_xgb_word2 )\n",
    "    vectorizer = CountVectorizer( **vect_params2 )\n",
    "    #vectorizer = TfidfVectorizer( **vect_params2 )\n",
    "    model      = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "    model.fit(X_train_exploded, y_train_exploded)\n",
    "\n",
    "    y_pred_train_probas = model.predict_proba(X_train)\n",
    "    y_pred_dev_probas   = model.predict_proba(X_dev)\n",
    "    \n",
    "    _, threshold_train = find_best_threshold(y_train_encoded, y_pred_train_probas)\n",
    "    _, threshold_dev   = find_best_threshold(y_dev_encoded, y_pred_dev_probas)\n",
    "    print(f'Best train and test thresholds: {threshold_train}, {threshold_dev}\\n')\n",
    "    \n",
    "    y_pred_train_encoded = convert_preds(y_pred_train_probas, threshold=threshold_train)\n",
    "    y_pred_dev_encoded   = convert_preds(y_pred_dev_probas, threshold=threshold_dev)\n",
    "    labels = list(label2key.keys())\n",
    "\n",
    "    print('PARAM:', param)\n",
    "    print('\\nTRAINSET')\n",
    "    print( classification_report( y_train_encoded, y_pred_train_encoded, target_names=labels, digits=4 ) )\n",
    "    clf_rep1 = classification_report( y_train_encoded, y_pred_train_encoded, target_names=labels, output_dict=True )\n",
    "\n",
    "    print('\\nDEVSET')\n",
    "    print( classification_report( y_dev_encoded, y_pred_dev_encoded, target_names=labels, digits=4 ) )\n",
    "    clf_rep2 = classification_report( y_dev_encoded, y_pred_dev_encoded, target_names=labels, output_dict=True )    \n",
    "\n",
    "    res.append(( clf_rep2['micro avg']['f1-score'], clf_rep2['macro avg']['f1-score'],\n",
    "                 clf_rep1['micro avg']['f1-score'], clf_rep1['macro avg']['f1-score'], param, threshold_dev, ))\n",
    "    print('\\nBest macro F1 score:', round(sorted(res, key=lambda x: x[1], reverse=True)[0][1], 4) )\n",
    "    print('\\n', '='*77, '\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61ad737a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6162162162162161, 0.5637923782787123, 0.9369988545246278, 0.9550211593522074, 1, 0.21)\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(res, key=lambda x: x[1], reverse=True):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f66b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample alone\n",
    "subsample2 = '''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# colsample_bylevel when subsample = 0.87\n",
    "colsample_bylevel2 = '''\n",
    "\n",
    "'''\n",
    "\n",
    "# colsample_bylevel when subsample = 0.7\n",
    "colsample_bylevel2 = '''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = '''\n",
    "\n",
    "'''\n",
    "\n",
    "alpha = '''\n",
    "\n",
    "'''\n",
    "\n",
    "lambda1 = '''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf726c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9709a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b7732e",
   "metadata": {},
   "source": [
    "# Best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568638a",
   "metadata": {},
   "source": [
    "### Feature col: essay_clean (sadness downsampled to neutral (202))¶\n",
    "Using augmented data  \n",
    "Macro F1 = 0.5638  \n",
    "The spellchecked columns has slightly better results (Macro F1 = 0.574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word2 = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 3,          # 3 - 0.5489\n",
    "        'learning_rate': 0.1,    #                            # eta\n",
    "        'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.9,           # 0.9  (0.5638, thres 0.21)     # 0-1    \n",
    "        'colsample_bylevel': 1.0,   #0.55 (0.5741, thres 0.25)     # 0-1\n",
    "        'colsample_bynode': 1.0,                                    # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                                    # 0-1  \n",
    "        'seed': 2,\n",
    "        'num_class': 8,\n",
    "        #'use_label_encoder': False,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 0.45,    # 0.45 - 0.5285\n",
    "        'min_df': 1,\n",
    "        'analyzer': 'char_wb',\n",
    "        'ngram_range': (1,7),\n",
    "        'binary': True,\n",
    "        'stop_words': stopwords_combined,\n",
    "    }\n",
    "# Countvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a465e1",
   "metadata": {},
   "source": [
    "```\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Anger     0.9561    0.9237    0.9397       118\n",
    "     Disgust     0.9880    0.8913    0.9371        92\n",
    "        Fear     1.0000    1.0000    1.0000        58\n",
    "        Hope     0.9683    0.9683    0.9683        63\n",
    "         Joy     1.0000    1.0000    1.0000        70\n",
    "     Neutral     0.9621    0.8826    0.9206       230\n",
    "     Sadness     0.9385    0.8317    0.8819       202\n",
    "    Surprise     1.0000    0.9853    0.9926        68\n",
    "\n",
    "   micro avg     0.9680    0.9079    0.9370       901\n",
    "   macro avg     0.9766    0.9354    0.9550       901\n",
    "weighted avg     0.9673    0.9079    0.9362       901\n",
    " samples avg     0.9800    0.9442    0.9516       901\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Anger     0.4694    0.6053    0.5287        38\n",
    "     Disgust     0.3947    0.6250    0.4839        24\n",
    "        Fear     0.5000    0.6250    0.5556         8\n",
    "        Hope     0.4737    0.5625    0.5143        16\n",
    "         Joy     1.0000    0.5000    0.6667         2\n",
    "     Neutral     0.4535    0.7222    0.5571        54\n",
    "     Sadness     0.8000    0.7525    0.7755       101\n",
    "    Surprise     0.2727    1.0000    0.4286         3\n",
    "\n",
    "   micro avg     0.5534    0.6951    0.6162       246\n",
    "   macro avg     0.5455    0.6741    0.5638       246\n",
    "weighted avg     0.5975    0.6951    0.6317       246\n",
    " samples avg     0.6034    0.7163    0.6298       246\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad27e5",
   "metadata": {},
   "source": [
    "### Feature col: essay_clean_spellchecked (upsample to sadness (383))\n",
    "Before augmented data was used  \n",
    "Macro F1 = 0.5057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,                                 # eta\n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.7,                                     # 0-1    \n",
    "    'colsample_bylevel': 0.45,  #0.45                            # 0-1\n",
    "    'colsample_bynode': 1.0,  #0.45                             # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 8,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char_wb',\n",
    "    'ngram_range': (1,5),\n",
    "    'binary': True,\n",
    "    'stop_words': stopwords_combined,\n",
    "}\n",
    "# Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab62aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d35f73e",
   "metadata": {},
   "source": [
    "### Feature col: essay_clean_spellchecked (sadness downsampled to neutral (202))\n",
    "Before augmented data was used  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word2 = {\n",
    "        'n_estimators': 145,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,    # 0.3 is close too          # eta\n",
    "        'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.7,           # 0.7                                # 0-1    \n",
    "        'colsample_bylevel': 0.28,   #0.28 (0.5342)                  # 0-1\n",
    "        'colsample_bynode': 1.0,  #0.28 (0.5342, thres=0.26)      # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                                    # 0-1  \n",
    "        'seed': 2,\n",
    "        'num_class': 8,\n",
    "        #'use_label_encoder': False,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 1.0,\n",
    "        'min_df': 1,\n",
    "        'analyzer': 'char_wb',\n",
    "        'ngram_range': (1,5),\n",
    "        'binary': True,\n",
    "        'stop_words': stopwords_combined,\n",
    "    }   \n",
    "clf        = XGBClassifier( **clf_params_xgb_word2 )\n",
    "vectorizer = CountVectorizer( **vect_params2 )\n",
    "model      = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f221c",
   "metadata": {},
   "source": [
    "```\n",
    "Best train and test thresholds: 0.07, 0.23\n",
    "\n",
    "PARAM: 1\n",
    "\n",
    "TRAINSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Anger     0.9915    0.9915    0.9915       118\n",
    "     Disgust     1.0000    1.0000    1.0000        92\n",
    "        Fear     1.0000    1.0000    1.0000        33\n",
    "        Hope     1.0000    1.0000    1.0000        32\n",
    "         Joy     1.0000    1.0000    1.0000        10\n",
    "     Neutral     0.9808    0.8870    0.9315       230\n",
    "     Sadness     0.9851    0.9802    0.9826       202\n",
    "    Surprise     1.0000    1.0000    1.0000        19\n",
    "\n",
    "   micro avg     0.9888    0.9579    0.9731       736\n",
    "   macro avg     0.9947    0.9823    0.9882       736\n",
    "weighted avg     0.9885    0.9579    0.9725       736\n",
    " samples avg     0.9935    0.9749    0.9792       736\n",
    "\n",
    "\n",
    "DEVSET\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Anger     0.4500    0.4737    0.4615        38\n",
    "     Disgust     0.6154    0.6667    0.6400        24\n",
    "        Fear     0.6667    0.5000    0.5714         8\n",
    "        Hope     0.5385    0.4375    0.4828        16\n",
    "         Joy     0.0000    0.0000    0.0000         2\n",
    "     Neutral     0.4824    0.7593    0.5899        54\n",
    "     Sadness     0.7788    0.8020    0.7902       101\n",
    "    Surprise     1.0000    0.6667    0.8000         3\n",
    "\n",
    "   micro avg     0.6123    0.6870    0.6475       246\n",
    "   macro avg     0.5665    0.5382    0.5420       246\n",
    "weighted avg     0.6241    0.6870    0.6474       246\n",
    " samples avg     0.6683    0.7139    0.6659       246\n",
    "\n",
    "\n",
    "Best macro F1 score: 0.542\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac2912",
   "metadata": {},
   "source": [
    "### Feature column: essay_clean_spellchecked_ts (sadness downsampled to neutral (202))\n",
    "Before augmented data was used  \n",
    "F1 macro never got over 0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word = {\n",
    "    'n_estimators': 125,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.43,    # 0.3 is close too          # eta\n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0.61,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,  #0.45                            # 0-1\n",
    "    'colsample_bynode': 1.0,  #0.45                             # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 8,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'char_wb',\n",
    "    'ngram_range': (1,4),\n",
    "    'binary': True,\n",
    "    'stop_words': stopwords_combined,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47844e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d8ed28",
   "metadata": {},
   "source": [
    "### Feature columns: tsk (sadness downsampled to neutral (202))\n",
    "Before augmented data was used  \n",
    "Never got over 0.45 before fine-tuning regularization params (word or char ngrams, Tfidf or Counvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31abf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word2 = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.32,    # 0.3 is close too          # eta\n",
    "    'objective': 'multi:softmax',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1.0,                                     # 0-1    \n",
    "    'colsample_bylevel': 1.0,  #0.45                            # 0-1\n",
    "    'colsample_bynode': 1.0,  #0.45                             # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                              # 0-1  \n",
    "    'seed': 2,\n",
    "    'num_class': 8,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 1.0,\n",
    "        'min_df': 1,\n",
    "        'analyzer': 'word',\n",
    "        'ngram_range': (1,1),\n",
    "        'binary': False,\n",
    "        'stop_words': stopwords_combined,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
