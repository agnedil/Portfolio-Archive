{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "# EMPATHY TRACK - BASELINE REGRESSOR\n",
    "## ACL 2023 Conference\n",
    "## WASSA 2023 Shared Task on Empathy, Emotion, and Personality Detection in Interactions\n",
    "More details [here](https://codalab.lisn.upsaclay.fr/competitions/11167#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import ftfy\n",
    "import pycld2 as cld2\n",
    "import time\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0a230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_8cats =      [ \"'s\", 'a', 'about', 'after', 'again', 'all', 'am', 'america', 'an', 'and', 'animal', 'animals',\n",
    "                    'are', 'around', 'as', 'at', 'bad', 'be', 'because', 'but', 'by', 'can', 'children',\n",
    "                    'crazy', 'death', 'do', 'even', 'find', 'for', 'from', 'get', 'go', 'had', 'has', 'have',\n",
    "                    'having', 'he', 'his', 'horrible', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'just',\n",
    "                    'kill', 'killed', 'know', 'like', 'live', 'life', 'lives', 'lived', 'm', 'make', 'makes',\n",
    "                    'man', 'me', 'mind', 'more', 'most', 'much', 'my', 'need', 'never', 'no', 'not', 'now',\n",
    "                    'of', 'on', 'one', 'or', 'other', 'out', 'people', 'place', 'put', 'really', 'sad', 'see',\n",
    "                    'seems', 'situation', 'so', 'some', 'something', 'species', 'stop', 'story',\n",
    "                    'such', 't', 'take', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'thing',\n",
    "                    'things', 'think', 'this', 'time', 'to', 'type', 'up', 'us', 'very', 'war', 'was', 'way',\n",
    "                    'we', 'were', 'what', 'when', 'with', 'worse', 'would', 'you',\n",
    "                   ]\n",
    "\n",
    "words_7cats      = [ 'age', 'air', 'also', 'always', 'any', 'article', 'attack', 'away', 'back', 'been', 'before',\n",
    "                     'being', 'believe', 'both', 'cause', 'child', 'could', 'country', 'day', 'deal', 'did', 'die',\n",
    "                     'disease', 'done', 'down', 'during', 'dying', 'each', 'either', 'end', 'facing', 'feel',\n",
    "                     'felt', 'first', 'food', 'future', 'girl', 'glad', 'going', 'good', 'government', 'great',\n",
    "                     'guess', 'happened', 'happening', 'hard', 'harm', 'hate', 'her', 'high', 'him', 'humans',\n",
    "                     'imagine', 'instead', 'interesting', 'job', 'jobs', 'keep', 'kids', 'leave', 'left', 'let',\n",
    "                     'life', 'living', 'lost', 'lot', 'make', 'many', 'needs', 'new', 'normal', 'often', 'oil',\n",
    "                     'only', 'over', 'pain', 'person', 'places', 'poor', 'population', 'probably', 'problem',\n",
    "                     'protect', 'read', 'reading', 'real', 'same', 'say', 'she', 'should', 'show', 'sick',\n",
    "                     'society', 'someone', 'sounds', 'start', 'still', 'suffering', 'sure', 'terrible',\n",
    "                     'thinking', 'those', 'though', 'thought', 'twice', 'under', 'water', 'were', 'where',\n",
    "                     'which', 'who', 'whole', 'why', 'wildlife', 'will', 'woman', 'wonder', 'world', 'worried',\n",
    "                     'years', 'your', ]\n",
    "\n",
    "experimental_sw = words_7cats + words_8cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafb5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "318\n",
      "['a', 'cant', 'inc', 'hasnt', 'else', 'call', 'or', 'former', 'what', 'ours', 'between', 'toward', 'mine', 'each', 'so', 'when', 'nowhere', 'thereupon', 'otherwise', 'onto', 'whereas', 'ltd', 'thereby', 'found', 'side', 'until', 'from', 'find', 'only', 'thence', 'thru', 'thin', 'me', 'now', 'get', 'yours', 'anyhow', 'also', 'con', 'to', 'are', 'and', 'anything', 'sixty', 'why', 'seeming', 'as', 'him', 'take', 'eight', 'elsewhere', 'ten', 'an', 'where', 'down', 'had', 'one', 'put', 'except', 'afterwards', 'fill', 'even', 'thus', 'under', 'over', 'that', 'those', 'full', 'up', 'which', 'nevertheless', 'yet', 'towards', 'with', 'third', 'into', 'be', 'hereafter', 'among', 'well', 'nothing', 'none', 'any', 'empty', 'see', 'twenty', 'become', 'here', 'was', 'cannot', 'hereupon', 'since', 'first', 'because', 'somewhere', 'them', 'most', 'it', 'i', 'last', 'against', 'per', 'he', 'is', 'sometime', 'four', 'nor', 'seemed', 'two', 'namely', 'cry', 'were', 'whatever', 'if', 'another', 'mostly', 'wherein', 'our', 'behind', 'amoungst', 'always', 'we', 'us', 'above', 'describe', 'would', 'whom', 'five', 'seems', 'thereafter', 'of', 'hundred', 'others', 'its', 'couldnt', 'at', 'noone', 'ourselves', 'by', 'serious', 'ie', 'forty', 'neither', 'done', 'may', 'everything', 'beyond', 'herself', 'whence', 'nobody', 'six', 'through', 'mill', 'next', 'am', 'both', 'once', 'yourselves', 'whoever', 'twelve', 'hereby', 'whither', 'before', 'therein', 'less', 'co', 'whereafter', 'although', 'almost', 'together', 'nine', 'via', 'whereby', 'already', 'made', 'somehow', 'de', 'upon', 'too', 'part', 'this', 'been', 'sometimes', 'some', 'becomes', 'whose', 'has', 'often', 'never', 'anyway', 'all', 'how', 'have', 'herein', 'while', 'every', 'go', 'back', 'for', 'became', 'itself', 'own', 'eleven', 'top', 'who', 'something', 'bill', 'across', 'whenever', 'yourself', 'in', 'myself', 'enough', 'much', 'beforehand', 'three', 'wherever', 'several', 'however', 'whether', 'must', 'there', 'perhaps', 'during', 'many', 'fire', 'fifteen', 'very', 'eg', 'rather', 'detail', 'latterly', 'please', 'sincere', 'anyone', 'without', 'ever', 'meanwhile', 'can', 'about', 'again', 'further', 'could', 'system', 'anywhere', 'same', 'keep', 'interest', 'whole', 'out', 'moreover', 'should', 'etc', 'but', 'formerly', 'his', 'on', 'front', 'un', 'after', 'within', 'along', 'no', 'not', 'give', 'indeed', 'due', 'fifty', 'hence', 'themselves', 'might', 'hers', 'alone', 'than', 'seem', 'their', 'will', 'besides', 'your', 'beside', 'whereupon', 'either', 'thick', 'throughout', 'do', 'show', 'becoming', 'everyone', 'more', 'other', 'amongst', 'the', 'move', 'then', 'she', 'her', 'these', 'himself', 'though', 'latter', 'off', 'therefore', 'amount', 'being', 'few', 'such', 'someone', 'you', 'below', 'everywhere', 'around', 'name', 'my', 're', 'they', 'least', 'bottom', 'still']\n",
      "\n",
      "NLTK:\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Lemur\n",
      "431\n",
      "['.', ',', '?', '!', \"'\", '\"', \"''\", '`', '``', '*', '-', '/', '+', 'a', 'about', 'above', 'according', 'across', 'after', 'afterwards', 'again', 'against', 'albeit', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'are', 'around', 'as', 'at', 'av', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'canst', 'certain', 'cf', 'choose', 'contrariwise', 'cos', 'could', 'cu', 'day', 'do', 'does', \"doesn't\", 'doing', 'dost', 'doth', 'double', 'down', 'dual', 'during', 'each', 'either', 'else', 'elsewhere', 'enough', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'except', 'excepted', 'excepting', 'exception', 'exclude', 'excluding', 'exclusive', 'far', 'farther', 'farthest', 'few', 'ff', 'first', 'for', 'formerly', 'forth', 'forward', 'from', 'front', 'further', 'furthermore', 'furthest', 'get', 'go', 'had', 'halves', 'hardly', 'has', 'hast', 'hath', 'have', 'he', 'hence', 'henceforth', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereto', 'hereupon', 'hers', 'herself', 'him', 'himself', 'hindmost', 'his', 'hither', 'hitherto', 'how', 'however', 'howsoever', 'i', 'ie', 'if', 'in', 'inasmuch', 'inc', 'include', 'included', 'including', 'indeed', 'indoors', 'inside', 'insomuch', 'instead', 'into', 'inward', 'inwards', 'is', 'it', 'its', 'itself', 'just', 'kind', 'kg', 'km', 'last', 'latter', 'latterly', 'less', 'lest', 'let', 'like', 'little', 'ltd', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'moreover', 'most', 'mostly', 'more', 'mr', 'mrs', 'ms', 'much', 'must', 'my', 'myself', 'namely', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'nonetheless', 'noone', 'nope', 'nor', 'not', 'nothing', 'notwithstanding', 'now', 'nowadays', 'nowhere', 'of', 'off', 'often', 'ok', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'own', 'per', 'perhaps', 'plenty', 'provide', 'quite', 'rather', 'really', 'round', 'said', 'sake', 'same', 'sang', 'save', 'saw', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'seldom', 'selves', 'sent', 'several', 'shalt', 'she', 'should', 'shown', 'sideways', 'since', 'slept', 'slew', 'slung', 'slunk', 'smote', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'spake', 'spat', 'spoke', 'spoken', 'sprang', 'sprung', 'stave', 'staves', 'still', 'such', 'supposing', 'than', 'that', 'the', 'thee', 'their', 'them', 'themselves', 'then', 'thence', 'thenceforth', 'there', 'thereabout', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereto', 'thereupon', 'these', 'they', 'this', 'those', 'thou', 'though', 'thrice', 'through', 'throughout', 'thru', 'thus', 'thy', 'thyself', 'till', 'to', 'together', 'too', 'toward', 'towards', 'ugh', 'unable', 'under', 'underneath', 'unless', 'unlike', 'until', 'up', 'upon', 'upward', 'upwards', 'us', 'use', 'used', 'using', 'very', 'via', 'vs', 'want', 'was', 'we', 'week', 'well', 'were', 'what', 'whatever', 'whatsoever', 'when', 'whence', 'whenever', 'whensoever', 'where', 'whereabouts', 'whereafter', 'whereas', 'whereat', 'whereby', 'wherefore', 'wherefrom', 'wherein', 'whereinto', 'whereof', 'whereon', 'wheresoever', 'whereto', 'whereunto', 'whereupon', 'wherever', 'wherewith', 'whether', 'whew', 'which', 'whichever', 'whichsoever', 'while', 'whilst', 'whither', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whomever', 'whomsoever', 'whose', 'whosoever', 'why', 'will', 'wilt', 'with', 'within', 'without', 'worse', 'worst', 'would', 'wow', 'ye', 'yet', 'year', 'yippee', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "Other:\n",
      "154\n",
      "['i', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', 'my', 'myself', 'nor', 'of', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', 'would', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "COMBINED:\n",
      "579\n",
      "['a', 'cant', 'inc', 'somebody', 'hasnt', 'spat', 'whichever', 'else', 'call', 'or', 'did', 'former', 'what', 'ours', 'included', 'between', 'toward', 'lest', 'mine', 'shouldn', 'sent', \"hasn't\", 'each', 'let', 'so', 'when', 'nowhere', 'yippee', 'thereupon', 'otherwise', 'onto', '/', 'exclusive', 'whereas', \"doesn't\", 'ltd', \"i've\", 'thereby', 'found', 'side', 'until', \"we'll\", 'doing', 'hereto', \"they've\", 'howsoever', 'from', 'find', 'only', 'thence', 'mrs', \"i'll\", 'ought', 'mightn', 'seeing', 'thru', 'thin', 'whatsoever', 'me', 'now', 'get', 'wouldn', 'yours', 'nowadays', 'maybe', 'anyhow', 'vs', 'also', 'con', 'to', 'are', 'hitherto', 'and', 'anything', 'sixty', 'why', '?', 'seeming', 'inasmuch', 'y', 'sake', \"when's\", 'as', 'slept', 'him', 'take', 'eight', 'elsewhere', 'thou', 'ten', 'slunk', 'an', 'halves', 'where', 'farthest', 'down', 'had', \"she'd\", 'one', 'nope', \"wouldn't\", 'put', 'except', 'weren', 'afterwards', 'fill', 'even', 'thy', 'year', 'thus', 'under', 'over', 'ma', 'sideways', \"he'd\", 'that', 'those', 'full', \"where's\", 'sprung', 'thee', 'hasn', 'seldom', 'up', 'which', 'thereto', 'nevertheless', 'yet', '!', 'stave', 'like', 'unless', 'hindmost', 'towards', 'contrariwise', 'seen', 'with', 'third', 'km', 'hereabouts', 'kind', 'into', 'be', 'wow', 'whichsoever', \"i'm\", 'hereafter', 'whoa', 'double', 'among', 'till', 'well', 'nothing', \"that'll\", 'none', 'any', 'couldn', 'empty', 'see', 'twenty', \"shouldn't\", 'become', 'here', 'was', 'cannot', \"he'll\", 't', 'hereupon', 'since', '-', 'according', 'first', 'because', 'want', \"you'll\", 'anybody', 'hither', 'whereinto', 'somewhere', 'them', 'most', 'it', 'i', 'sang', 'last', 'wheresoever', 'against', 'per', 'wilt', 'he', 'is', '.', 'sometime', 'albeit', 'four', \"needn't\", 'nor', \"should've\", 'supposing', 'cu', 'seemed', 'two', 'kg', 'wherefrom', 'inward', 'namely', 'instead', 'd', 'staves', 'doth', 've', 'cry', 'ff', \"you're\", 'won', 'thenceforth', 'were', \"isn't\", ',', 'mustn', 'whatever', 'if', 'another', 'mostly', 'whomever', \"she's\", 'wherein', 'our', 'really', 'behind', 'farther', 'amoungst', 'always', \"i'd\", 'we', 'us', 'above', 'describe', \"hadn't\", '\"', 'provide', 'would', 'dost', 'whom', 'spake', 'five', 'day', 'seems', 'thereafter', 'week', 'of', 'shalt', 'wherefore', 'hundred', 'others', 'its', \"why's\", 'couldnt', \"''\", 'at', 'noone', 'ourselves', 'by', 'sprang', 'using', 'serious', 'ie', 'forty', 'neither', 'done', 'may', 'everything', 'beyond', 'herself', 'choose', 'whence', 'nobody', 'six', 'through', 'mill', 'next', 'shan', 'am', 'cf', 'both', 'once', 'yourselves', \"they'll\", 'round', \"he's\", 'certain', 'dual', 'whoever', 'whereat', 'twelve', 'hereby', 'whither', \"aren't\", 'before', 'meantime', 'isn', 'therein', 'less', 'saw', 'co', 'doesn', 'everybody', 'unlike', 'whew', 'spoken', 'whereafter', 'although', 'far', 'av', 'almost', 'inwards', 'together', 'wasn', 'nine', 'via', 'whereby', 'thrice', 'already', 'made', 'somehow', 'de', 'upon', 'ye', 'too', 'whereon', 'part', 'mr', 'whereof', 'this', '*', 'been', 'selves', \"she'll\", 'wherewith', 'hast', 'apart', 'sometimes', 'some', \"how's\", 'whereunto', 'becomes', 'needn', 'having', 'whose', \"couldn't\", 'including', 'has', 'inside', 'often', 'never', 'hardly', \"they'd\", 'anyway', 'all', 'how', 'have', '``', 'herein', 'while', 'every', \"they're\", 'go', 'back', 'for', 'became', 'itself', 'own', 'excluding', 'eleven', \"don't\", 'top', 'thereof', 'slung', 'whereabouts', 'who', 'something', 'bill', 'whomsoever', 'forward', \"didn't\", \"mustn't\", 'aren', 'forth', 'across', 'ain', 'whenever', 'spoke', 'yourself', 'ugh', 'in', 'myself', 'cos', 'enough', 'much', 'beforehand', 'three', 'wherever', \"here's\", 'several', 'furthermore', 'said', 'however', 'exception', \"we'd\", '+', 'whether', 'must', 'there', 'perhaps', 'during', 'just', \"you'd\", 'ok', \"who's\", \"wasn't\", 'many', \"you've\", 'canst', 'underneath', 'didn', 'fire', 'excepted', 'fifteen', 'thereabout', 'thyself', 'unable', \"it's\", 'plenty', 'don', 'very', 'eg', 'outside', 'shown', 'rather', \"haven't\", 'somewhat', 'detail', 'latterly', 'please', 'smote', 'sincere', 'whensoever', 'anyone', \"what's\", 'without', '`', 'ever', 'meanwhile', 'can', 'about', 'again', 'm', 'further', 'could', 'system', 'anywhere', 'same', \"won't\", 'keep', 'interest', 'whole', 'upwards', 'upward', 'out', 'furthest', 'moreover', 'should', 'etc', 'henceforth', 'but', 'does', 'formerly', 'his', 'on', 'front', 's', 'un', 'after', 'et', 'within', 'along', 'theirs', 'nonetheless', 'no', 'worst', 'not', 'give', 'indeed', 'due', 'fifty', 'include', \"there's\", 'hence', 'themselves', 'might', \"we've\", 'hers', 'o', 'alone', 'indoors', 'than', 'whereto', 'worse', 'need', 'seem', \"weren't\", 'their', 'will', 'besides', 'your', 'thereon', 'ms', 'hath', 'beside', 'whereupon', 'used', 'little', 'either', 'll', 'thick', 'throughout', \"mightn't\", 'do', 'show', 'becoming', \"we're\", 'save', \"let's\", 'everyone', 'more', \"shan't\", 'other', 'amongst', 'the', 'whosoever', 'move', 'then', 'she', 'her', 'hadn', 'these', 'himself', 'use', 'haven', 'though', 'latter', 'off', \"'\", 'insomuch', 'therefore', 'excepting', 'amount', 'being', 'exclude', 'few', 'thereabouts', 'whilst', 'notwithstanding', 'slew', 'such', 'someone', \"that's\", 'you', 'below', 'everywhere', 'around', 'name', 'my', 're', 'they', 'least', 'bottom', 'still', 'quite']\n"
     ]
    }
   ],
   "source": [
    "# COMMON STOPWORDS\n",
    "from sklearn.feature_extraction import _stop_words    \n",
    "from nltk.corpus import stopwords                    \n",
    " \n",
    "print('Sklearn:')\n",
    "stopwords_sklearn = list(_stop_words.ENGLISH_STOP_WORDS)        # 318 words\n",
    "print(len(stopwords_sklearn))\n",
    "print(stopwords_sklearn)\n",
    "\n",
    "print('\\nNLTK:')\n",
    "stopwords_nltk = list(stopwords.words('english'))              # 180 words\n",
    "print(len(stopwords_nltk))\n",
    "print(stopwords_nltk)\n",
    "\n",
    "print('\\nLemur')                                               # 430 words\n",
    "stopwords_lemur = []\n",
    "with open('data/lemur_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords_lemur.append(line)\n",
    "print(len(stopwords_lemur))\n",
    "print(stopwords_lemur)\n",
    "\n",
    "print('\\nOther:')                                              # 153 words\n",
    "stopwords_other = [ \"i\", \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "print(len(stopwords_other))\n",
    "print(stopwords_other)\n",
    "\n",
    "print('\\nCOMBINED:')                                           # 579 words\n",
    "stopwords_combined = list(set(stopwords_sklearn + stopwords_nltk + stopwords_lemur + stopwords_other))\n",
    "print(len(stopwords_combined))\n",
    "print(stopwords_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4652e9",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 41) (208, 54)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/df_train.pkl'\n",
    "df_train = pd.read_pickle(file1)\n",
    "\n",
    "file2    = 'data/df_dev.pkl'\n",
    "df_dev   = pd.read_pickle(file2)\n",
    "\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc8d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare additional text columns (tsk = title, summary, keywords)\n",
    "df_train['essay_clean_tsk'] = df_train['gpt4_title'] + '. ' + df_train['gpt4_summary'] + ' ' +\\\n",
    "                                           df_train['gpt4_keywords'] + ' ' +\\\n",
    "                                           df_train['essay_clean']\n",
    "df_train['essay_clean_ts']  = df_train['gpt4_title'] + '. ' + df_train['gpt4_summary'] + ' ' +\\\n",
    "                                           df_train['essay_clean']\n",
    "df_train['title_summary_keywords']       = df_train['gpt4_title'] + '. ' + df_train['gpt4_summary'] + ' ' +\\\n",
    "                                           df_train['gpt4_keywords']\n",
    "df_train['title_summary']                = df_train['gpt4_title'] + '. ' + df_train['gpt4_summary']\n",
    "\n",
    "\n",
    "\n",
    "df_dev['essay_clean_tsk'] = df_dev['gpt4_title'] + '. ' + df_dev['gpt4_summary'] + ' ' +\\\n",
    "                                         df_dev['gpt4_keywords'] + ' ' +\\\n",
    "                                         df_dev['essay_clean']\n",
    "df_dev['essay_clean_ts']  = df_dev['gpt4_title'] + '. ' + df_dev['gpt4_summary'] + ' ' +\\\n",
    "                                         df_dev['essay_clean']\n",
    "df_dev['title_summary_keywords']       = df_dev['gpt4_title'] + '. ' + df_dev['gpt4_summary'] + ' ' +\\\n",
    "                                         df_dev['gpt4_keywords']\n",
    "df_dev['title_summary']                = df_dev['gpt4_title'] + '. ' + df_dev['gpt4_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c8de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.\n",
      "Empathy for Hurricane Victims and Hope for Aid Distribution. The text expresses sadness at seeing people living in poor conditions due to a storm, and hopes that aid reaches those in need. The author empathizes with the affected individuals, acknowledging the potential feelings of depression and hopelessness that come with such a life-changing event. breaks my heart, people living, conditions, aid, island, need, family, depressing, hopeless, whole life changed, storm, next meal It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.\n",
      "Empathy for Hurricane Victims and Hope for Aid Distribution. The text expresses sadness at seeing people living in poor conditions due to a storm, and hopes that aid reaches those in need. The author empathizes with the affected individuals, acknowledging the potential feelings of depression and hopelessness that come with such a life-changing event. It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.\n",
      "Empathy for Hurricane Victims and Hope for Aid Distribution. The text expresses sadness at seeing people living in poor conditions due to a storm, and hopes that aid reaches those in need. The author empathizes with the affected individuals, acknowledging the potential feelings of depression and hopelessness that come with such a life-changing event. breaks my heart, people living, conditions, aid, island, need, family, depressing, hopeless, whole life changed, storm, next meal\n",
      "Empathy for Hurricane Victims and Hope for Aid Distribution. The text expresses sadness at seeing people living in poor conditions due to a storm, and hopes that aid reaches those in need. The author empathizes with the affected individuals, acknowledging the potential feelings of depression and hopelessness that come with such a life-changing event.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\n",
      "Addressing the Challenges of Aid Distribution in Haiti. The text expresses concern about the lack of help for people in Haiti, acknowledging the country's challenges with poverty and corruption. The author understands the looting of supplies due to hunger and emphasizes the need to find alternative ways to distribute food and water. people, help, Haiti, richest, corrupt country, supplies, looted, crowds, hungry, food, water, survive, distribute I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\n",
      "Addressing the Challenges of Aid Distribution in Haiti. The text expresses concern about the lack of help for people in Haiti, acknowledging the country's challenges with poverty and corruption. The author understands the looting of supplies due to hunger and emphasizes the need to find alternative ways to distribute food and water. I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\n",
      "Addressing the Challenges of Aid Distribution in Haiti. The text expresses concern about the lack of help for people in Haiti, acknowledging the country's challenges with poverty and corruption. The author understands the looting of supplies due to hunger and emphasizes the need to find alternative ways to distribute food and water. people, help, Haiti, richest, corrupt country, supplies, looted, crowds, hungry, food, water, survive, distribute\n",
      "Addressing the Challenges of Aid Distribution in Haiti. The text expresses concern about the lack of help for people in Haiti, acknowledging the country's challenges with poverty and corruption. The author understands the looting of supplies due to hunger and emphasizes the need to find alternative ways to distribute food and water.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "After reading the article, you can't help but feel really sad and terrible for the people that were affected by the hurricane. It was a situation that they did not deserve and one that they most likely did not cause but mother nature has other plans for us. I feel bad for all the children as well as animals that are there as well with no shelter or food.\n",
      "The Emotional Impact of Hurricane Devastation on People and Animals. The text expresses sadness and sympathy for the people, children, and animals affected by a hurricane, acknowledging that they did not deserve the situation and were likely not responsible for it. reading, article, sad, terrible, people, affected, hurricane, situation, deserve, mother nature, plans, children, animals, shelter, food After reading the article, you can't help but feel really sad and terrible for the people that were affected by the hurricane. It was a situation that they did not deserve and one that they most likely did not cause but mother nature has other plans for us. I feel bad for all the children as well as animals that are there as well with no shelter or food.\n",
      "The Emotional Impact of Hurricane Devastation on People and Animals. The text expresses sadness and sympathy for the people, children, and animals affected by a hurricane, acknowledging that they did not deserve the situation and were likely not responsible for it. After reading the article, you can't help but feel really sad and terrible for the people that were affected by the hurricane. It was a situation that they did not deserve and one that they most likely did not cause but mother nature has other plans for us. I feel bad for all the children as well as animals that are there as well with no shelter or food.\n",
      "The Emotional Impact of Hurricane Devastation on People and Animals. The text expresses sadness and sympathy for the people, children, and animals affected by a hurricane, acknowledging that they did not deserve the situation and were likely not responsible for it. reading, article, sad, terrible, people, affected, hurricane, situation, deserve, mother nature, plans, children, animals, shelter, food\n",
      "The Emotional Impact of Hurricane Devastation on People and Animals. The text expresses sadness and sympathy for the people, children, and animals affected by a hurricane, acknowledging that they did not deserve the situation and were likely not responsible for it.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "It is so sad that someone who had such an amazing story died in such a freak accident. His life was filled with amazing triumphs only for him to die in such a sad way. It is truly heart breaking to think about. He came from nothing and truly got the american dream. He died in such a rare and crazy way that it is so sad.\n",
      "Tragic End to an Inspiring American Dream Story. The text expresses sadness over the tragic death of a person who had an incredible life story and achieved the American dream, only to die in a rare and unexpected accident. sad, amazing story, freak accident, life, amazing triumphs, die, heart breaking, came from nothing, American dream, rare, crazy way It is so sad that someone who had such an amazing story died in such a freak accident. His life was filled with amazing triumphs only for him to die in such a sad way. It is truly heart breaking to think about. He came from nothing and truly got the american dream. He died in such a rare and crazy way that it is so sad.\n",
      "Tragic End to an Inspiring American Dream Story. The text expresses sadness over the tragic death of a person who had an incredible life story and achieved the American dream, only to die in a rare and unexpected accident. It is so sad that someone who had such an amazing story died in such a freak accident. His life was filled with amazing triumphs only for him to die in such a sad way. It is truly heart breaking to think about. He came from nothing and truly got the american dream. He died in such a rare and crazy way that it is so sad.\n",
      "Tragic End to an Inspiring American Dream Story. The text expresses sadness over the tragic death of a person who had an incredible life story and achieved the American dream, only to die in a rare and unexpected accident. sad, amazing story, freak accident, life, amazing triumphs, die, heart breaking, came from nothing, American dream, rare, crazy way\n",
      "Tragic End to an Inspiring American Dream Story. The text expresses sadness over the tragic death of a person who had an incredible life story and achieved the American dream, only to die in a rare and unexpected accident.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "From reading the article, it looks like the world lost a kindhearted and generous person. If no drugs or alcohol were involved in the accident. I wonder what happen to make them crash. I wonder if it was common to be on the boat with no life jacket. The life jacket may not even mattered because of the speed and the rocks.\n",
      "\"Tragic Boat Accident: Reflecting on the Loss of a Kindhearted Individual and the Importance of Safety Measures\". The text discusses the loss of a kindhearted and generous person in an accident, speculating on the cause of the crash and the absence of life jackets. The effectiveness of life jackets in this situation is questioned due to the speed and presence of rocks. reading, article, world, lost, kindhearted, generous person, drugs, alcohol, accident, crash, boat, life jacket, speed, rocks From reading the article, it looks like the world lost a kindhearted and generous person. If no drugs or alcohol were involved in the accident. I wonder what happen to make them crash. I wonder if it was common to be on the boat with no life jacket. The life jacket may not even mattered because of the speed and the rocks.\n",
      "\"Tragic Boat Accident: Reflecting on the Loss of a Kindhearted Individual and the Importance of Safety Measures\". The text discusses the loss of a kindhearted and generous person in an accident, speculating on the cause of the crash and the absence of life jackets. The effectiveness of life jackets in this situation is questioned due to the speed and presence of rocks. From reading the article, it looks like the world lost a kindhearted and generous person. If no drugs or alcohol were involved in the accident. I wonder what happen to make them crash. I wonder if it was common to be on the boat with no life jacket. The life jacket may not even mattered because of the speed and the rocks.\n",
      "\"Tragic Boat Accident: Reflecting on the Loss of a Kindhearted Individual and the Importance of Safety Measures\". The text discusses the loss of a kindhearted and generous person in an accident, speculating on the cause of the crash and the absence of life jackets. The effectiveness of life jackets in this situation is questioned due to the speed and presence of rocks. reading, article, world, lost, kindhearted, generous person, drugs, alcohol, accident, crash, boat, life jacket, speed, rocks\n",
      "\"Tragic Boat Accident: Reflecting on the Loss of a Kindhearted Individual and the Importance of Safety Measures\". The text discusses the loss of a kindhearted and generous person in an accident, speculating on the cause of the crash and the absence of life jackets. The effectiveness of life jackets in this situation is questioned due to the speed and presence of rocks.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "That's sad. Regardless of what they find out happened, who was controlling what or if they had drugs in their system or whatever, it's sad. I don't know that they will find out anything, i just feel like lots of people will turn this into something it's not. It's unfortunate anytime a young person like this, with the world at their fingertips, loses their life in something that was controllable.\n",
      "Tragic Loss of a Young Life: Beyond Speculations and Blame. The text expresses sadness over a young person's death, regardless of the circumstances or factors involved. The author is concerned that people might misinterpret the situation and emphasizes the tragedy of losing a life that had potential. sad, find out, happened, controlling, drugs, system, young person, world, fingertips, loses, life, controllable, unfortunate That's sad. Regardless of what they find out happened, who was controlling what or if they had drugs in their system or whatever, it's sad. I don't know that they will find out anything, i just feel like lots of people will turn this into something it's not. It's unfortunate anytime a young person like this, with the world at their fingertips, loses their life in something that was controllable.\n",
      "Tragic Loss of a Young Life: Beyond Speculations and Blame. The text expresses sadness over a young person's death, regardless of the circumstances or factors involved. The author is concerned that people might misinterpret the situation and emphasizes the tragedy of losing a life that had potential. That's sad. Regardless of what they find out happened, who was controlling what or if they had drugs in their system or whatever, it's sad. I don't know that they will find out anything, i just feel like lots of people will turn this into something it's not. It's unfortunate anytime a young person like this, with the world at their fingertips, loses their life in something that was controllable.\n",
      "Tragic Loss of a Young Life: Beyond Speculations and Blame. The text expresses sadness over a young person's death, regardless of the circumstances or factors involved. The author is concerned that people might misinterpret the situation and emphasizes the tragedy of losing a life that had potential. sad, find out, happened, controlling, drugs, system, young person, world, fingertips, loses, life, controllable, unfortunate\n",
      "Tragic Loss of a Young Life: Beyond Speculations and Blame. The text expresses sadness over a young person's death, regardless of the circumstances or factors involved. The author is concerned that people might misinterpret the situation and emphasizes the tragedy of losing a life that had potential.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "After reading the article, my reaction is that it is very sad that boys that young have to be put behind bars. I think that children should be able to experience their childhood and have fun at that age. They should not be facing hardships at all. They should be playing with friends and be in school at that age and not locked up behind a cell.\n",
      "The Tragedy of Incarcerating Young Boys. The text expresses sadness about young boys being put behind bars, emphasizing that children should be enjoying their childhood, playing with friends, and attending school instead of facing hardships and being locked up. reading, article, reaction, sad, boys, young, behind bars, children, experience, childhood, fun, age, facing hardships, playing, friends, school, locked up, cell After reading the article, my reaction is that it is very sad that boys that young have to be put behind bars. I think that children should be able to experience their childhood and have fun at that age. They should not be facing hardships at all. They should be playing with friends and be in school at that age and not locked up behind a cell.\n",
      "The Tragedy of Incarcerating Young Boys. The text expresses sadness about young boys being put behind bars, emphasizing that children should be enjoying their childhood, playing with friends, and attending school instead of facing hardships and being locked up. After reading the article, my reaction is that it is very sad that boys that young have to be put behind bars. I think that children should be able to experience their childhood and have fun at that age. They should not be facing hardships at all. They should be playing with friends and be in school at that age and not locked up behind a cell.\n",
      "The Tragedy of Incarcerating Young Boys. The text expresses sadness about young boys being put behind bars, emphasizing that children should be enjoying their childhood, playing with friends, and attending school instead of facing hardships and being locked up. reading, article, reaction, sad, boys, young, behind bars, children, experience, childhood, fun, age, facing hardships, playing, friends, school, locked up, cell\n",
      "The Tragedy of Incarcerating Young Boys. The text expresses sadness about young boys being put behind bars, emphasizing that children should be enjoying their childhood, playing with friends, and attending school instead of facing hardships and being locked up.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "It sounds like these boys had a really rough life. I do think we all have personal responsibility for our choices at the end of the day though. Even though you might have it rough, ultimately it is up to you to decide to break the law or use drugs or not. So they had free will too and could have exercised that. Regardless, it is still sad that they went through a rough childhood. Nobody should have to endure that and kids are the saddest victims.\n",
      "Overcoming a Rough Childhood: Personal Responsibility and Free Will. The text discusses the difficult lives of some boys and emphasizes the importance of personal responsibility in making choices. Despite their rough upbringing, they still had free will to decide whether to break the law or use drugs. The text also expresses sympathy for their situation, as no child should have to endure such hardships. sounds, boys, rough life, personal responsibility, choices, end of the day, rough, ultimately, decide, break the law, use drugs, free will, exercised, sad, rough childhood, endure, kids, saddest victims It sounds like these boys had a really rough life. I do think we all have personal responsibility for our choices at the end of the day though. Even though you might have it rough, ultimately it is up to you to decide to break the law or use drugs or not. So they had free will too and could have exercised that. Regardless, it is still sad that they went through a rough childhood. Nobody should have to endure that and kids are the saddest victims.\n",
      "Overcoming a Rough Childhood: Personal Responsibility and Free Will. The text discusses the difficult lives of some boys and emphasizes the importance of personal responsibility in making choices. Despite their rough upbringing, they still had free will to decide whether to break the law or use drugs. The text also expresses sympathy for their situation, as no child should have to endure such hardships. It sounds like these boys had a really rough life. I do think we all have personal responsibility for our choices at the end of the day though. Even though you might have it rough, ultimately it is up to you to decide to break the law or use drugs or not. So they had free will too and could have exercised that. Regardless, it is still sad that they went through a rough childhood. Nobody should have to endure that and kids are the saddest victims.\n",
      "Overcoming a Rough Childhood: Personal Responsibility and Free Will. The text discusses the difficult lives of some boys and emphasizes the importance of personal responsibility in making choices. Despite their rough upbringing, they still had free will to decide whether to break the law or use drugs. The text also expresses sympathy for their situation, as no child should have to endure such hardships. sounds, boys, rough life, personal responsibility, choices, end of the day, rough, ultimately, decide, break the law, use drugs, free will, exercised, sad, rough childhood, endure, kids, saddest victims\n",
      "Overcoming a Rough Childhood: Personal Responsibility and Free Will. The text discusses the difficult lives of some boys and emphasizes the importance of personal responsibility in making choices. Despite their rough upbringing, they still had free will to decide whether to break the law or use drugs. The text also expresses sympathy for their situation, as no child should have to endure such hardships.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "This is a tragic and sad story about how some children can experience the foster care system. Shelton bounced from one home to another, getting into trouble along the way, before beginning a life of crime and going to prison as a young adult and then as an adult. Given the way he was raised, it is almost impossible to imagine an alternate ending.\n",
      "The Tragic Journey of a Foster Child: From Unstable Homes to a Life of Crime. The text narrates a tragic story of a child in the foster care system who constantly moved between homes, got into trouble, and eventually ended up in prison as a young adult and later as an adult, with no alternate outcome seeming possible due to his upbringing. tragic, sad story, children, foster care system, Shelton, one home to another, trouble, life of crime, prison, young adult, adult, raised, impossible, alternate ending This is a tragic and sad story about how some children can experience the foster care system. Shelton bounced from one home to another, getting into trouble along the way, before beginning a life of crime and going to prison as a young adult and then as an adult. Given the way he was raised, it is almost impossible to imagine an alternate ending.\n",
      "The Tragic Journey of a Foster Child: From Unstable Homes to a Life of Crime. The text narrates a tragic story of a child in the foster care system who constantly moved between homes, got into trouble, and eventually ended up in prison as a young adult and later as an adult, with no alternate outcome seeming possible due to his upbringing. This is a tragic and sad story about how some children can experience the foster care system. Shelton bounced from one home to another, getting into trouble along the way, before beginning a life of crime and going to prison as a young adult and then as an adult. Given the way he was raised, it is almost impossible to imagine an alternate ending.\n",
      "The Tragic Journey of a Foster Child: From Unstable Homes to a Life of Crime. The text narrates a tragic story of a child in the foster care system who constantly moved between homes, got into trouble, and eventually ended up in prison as a young adult and later as an adult, with no alternate outcome seeming possible due to his upbringing. tragic, sad story, children, foster care system, Shelton, one home to another, trouble, life of crime, prison, young adult, adult, raised, impossible, alternate ending\n",
      "The Tragic Journey of a Foster Child: From Unstable Homes to a Life of Crime. The text narrates a tragic story of a child in the foster care system who constantly moved between homes, got into trouble, and eventually ended up in prison as a young adult and later as an adult, with no alternate outcome seeming possible due to his upbringing.\n",
      "\n",
      " ============================================================================= \n",
      "\n",
      "Hello. I feel really terrible about the current coal mining situation in India. Many lives have been lost, properties destroyed, so many losses for the people of that country. It's really pathetic how the government has failed in their duty to protect the citizens. It's even more worrisome that they have also failed to compensate the victims. It's a pity\n",
      "The Coal Mining Crisis in India: Government's Failure to Protect and Compensate Citizens. The text expresses concern over the coal mining situation in India, highlighting the loss of lives, property damage, and the government's failure to protect citizens and compensate victims. coal mining, India, lives lost, properties destroyed, losses, government, duty, protect citizens, compensate, victims, pathetic, worrisome, pity Hello. I feel really terrible about the current coal mining situation in India. Many lives have been lost, properties destroyed, so many losses for the people of that country. It's really pathetic how the government has failed in their duty to protect the citizens. It's even more worrisome that they have also failed to compensate the victims. It's a pity\n",
      "The Coal Mining Crisis in India: Government's Failure to Protect and Compensate Citizens. The text expresses concern over the coal mining situation in India, highlighting the loss of lives, property damage, and the government's failure to protect citizens and compensate victims. Hello. I feel really terrible about the current coal mining situation in India. Many lives have been lost, properties destroyed, so many losses for the people of that country. It's really pathetic how the government has failed in their duty to protect the citizens. It's even more worrisome that they have also failed to compensate the victims. It's a pity\n",
      "The Coal Mining Crisis in India: Government's Failure to Protect and Compensate Citizens. The text expresses concern over the coal mining situation in India, highlighting the loss of lives, property damage, and the government's failure to protect citizens and compensate victims. coal mining, India, lives lost, properties destroyed, losses, government, duty, protect citizens, compensate, victims, pathetic, worrisome, pity\n",
      "The Coal Mining Crisis in India: Government's Failure to Protect and Compensate Citizens. The text expresses concern over the coal mining situation in India, highlighting the loss of lives, property damage, and the government's failure to protect citizens and compensate victims.\n",
      "\n",
      " ============================================================================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify prepared text columns\n",
    "temp = df_train[['essay_clean', 'essay_clean_tsk',\n",
    "       'essay_clean_ts', 'title_summary_keywords',\n",
    "       'title_summary']]\n",
    "for a, b, c, d, e in temp.values[:10]:\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(e)\n",
    "    print('\\n', '='*77, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be3d988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column: title_summary_keywords\n"
     ]
    }
   ],
   "source": [
    "candidate_cols = [ 'essay_clean', 'essay_clean_tsk',\n",
    "                   'essay_clean_ts', 'title_summary_keywords',\n",
    "                   'title_summary', ]\n",
    "text_col    = candidate_cols[3]\n",
    "target_col  = 'distress'\n",
    "print('Text column:', text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (792,) (792,) (208,) (208,)\n"
     ]
    }
   ],
   "source": [
    "# for testing on training set\n",
    "X_train = df_train[text_col].values\n",
    "y_train = df_train[target_col].values\n",
    "\n",
    "X_dev = df_dev[text_col].values\n",
    "y_dev = df_dev[target_col].values\n",
    "\n",
    "#X_train, y_train = sklearn.utils.shuffle( X_train, y_train, random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e01147",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_rf = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'entropy',                         # “gini”, “entropy”\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'auto',                      # “auto”, “sqrt”, “log2”\n",
    "    'class_weight': None,                        # dict, 'balanced', 'balanced_subsample', None\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df367a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_xgb_word = {\n",
    "    'n_estimators': 145,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.3,    # 0.3 is close too          # eta\n",
    "    'objective': 'reg:squarederror',                         # multi:softmax, multi:softprob, rank:pairwise\n",
    "    'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "    'base_score': 0.5,\n",
    "    'booster': 'gbtree',                                  # gbtree, dart\n",
    "    'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "    'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "    'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "    'reg_alpha': 0,                                       # L1 reg., larger - more conservative\n",
    "    'reg_lambda': 1,                                      # L2 rreg., larger - more conservative\n",
    "    'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "    'max_delta_step': 1,                                  # 1-10\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.7,           # 0.7                                # 0-1    \n",
    "    'colsample_bylevel': 0.28,   #0.28 (0.5342)                  # 0-1\n",
    "    'colsample_bynode': 1.0,  #0.28 (0.5342, thres=0.26)      # optimized for higher recall\n",
    "    'colsample_bytree': 1.0,                                    # 0-1  \n",
    "    'seed': 2,\n",
    "    #'use_label_encoder': False,\n",
    "    'random_state': random_state,\n",
    "    'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'word',\n",
    "    'ngram_range': (1,3),\n",
    "    'binary': True,\n",
    "    'stop_words': stopwords_combined,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54c3588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier( **clf_params_rf )\n",
    "clf = XGBRegressor( **clf_params_xgb_word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(binary=True, ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;a&#x27;, &#x27;cant&#x27;, &#x27;inc&#x27;, &#x27;somebody&#x27;,\n",
       "                                             &#x27;hasnt&#x27;, &#x27;spat&#x27;, &#x27;whichever&#x27;,\n",
       "                                             &#x27;else&#x27;, &#x27;call&#x27;, &#x27;or&#x27;, &#x27;did&#x27;,\n",
       "                                             &#x27;former&#x27;, &#x27;what&#x27;, &#x27;ours&#x27;,\n",
       "                                             &#x27;included&#x27;, &#x27;between&#x27;, &#x27;toward&#x27;,\n",
       "                                             &#x27;lest&#x27;, &#x27;mine&#x27;, &#x27;shouldn&#x27;, &#x27;sent&#x27;,\n",
       "                                             &quot;hasn&#x27;t&quot;, &#x27;each&#x27;, &#x27;let&#x27;, &#x27;so&#x27;,\n",
       "                                             &#x27;when&#x27;, &#x27;nowhere&#x27;, &#x27;yippee&#x27;,\n",
       "                                             &#x27;thereupon&#x27;, &#x27;otherwise&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBRegressor(...\n",
       "                              feature_types=None, gamma=0, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                              interaction_constraints=None, learning_rate=0.3,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=1,\n",
       "                              max_depth=6, max_leaves=None, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=145, n_jobs=-1,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=47, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(binary=True, ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;a&#x27;, &#x27;cant&#x27;, &#x27;inc&#x27;, &#x27;somebody&#x27;,\n",
       "                                             &#x27;hasnt&#x27;, &#x27;spat&#x27;, &#x27;whichever&#x27;,\n",
       "                                             &#x27;else&#x27;, &#x27;call&#x27;, &#x27;or&#x27;, &#x27;did&#x27;,\n",
       "                                             &#x27;former&#x27;, &#x27;what&#x27;, &#x27;ours&#x27;,\n",
       "                                             &#x27;included&#x27;, &#x27;between&#x27;, &#x27;toward&#x27;,\n",
       "                                             &#x27;lest&#x27;, &#x27;mine&#x27;, &#x27;shouldn&#x27;, &#x27;sent&#x27;,\n",
       "                                             &quot;hasn&#x27;t&quot;, &#x27;each&#x27;, &#x27;let&#x27;, &#x27;so&#x27;,\n",
       "                                             &#x27;when&#x27;, &#x27;nowhere&#x27;, &#x27;yippee&#x27;,\n",
       "                                             &#x27;thereupon&#x27;, &#x27;otherwise&#x27;, ...])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBRegressor(...\n",
       "                              feature_types=None, gamma=0, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "                              interaction_constraints=None, learning_rate=0.3,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=1,\n",
       "                              max_depth=6, max_leaves=None, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=145, n_jobs=-1,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=47, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, ngram_range=(1, 3),\n",
       "                stop_words=[&#x27;a&#x27;, &#x27;cant&#x27;, &#x27;inc&#x27;, &#x27;somebody&#x27;, &#x27;hasnt&#x27;, &#x27;spat&#x27;,\n",
       "                            &#x27;whichever&#x27;, &#x27;else&#x27;, &#x27;call&#x27;, &#x27;or&#x27;, &#x27;did&#x27;, &#x27;former&#x27;,\n",
       "                            &#x27;what&#x27;, &#x27;ours&#x27;, &#x27;included&#x27;, &#x27;between&#x27;, &#x27;toward&#x27;,\n",
       "                            &#x27;lest&#x27;, &#x27;mine&#x27;, &#x27;shouldn&#x27;, &#x27;sent&#x27;, &quot;hasn&#x27;t&quot;, &#x27;each&#x27;,\n",
       "                            &#x27;let&#x27;, &#x27;so&#x27;, &#x27;when&#x27;, &#x27;nowhere&#x27;, &#x27;yippee&#x27;,\n",
       "                            &#x27;thereupon&#x27;, &#x27;otherwise&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=0.28, colsample_bynode=1.0, colsample_bytree=1.0,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=&#x27;merror&#x27;, feature_types=None, gamma=0, gpu_id=None,\n",
       "             grow_policy=None, importance_type=&#x27;gain&#x27;,\n",
       "             interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=1,\n",
       "             max_depth=6, max_leaves=None, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=145, n_jobs=-1,\n",
       "             num_parallel_tree=None, predictor=None, random_state=47, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(binary=True, ngram_range=(1, 3),\n",
       "                                 stop_words=['a', 'cant', 'inc', 'somebody',\n",
       "                                             'hasnt', 'spat', 'whichever',\n",
       "                                             'else', 'call', 'or', 'did',\n",
       "                                             'former', 'what', 'ours',\n",
       "                                             'included', 'between', 'toward',\n",
       "                                             'lest', 'mine', 'shouldn', 'sent',\n",
       "                                             \"hasn't\", 'each', 'let', 'so',\n",
       "                                             'when', 'nowhere', 'yippee',\n",
       "                                             'thereupon', 'otherwise', ...])),\n",
       "                ('clf',\n",
       "                 XGBRegressor(...\n",
       "                              feature_types=None, gamma=0, gpu_id=None,\n",
       "                              grow_policy=None, importance_type='gain',\n",
       "                              interaction_constraints=None, learning_rate=0.3,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=1,\n",
       "                              max_depth=6, max_leaves=None, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=145, n_jobs=-1,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=47, ...))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = TfidfVectorizer( **vect_params )\n",
    "vectorizer = CountVectorizer( **vect_params )\n",
    "model       = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9856166305669771 0.22630210756004965\n"
     ]
    }
   ],
   "source": [
    "df_train['distress_pred_regressor'] = model.predict(X_train)\n",
    "df_dev['distress_pred_regressor']   = model.predict(X_dev)\n",
    "print( df_train[target_col].corr(df_train['distress_pred_regressor'], method='pearson'),\n",
    "       df_dev[target_col].corr(df_dev['distress_pred_regressor'], method='pearson') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b21a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3c932f",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478901b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "199f3704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.3296804860576358\n",
      "Train Pearson: 0.9996773582290357\n",
      "Param: 1\n",
      "\n",
      "Best Dev Pearson: 0.3297\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.3275640909912672\n",
      "Train Pearson: 0.9997305188634159\n",
      "Param: 2\n",
      "\n",
      "Best Dev Pearson: 0.3297\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.3577123433156057\n",
      "Train Pearson: 0.9998074209934859\n",
      "Param: 3\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.28988069321925447\n",
      "Train Pearson: 0.9998880231808684\n",
      "Param: 4\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.26799356952017594\n",
      "Train Pearson: 0.9998806123704213\n",
      "Param: 5\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.2654923819492749\n",
      "Train Pearson: 0.9999224046462216\n",
      "Param: 6\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.2135686052765268\n",
      "Train Pearson: 0.9999660898271479\n",
      "Param: 7\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.31618665998300477\n",
      "Train Pearson: 0.999945652726585\n",
      "Param: 8\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.2884393498746915\n",
      "Train Pearson: 0.9999707348013293\n",
      "Param: 9\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.27268139825228843\n",
      "Train Pearson: 0.9999682489256385\n",
      "Param: 10\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.26676618558385573\n",
      "Train Pearson: 0.9999811434520202\n",
      "Param: 11\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "Dev Pearson: 0.251629830747169\n",
      "Train Pearson: 0.9999913274280439\n",
      "Param: 15\n",
      "\n",
      "Best Dev Pearson: 0.3577\n",
      "\n",
      "=============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "#params1 = [i/100 for i in range(2,101,2)] + [i for i in range(100,801,25)]\n",
    "#params1 = [i/100 for i in range(2,101,2)]\n",
    "#params1 = [i for i in range(25,401,25)]\n",
    "params1 = [1,2,3,4,5,6,7,8,9,10,11,15]\n",
    "#params1 = [7,8,9,10,11,12,14,15]\n",
    "#params1 = [1]\n",
    "\n",
    "for param1 in params1:\n",
    "    clf_params_xgb_word2 = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 6,          # 3 - 0.5489\n",
    "        'learning_rate': 0.4,    #                            # eta\n",
    "        'objective': 'reg:squarederror',  \n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 0.02,                                       # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1.0,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 1.0,           # 0.9  (0.5638, thres 0.21)     # 0-1    \n",
    "        'colsample_bylevel': 1.0,   #0.55 (0.5741, thres 0.25)     # 0-1\n",
    "        'colsample_bynode': 1.0,                                    # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                                    # 0-1  \n",
    "        'seed': 2,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 0.95,\n",
    "        'min_df': 3,\n",
    "        'analyzer': 'char_wb',\n",
    "        'ngram_range': (1,5),\n",
    "        'binary': True,\n",
    "        'stop_words': stopwords_combined,\n",
    "    }\n",
    "\n",
    "    clf        = XGBRegressor( **clf_params_xgb_word2 )\n",
    "    vectorizer = CountVectorizer( **vect_params2 )\n",
    "    #vectorizer = TfidfVectorizer( **vect_params2 )\n",
    "    model      = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    df_train['distress_pred_regressor'] = model.predict(X_train)\n",
    "    df_dev['distress_pred_regressor']   = model.predict(X_dev)\n",
    "    pearson_train = df_train[target_col].corr(df_train['distress_pred_regressor'], method='pearson')\n",
    "    pearson_dev   = df_dev[target_col].corr(df_dev['distress_pred_regressor'], method='pearson')\n",
    "    print('Dev Pearson:', pearson_dev)\n",
    "    print('Train Pearson:', pearson_train)\n",
    "\n",
    "    res.append(( pearson_dev, pearson_train, param1 ))\n",
    "    print('Param:', param1)\n",
    "    print('\\nBest Dev Pearson:', round(sorted(res, key=lambda x: x[0], reverse=True)[0][0], 4) )\n",
    "    print('\\n', '='*77, '\\n', sep='')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61ad737a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3577123433156057, 0.9998074209934859, 3)\n",
      "(0.3296804860576358, 0.9996773582290357, 1)\n",
      "(0.3275640909912672, 0.9997305188634159, 2)\n",
      "(0.31618665998300477, 0.999945652726585, 8)\n",
      "(0.28988069321925447, 0.9998880231808684, 4)\n",
      "(0.2884393498746915, 0.9999707348013293, 9)\n",
      "(0.27268139825228843, 0.9999682489256385, 10)\n",
      "(0.26799356952017594, 0.9998806123704213, 5)\n",
      "(0.26676618558385573, 0.9999811434520202, 11)\n",
      "(0.2654923819492749, 0.9999224046462216, 6)\n",
      "(0.251629830747169, 0.9999913274280439, 15)\n",
      "(0.2135686052765268, 0.9999660898271479, 7)\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(res, key=lambda x: x[0], reverse=True):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9709a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b7732e",
   "metadata": {},
   "source": [
    "# Best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76e597",
   "metadata": {},
   "source": [
    "'essay_clean'  \n",
    "Best Dev Pearson: 0.4421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdaf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param1 in params1:\n",
    "    clf_params_xgb_word2 = {\n",
    "        'n_estimators': 25,\n",
    "        'max_depth': 6,          # 3 - 0.5489\n",
    "        'learning_rate': 0.4,    #                            # eta\n",
    "        'objective': 'reg:squarederror',  \n",
    "        'eval_metric': 'merror',                              # multiclass - merror, mlogloss\n",
    "        'base_score': 0.5,\n",
    "        'booster': 'gbtree',                                  # gbtree, dart\n",
    "        'tree_method': 'auto',                                # auto, exact, approx, hist and gpu_hist\n",
    "        'importance_type': 'gain',                            # default“gain”,“weight”,“cover”,“total_gain”,“total_cover”\n",
    "        'gamma': 0,                                           # larger - more conservative, [0, inf]\n",
    "        'reg_alpha': 0.02,                                       # L1 reg., larger - more conservative\n",
    "        'reg_lambda': 1.0,                                      # L2 rreg., larger - more conservative\n",
    "        'sampling_method': 'uniform',                         # uniform, gradient_based\n",
    "        'max_delta_step': 1,                                  # 1-10\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 1.0,           # 0.9  (0.5638, thres 0.21)     # 0-1    \n",
    "        'colsample_bylevel': 1.0,   #0.55 (0.5741, thres 0.25)     # 0-1\n",
    "        'colsample_bynode': 1.0,                                    # optimized for higher recall\n",
    "        'colsample_bytree': 1.0,                                    # 0-1  \n",
    "        'seed': 2,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,    \n",
    "}\n",
    "\n",
    "    vect_params2 = {\n",
    "        'max_df': 0.95,\n",
    "        'min_df': 3,\n",
    "        'analyzer': 'char_wb',\n",
    "        'ngram_range': (1,7),\n",
    "        'binary': True,\n",
    "        'stop_words': stopwords_combined,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad5252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
