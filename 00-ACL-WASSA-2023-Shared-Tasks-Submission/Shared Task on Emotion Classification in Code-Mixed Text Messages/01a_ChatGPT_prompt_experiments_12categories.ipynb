{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975703f6",
   "metadata": {},
   "source": [
    "# CLASSIFICATION WITH 12 CATEGORIES\n",
    "## The Association for Computational Linguistics\n",
    "## WASSA 2023 Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages\n",
    "See more details [here](https://codalab.lisn.upsaclay.fr/competitions/10864#learn_the_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "83e22f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re, os\n",
    "import time\n",
    "import zipfile\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "import tiktoken\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "#os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91371be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    '''Return number of tokens used in a list of messages for ChatGPT'''\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        #print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        #print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        #print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56940aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new new version (Dec 2022)\n",
    "def upsample_all( df_, labels_col='target', random_state=47 ):\n",
    "    '''\n",
    "        Upsample each class in column labels_col of pandas dataframe df_\n",
    "        to the number of data points in majority class\n",
    "    '''\n",
    "    # get sub-dataframes for each class & max length\n",
    "    labels = df_[labels_col].unique()\n",
    "    dframes, df_lengths = dict(), dict()\n",
    "    for i in labels:\n",
    "        temp          = df_[ df_[labels_col] == i ]\n",
    "        dframes[i]    = temp.copy()\n",
    "        df_lengths[i] = len(temp)\n",
    "\n",
    "    max_len = max( list(df_lengths.values()) )\n",
    "    df_lengths = {k: max_len-v for k,v in df_lengths.items()}                     # difference - how many to resample\n",
    "\n",
    "    # upsample with replacement to max length\n",
    "    for i in labels:\n",
    "        if df_lengths[i] == max_len:\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # we know it's overrepresented\n",
    "        else:\n",
    "            if len(dframes[i]) >= df_lengths[i]:\n",
    "                replace = False                                                      # enough data points\n",
    "            else:\n",
    "                replace = True\n",
    "            temp = dframes[i].sample( df_lengths[i], replace=replace, random_state=random_state )\n",
    "            dframes[i] = pd.concat( [dframes[i].copy(), temp.copy()] )               # df len + (max_len-df len)\n",
    "            dframes[i] = dframes[i].sample( frac=1, random_state=random_state )      # shuffle\n",
    "\n",
    "    # combine and reshuffle\n",
    "    df_merged = pd.concat( list(dframes.values()) )\n",
    "    df_merged = df_merged.sample( frac=1, random_state=random_state ).reset_index(drop=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbb2ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral',\n",
       " 1: 'joy',\n",
       " 2: 'trust',\n",
       " 3: 'disgust',\n",
       " 4: 'optimism',\n",
       " 5: 'anticipation',\n",
       " 6: 'sadness',\n",
       " 7: 'fear',\n",
       " 8: 'surprise',\n",
       " 9: 'anger',\n",
       " 10: 'pessimism',\n",
       " 11: 'love'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the order of decreasing frequency\n",
    "label2key = {\n",
    "    'neutral': 0,\n",
    "    'joy': 1,\n",
    "    'trust': 2,\n",
    "    'disgust': 3,\n",
    "    'optimism': 4,\n",
    "    'anticipation': 5,\n",
    "    'sadness': 6,\n",
    "    'fear': 7,\n",
    "    'surprise': 8,\n",
    "    'anger': 9,\n",
    "    'pessimism': 10,\n",
    "    'love':  11,\n",
    "}\n",
    "key2label = { v: k for k,v in label2key.items()}\n",
    "key2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfc9a3f",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9530, 2) (1191, 2)\n"
     ]
    }
   ],
   "source": [
    "file1    = 'data/mcec_train.csv'\n",
    "df_train = pd.read_csv(file1)\n",
    "\n",
    "file2    = 'data/mcec_dev.csv'\n",
    "df_dev   = pd.read_csv(file2)\n",
    "\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d76d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral         3262\n",
      "trust           1118\n",
      "joy             1022\n",
      "optimism         880\n",
      "anticipation     832\n",
      "disgust          687\n",
      "sadness          486\n",
      "fear             453\n",
      "anger            226\n",
      "surprise         199\n",
      "love             187\n",
      "pessimism        178\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes.I am in fyp lab cabin.but fyp presentation...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yar insan ka bcha bn chawliyn na mar :p</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terai uncle nai kahna hai kai ham nai to bahr ...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yr ajao I m cming in the club</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion\n",
       "0  Yes.I am in fyp lab cabin.but fyp presentation...  neutral\n",
       "1           Yar insan ka bcha bn chawliyn na mar :p       joy\n",
       "2  Terai uncle nai kahna hai kai ham nai to bahr ...  disgust\n",
       "3                      Yr ajao I m cming in the club  neutral\n",
       "4  Mje wese Nimra ahmad ka Qur'aan ki aayaat k ba...      joy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train['Emotion'].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4a0b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = df_train['Emotion'].map( label2key )\n",
    "df_dev['target']   = df_dev['Emotion'].map( label2key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0f3ff39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets:  (9530,) (9530,) (1191,) (1191,)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['Text'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "X_dev = df_dev['Text'].values\n",
    "y_dev = df_dev['target'].values\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle( X_train, y_train, random_state=random_state, ) \n",
    "print( 'Shape of datasets: ', X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca84cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76dbc948",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c3541e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params_nb = {\n",
    "    'alpha': 1.0,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "clf_params_lr = {\n",
    "    'C': 1.0,\n",
    "    'solver': 'liblinear',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter': 500,\n",
    "    'random_state': random_state,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e92e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_params = {\n",
    "    'max_df': 1.0,\n",
    "    'min_df': 1,\n",
    "    'analyzer': 'word',\n",
    "    'ngram_range': (1,1),\n",
    "    'binary': False,\n",
    "    'stop_words': 'english',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6fe842a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=47,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=47,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, random_state=47, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(stop_words='english')),\n",
       "                ('clf',\n",
       "                 LogisticRegression(max_iter=500, random_state=47,\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer( **vect_params )\n",
    "#vectorizer = CountVectorizer( **vect_params )\n",
    "\n",
    "#clf = MultinomialNB( **clf_params_nb )\n",
    "clf = LogisticRegression( **clf_params_lr )\n",
    "\n",
    "\n",
    "model = Pipeline( steps=[('vect', vectorizer), ('clf', clf)] )\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a18eef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['neutral', 'joy', 'trust', 'disgust', 'optimism', 'anticipation', 'sadness', 'fear', 'surprise', 'anger', 'pessimism', 'love']\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_dev   = model.predict(X_dev)\n",
    "\n",
    "# add prediction to dataframe\n",
    "df_dev['clf_pred'] = y_pred_dev\n",
    "df_dev['clf_pred_emotion'] = df_dev['clf_pred'].map( key2label )\n",
    "\n",
    "# labels for classification report\n",
    "labels = list(label2key.keys())\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cfd5f419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "TfidfVectorizer(stop_words='english')\n",
      "\n",
      "Classifier:\n",
      "LogisticRegression(max_iter=500, random_state=47, solver='liblinear')\n",
      "\n",
      "\n",
      "TRAINSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5001    0.9700    0.6599      3262\n",
      "         joy     0.8374    0.5744    0.6814      1022\n",
      "       trust     0.8073    0.6297    0.7075      1118\n",
      "     disgust     0.8831    0.3188    0.4684       687\n",
      "    optimism     0.7435    0.5500    0.6323       880\n",
      "anticipation     0.8594    0.3305    0.4774       832\n",
      "     sadness     0.8992    0.2387    0.3772       486\n",
      "        fear     0.9236    0.2936    0.4456       453\n",
      "    surprise     1.0000    0.0653    0.1226       199\n",
      "       anger     0.9737    0.1637    0.2803       226\n",
      "   pessimism     1.0000    0.1124    0.2020       178\n",
      "        love     0.8657    0.3102    0.4567       187\n",
      "\n",
      "    accuracy                         0.6097      9530\n",
      "   macro avg     0.8577    0.3798    0.4593      9530\n",
      "weighted avg     0.7324    0.6097    0.5782      9530\n",
      "\n",
      "DEVSET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4066    0.9149    0.5630       388\n",
      "         joy     0.7183    0.3893    0.5050       131\n",
      "       trust     0.5960    0.4720    0.5268       125\n",
      "     disgust     0.6538    0.1504    0.2446       113\n",
      "    optimism     0.6620    0.4273    0.5193       110\n",
      "anticipation     0.5152    0.1809    0.2677        94\n",
      "     sadness     0.7500    0.0968    0.1714        62\n",
      "        fear     0.6667    0.0769    0.1379        52\n",
      "    surprise     0.0000    0.0000    0.0000        35\n",
      "       anger     1.0000    0.0286    0.0556        35\n",
      "   pessimism     0.0000    0.0000    0.0000        29\n",
      "        love     1.0000    0.1765    0.3000        17\n",
      "\n",
      "    accuracy                         0.4702      1191\n",
      "   macro avg     0.5807    0.2428    0.2743      1191\n",
      "weighted avg     0.5497    0.4702    0.4074      1191\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrew/opt/anaconda3/envs/top/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# print classification reports\n",
    "print('Vectorizer:\\n', model['vect'], '\\n', sep='')\n",
    "print('Classifier:\\n', model['clf'], '\\n', sep='')\n",
    "\n",
    "print('\\nTRAINSET')\n",
    "print( classification_report( y_train, y_pred_train, target_names=labels, digits=4 ) )\n",
    "\n",
    "print('DEVSET')\n",
    "print( classification_report( y_dev, y_pred_dev, target_names=labels, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "36beaba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create zip file for submission\n",
    "file     = 'data/results.csv'\n",
    "zip_file = 'data/results.zip'\n",
    "\n",
    "with open(file, 'w', encoding='utf-8') as f:\n",
    "    f.write( '\\n'.join(df_dev['pred_emotion'].tolist()) )\n",
    "\n",
    "with zipfile.ZipFile(zip_file, \"w\", compression=zipfile.ZIP_STORED) as zf:        # , compression=zipfile.ZIP_DEFLATED\n",
    "    zf.write( file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7a589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396d0fce",
   "metadata": {},
   "source": [
    "# ChatGPT API: Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad9e7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text below may contain words or phrases in Urdu or Roman Urdu along with English. Translate the text below and write it in English only. Output only the English translation and nothing else. Text: \"This is a text sample\" \n",
      "\n",
      "Act as a very accurate zero-shot text classifier and classify the provided text into one most relevant category from the following list of categories: neutral, joy, trust, disgust, optimism, anticipation, sadness, fear, surprise, anger, pessimism, love. Output only one most relevant category and nothing else. Text: \"This is a text sample\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_one   = '''The text below may contain words or phrases in Urdu or Roman Urdu along with English. Translate the text below and write it in English only. Output only the English translation and nothing else. Text: \"{}\"'''\n",
    "prompt_two   = '''Act as a very accurate zero-shot text classifier and classify the provided text into one most relevant category from the following list of categories: neutral, joy, trust, disgust, optimism, anticipation, sadness, fear, surprise, anger, pessimism, love. Output only one most relevant category and nothing else. Text: \"{}\"'''\n",
    "s = 'This is a text sample'\n",
    "print(prompt_one.format(s), '\\n')\n",
    "print(prompt_two.format(s), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc99c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using followup Q1 can improve the reponse. If the reponse has multiple words, first parse it and try to find\n",
    "# the category in it. Only if this doesn't work, send followup Q2. ChatGPT can offer the second category in reponse\n",
    "# to Q1, but can change its mind again and offer a third category if asked Q2\n",
    "followup1 = 'Are you sure about that?'\n",
    "followup2 = 'Output only the category and nothing else'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7583ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimism', 'fear', 'disgust', 'surprise', 'trust', 'love', 'sadness', 'pessimism', 'anticipation', 'anger', 'joy', 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model          = 'gpt-3.5-turbo'\n",
    "labels_set     = set(list(label2key.keys()))\n",
    "clean = re.compile(r'[^a-zA-Z ]+')\n",
    "multi_spaces = re.compile('\\s{2,}')\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee57217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_label(label_):\n",
    "    '''\n",
    "       Verify if label_ contains any of the categories\n",
    "       from the predefined set of labels\n",
    "    '''\n",
    "    label_ = clean.sub(' ', label_)\n",
    "    label_ = multi_spaces.sub(' ', label_).lower().split()\n",
    "    res    = [i for i in label_ if i in labels_set]\n",
    "    res    = list(set(res))\n",
    "    return '/'.join(res) if res else None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0eb629b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text_, prompt_):\n",
    "    '''Translate text_ using prompt_ and ChatGPT API'''    \n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [            \n",
    "            { \"role\": \"system\", \"content\": \"You are a clever translator from Urdu and Roman Urdu.\", },\n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    num_tokens_tiktoken = num_tokens_from_messages(messages, model)\n",
    "    if num_tokens_tiktoken > 3950:\n",
    "        print(f'Number of tokens is {num_tokens_tiktoken} which exceeds 3950')\n",
    "        print(f'TEXT: {text_}\\n')\n",
    "        return None\n",
    "        \n",
    "    # get response\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0,          # range(0,2), the more the less deterministic / focused\n",
    "        top_p = 1,                # top probability mass, e.g. 0.1 = only tokens from top 10% proba mass\n",
    "        n = 1,                    # number of chat completions\n",
    "        #max_tokens = 50,          # tokens to return\n",
    "        stream = False,        \n",
    "        stop=None,                # sequence to stop generation (new line, end of text, etc.)\n",
    "        )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f9fa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text_, prompt_):\n",
    "    '''Classify text_ using prompt_ and ChatGPT API'''    \n",
    "        \n",
    "    # compose messages and check num_tokens\n",
    "    messages = [\n",
    "            { \"role\": \"system\", \"content\": \"You are a very accurate zero-shot text classifier.\", },            \n",
    "            { \"role\": \"user\", \"content\": prompt_.format(text_), },\n",
    "            ]\n",
    "    num_tokens_tiktoken = num_tokens_from_messages(messages, model)\n",
    "    if num_tokens_tiktoken > 3950:\n",
    "        print(f'Number of tokens is {num_tokens_tiktoken} which exceeds 3950')\n",
    "        print(f'TEXT: {text_}\\n')\n",
    "        return None\n",
    "        \n",
    "    # get response\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0,          # range(0,2), the more the less deterministic / focused\n",
    "        top_p = 1,                # top probability mass, e.g. 0.1 = only tokens from top 10% proba mass\n",
    "        n = 1,                    # number of chat completions\n",
    "        max_tokens = 50,          # tokens to return\n",
    "        stream = False,        \n",
    "        stop=None,                # sequence to stop generation (new line, end of text, etc.)\n",
    "        )\n",
    "    label_         = response['choices'][0]['message']['content'].strip()\n",
    "    #num_tokens_api = response['usage']['prompt_tokens']\n",
    "        \n",
    "    # if label > 1 word long OR label has additional characters\n",
    "    old_label = label_\n",
    "    label_    = verify_label(label_)\n",
    "        \n",
    "    # if label not found in response text - second, extended chat\n",
    "    if label_ is None:\n",
    "        new_messages = [\n",
    "            { \"role\": \"assistant\", \"content\": old_label, },\n",
    "            { \"role\": \"user\", \"content\": followup1, }\n",
    "            ]\n",
    "        messages += new_messages\n",
    "        \n",
    "        response  = openai.ChatCompletion.create(\n",
    "            model = model,\n",
    "            messages = messages,\n",
    "            temperature = 0,          # range(0,2), the more the less deterministic / focused\n",
    "            top_p = 1,                # top probability mass, e.g. 0.1 = only tokens from top 10% proba mass\n",
    "            n = 1,                    # number of chat completions\n",
    "            max_tokens = 50,          # tokens to return\n",
    "            stream = False,        \n",
    "            stop=None,                # sequence to stop generation (new line, end of text, etc.)\n",
    "            )\n",
    "        label_         = response['choices'][0]['message']['content'].strip()\n",
    "        #num_tokens_api += response['usage']['prompt_tokens']\n",
    "        \n",
    "        old_label = label_\n",
    "        label_    = verify_label(label_)\n",
    "            \n",
    "    return label_ if label_ is not None else old_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bb18d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text below may contain words or phrases in Urdu or Roman Urdu along with English. Translate the text below and write it in English only. Output only the English translation and nothing else. Text: \"Lakin wo abhe dar dar ka chalata ha\"\n",
      "\n",
      "GROUNDTRUTH LABEL:\n",
      "f/e/a/r\n",
      "\n",
      "PREDICTED LABEL:\n",
      "fear\n"
     ]
    }
   ],
   "source": [
    "# test as single prompt\n",
    "idx = 4\n",
    "text, groundtruth_labels = df_dev[['Text', 'Emotion']].values[idx]\n",
    "label = classify_text(text, prompt_one)\n",
    "\n",
    "print(prompt_one.format( text ))\n",
    "print(f\"\\nGROUNDTRUTH LABEL:\\n{'/'.join( groundtruth_labels )}\")\n",
    "print(f\"\\nPREDICTED LABEL:\\n{label}\")\n",
    "#print(f'\\nTOTAL TOKENS: {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "02a040b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0fba1fd07c4b72a4cf6e076235b3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt 4 tqdm results - 1191/1191 [20:32<00:00, 1.02it/s]\n",
    "df_dev['text_translated'] = df_dev['Text'].progress_apply( lambda x: translate_text(x, prompt_one) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4efc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean translation\n",
    "def clean_translation(s):\n",
    "    s = s.strip()\n",
    "    if s.startswith('\"'):\n",
    "        s = s[1:]\n",
    "    if s.endswith('\"'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "        \n",
    "df_dev['text_translated'] = df_dev['text_translated'].apply( clean_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e957b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91df7ce26934d99890c6c59b17e8a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt 1 tqdm results - 1191/1191 [14:30<00:00, 1.20it/s]\n",
    "# prompt 2 tqdm results - 1191/1191 [14:03<00:00, 1.33it/s]\n",
    "# prompt 3 tqdm results - 1191/1191 [12:01<00:00, 2.07it/s]\n",
    "# prompt 4 tqdm results - 1191/1191 [11:39<00:00, 1.64it/s]\n",
    "\n",
    "df_dev['gpt_pred'] = df_dev['text_translated'].progress_apply( lambda x: classify_text(x, prompt_two) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7dedb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify labels\n",
    "df_dev['gpt_pred2'] = df_dev['gpt_pred'].apply( lambda x: verify_label(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92811b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anticipation     708\n",
       "neutral          118\n",
       "joy               85\n",
       "trust             63\n",
       "sadness           54\n",
       "disgust           42\n",
       "optimism          27\n",
       "fear              27\n",
       "love              23\n",
       "pessimism         18\n",
       "anger             13\n",
       "surprise           2\n",
       "disgust/trust      1\n",
       "anger/trust        1\n",
       "Name: gpt_pred2, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['gpt_pred2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3082d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean some labels\n",
    "df_dev = df_dev.replace('disgust/trust', 'disgust')\n",
    "df_dev = df_dev.replace('anger/trust', 'trust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ca209010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text                0\n",
       "Emotion             0\n",
       "target              0\n",
       "gpt_pred            0\n",
       "text_translated     0\n",
       "text_translated2    0\n",
       "gpt_pred2           9\n",
       "clf_pred            0\n",
       "clf_pred_emotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfbbee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>target</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>text_translated</th>\n",
       "      <th>text_translated2</th>\n",
       "      <th>gpt_pred2</th>\n",
       "      <th>clf_pred</th>\n",
       "      <th>clf_pred_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Wo behan ka lora paper e ni de kar aya tha isl...</td>\n",
       "      <td>anger</td>\n",
       "      <td>9</td>\n",
       "      <td>I apologize for the mistake. The correct categ...</td>\n",
       "      <td>He didn't bring the brother's exam paper, that...</td>\n",
       "      <td>He didn't bring the brother's exam paper, that...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Yar wo to bht purani ha jb tm or m colage m ht...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>3</td>\n",
       "      <td>I apologize for the confusion. Based on the pr...</td>\n",
       "      <td>Friend, that is very old. Do you remember when...</td>\n",
       "      <td>Friend, that is very old. Do you remember when...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Yr mere father chairman thy zakat commitee k u...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>6</td>\n",
       "      <td>I apologize for the mistake. After reviewing t...</td>\n",
       "      <td>My father was the chairman of the Zakat commit...</td>\n",
       "      <td>My father was the chairman of the Zakat commit...</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Thanks sir je. Aj to sir je watt lag gae 3 dut...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes, based on the text \"I am experiencing seve...</td>\n",
       "      <td>Thanks sir. Today, sir, I got stuck with 3 dut...</td>\n",
       "      <td>Thanks sir. Today, sir, I got stuck with 3 dut...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>bhai gggg ma ap ko inthai ahtram sy pucha b th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>I apologize for the mistake. Based on the prov...</td>\n",
       "      <td>Brother, I asked you with great respect and al...</td>\n",
       "      <td>Brother, I asked you with great respect and al...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>Yar wo to bht purani ha jb tm or m colage m ht...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>3</td>\n",
       "      <td>I apologize for the confusion. Based on the pr...</td>\n",
       "      <td>Friend, that is very old. Do you remember when...</td>\n",
       "      <td>Friend, that is very old. Do you remember when...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>Yr jo be ha is tarhn nai krna chia usy.!\\nKya ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>3</td>\n",
       "      <td>I apologize for the mistake. After re-evaluati...</td>\n",
       "      <td>You shouldn't do it in this way, friend! What ...</td>\n",
       "      <td>You shouldn't do it in this way, friend! What ...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>or har time ek e bt krti ho movie movie</td>\n",
       "      <td>disgust</td>\n",
       "      <td>3</td>\n",
       "      <td>I apologize for the confusion. As a zero-shot ...</td>\n",
       "      <td>And she always talks about one thing, movies, ...</td>\n",
       "      <td>And she always talks about one thing, movies, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Yara mobile  py result pata e nai chal raha,,,</td>\n",
       "      <td>disgust</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes, I am quite confident that the most releva...</td>\n",
       "      <td>I am unable to check my exam results on my mob...</td>\n",
       "      <td>I am unable to check my exam results on my mob...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Emotion  target  \\\n",
       "102   Wo behan ka lora paper e ni de kar aya tha isl...    anger       9   \n",
       "269   Yar wo to bht purani ha jb tm or m colage m ht...  disgust       3   \n",
       "311   Yr mere father chairman thy zakat commitee k u...  sadness       6   \n",
       "430   Thanks sir je. Aj to sir je watt lag gae 3 dut...  disgust       3   \n",
       "600   bhai gggg ma ap ko inthai ahtram sy pucha b th...  neutral       0   \n",
       "606   Yar wo to bht purani ha jb tm or m colage m ht...  disgust       3   \n",
       "671   Yr jo be ha is tarhn nai krna chia usy.!\\nKya ...  disgust       3   \n",
       "823             or har time ek e bt krti ho movie movie  disgust       3   \n",
       "1054     Yara mobile  py result pata e nai chal raha,,,  disgust       3   \n",
       "\n",
       "                                               gpt_pred  \\\n",
       "102   I apologize for the mistake. The correct categ...   \n",
       "269   I apologize for the confusion. Based on the pr...   \n",
       "311   I apologize for the mistake. After reviewing t...   \n",
       "430   Yes, based on the text \"I am experiencing seve...   \n",
       "600   I apologize for the mistake. Based on the prov...   \n",
       "606   I apologize for the confusion. Based on the pr...   \n",
       "671   I apologize for the mistake. After re-evaluati...   \n",
       "823   I apologize for the confusion. As a zero-shot ...   \n",
       "1054  Yes, I am quite confident that the most releva...   \n",
       "\n",
       "                                        text_translated  \\\n",
       "102   He didn't bring the brother's exam paper, that...   \n",
       "269   Friend, that is very old. Do you remember when...   \n",
       "311   My father was the chairman of the Zakat commit...   \n",
       "430   Thanks sir. Today, sir, I got stuck with 3 dut...   \n",
       "600   Brother, I asked you with great respect and al...   \n",
       "606   Friend, that is very old. Do you remember when...   \n",
       "671   You shouldn't do it in this way, friend! What ...   \n",
       "823   And she always talks about one thing, movies, ...   \n",
       "1054  I am unable to check my exam results on my mob...   \n",
       "\n",
       "                                       text_translated2 gpt_pred2  clf_pred  \\\n",
       "102   He didn't bring the brother's exam paper, that...      None         0   \n",
       "269   Friend, that is very old. Do you remember when...      None         3   \n",
       "311   My father was the chairman of the Zakat commit...      None         6   \n",
       "430   Thanks sir. Today, sir, I got stuck with 3 dut...      None         0   \n",
       "600   Brother, I asked you with great respect and al...      None         0   \n",
       "606   Friend, that is very old. Do you remember when...      None         3   \n",
       "671   You shouldn't do it in this way, friend! What ...      None         3   \n",
       "823   And she always talks about one thing, movies, ...      None         3   \n",
       "1054  I am unable to check my exam results on my mob...      None         3   \n",
       "\n",
       "     clf_pred_emotion  \n",
       "102           neutral  \n",
       "269           disgust  \n",
       "311           sadness  \n",
       "430           neutral  \n",
       "600           neutral  \n",
       "606           disgust  \n",
       "671           disgust  \n",
       "823           disgust  \n",
       "1054          disgust  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[ df_dev['gpt_pred2'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5961083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ChatGPT made no prediction, choose the prediction coming from the classifier\n",
    "def improve_predictions(row):\n",
    "    if row['gpt_pred2'] is None:\n",
    "        row['gpt_pred2'] = row['clf_pred_emotion']\n",
    "    return row\n",
    "\n",
    "df_dev = df_dev.apply( improve_predictions, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f659274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger     0.3846    0.1429    0.2083        35\n",
      "anticipation     0.1045    0.7872    0.1845        94\n",
      "     disgust     0.5000    0.2124    0.2981       113\n",
      "        fear     0.2963    0.1538    0.2025        52\n",
      "         joy     0.7412    0.4809    0.5833       131\n",
      "        love     0.3043    0.4118    0.3500        17\n",
      "     neutral     0.4463    0.1392    0.2122       388\n",
      "    optimism     0.1852    0.0455    0.0730       110\n",
      "   pessimism     0.3333    0.2069    0.2553        29\n",
      "     sadness     0.4727    0.4194    0.4444        62\n",
      "    surprise     0.5000    0.0286    0.0541        35\n",
      "       trust     0.1094    0.0560    0.0741       125\n",
      "\n",
      "    accuracy                         0.2351      1191\n",
      "   macro avg     0.3648    0.2570    0.2450      1191\n",
      "weighted avg     0.3872    0.2351    0.2416      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev      = df_dev['Emotion'].values\n",
    "y_dev_pred = df_dev['gpt_pred2'].values\n",
    "print( classification_report( y_dev, y_dev_pred, digits=4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "314abdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df with translation\n",
    "file = 'data/df_dev_translated.pkl'\n",
    "df_dev.to_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfd118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3c932f",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693df45",
   "metadata": {},
   "source": [
    "### Prompts and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd45015",
   "metadata": {},
   "source": [
    "__Prompt 1__: Act as a very accurate zero-shot text classifier and classify the provided text as one most relevant emotion from the following list of emotions: neutral, joy, trust, disgust, optimism, anticipation, sadness, fear, surprise, anger, pessimism, love. Classify the following text with the most relevant emotion from the above list and output ONLY the emotion and nothing else: \"This is a text sample\"\n",
    "\n",
    "Dev set results:\n",
    "```\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "     neutral     0.2500    0.3143    0.2785        35\n",
    "         joy     0.1076    0.7553    0.1883        94\n",
    "       trust     0.5185    0.1239    0.2000       113\n",
    "     disgust     0.5000    0.1346    0.2121        52\n",
    "    optimism     0.7523    0.6260    0.6833       131\n",
    "anticipation     0.2692    0.4118    0.3256        17\n",
    "     sadness     0.4449    0.2603    0.3285       388\n",
    "        fear     0.3000    0.0273    0.0500       110\n",
    "    surprise     0.3333    0.0345    0.0625        29\n",
    "       anger     0.5217    0.3871    0.4444        62\n",
    "   pessimism     0.6667    0.0571    0.1053        35\n",
    "        love     0.0455    0.0080    0.0136       125\n",
    "\n",
    "    accuracy                         0.2720      1191\n",
    "   macro avg     0.3925    0.2617    0.2410      1191\n",
    "weighted avg     0.4057    0.2720    0.2719      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2c74a",
   "metadata": {},
   "source": [
    "__Prompt 2__: Act as a very accurate zero-shot text classifier and classify the provided text into one most relevant category from the following list of categories: neutral, joy, trust, disgust, optimism, anticipation, sadness, fear, surprise, anger, pessimism, love. Classify the following text with the most relevant category from the above list and output ONLY the category and nothing else: \"This is a text sample\"\n",
    "\n",
    "__Some of the responses__:\n",
    "'Yes, I am sure. As a language model, I can only classify text based on the patterns and context of the language that I have been trained on. If the text is written in a language that I am not familiar with, I cannot accurately'\n",
    "\n",
    "Dev set results:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       anger     0.3158    0.3429    0.3288        35\n",
    "anticipation     0.1059    0.6277    0.1813        94\n",
    "     disgust     0.4651    0.1770    0.2564       113\n",
    "        fear     0.3030    0.1923    0.2353        52\n",
    "         joy     0.7360    0.7023    0.7187       131\n",
    "        love     0.2500    0.4118    0.3111        17\n",
    "     neutral     0.4422    0.3351    0.3812       388\n",
    "    optimism     0.4167    0.0455    0.0820       110\n",
    "   pessimism     0.2500    0.0345    0.0606        29\n",
    "     sadness     0.5106    0.3871    0.4404        62\n",
    "    surprise     0.6667    0.0571    0.1053        35\n",
    "       trust     0.0000    0.0000    0.0000       125\n",
    "\n",
    "    accuracy                         0.3039      1191\n",
    "   macro avg     0.3718    0.2761    0.2584      1191\n",
    "weighted avg     0.3943    0.3039    0.3013      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f3021",
   "metadata": {},
   "source": [
    "__Prompt 3__: Translate the text below into English and classify it into one most relevant category from the following list of categories: neutral, joy, trust, disgust, optimism, anticipation, sadness, fear, surprise, anger, pessimism, love. Output ONLY the category and nothing else. Text: \"This is a text sample\"\n",
    "\n",
    "__Some of the responses to additional clarifying question: \"Are you sure about that?\":__\n",
    "* Yes, I am sure. As a language model, I can only classify text that is in a language I understand. The text you provided appears to be in an unknown language or possibly a code, and therefore cannot be classified into any of the given                                                1\n",
    "* Yes, I am sure. The text is not in English and cannot be translated into English. As a language model, I can only classify text that is in a language that I understand.                                                                 * Yes, I am sure. The text \"a ja yr me bhr ho a bi ja\" does not contain any recognizable words or phrases in English or any other language that I am familiar with. Therefore, it cannot be classified into any of the                     \n",
    "* Yes, I am sure. The text is not in English and I am not programmed to recognize or classify text in any other language.                                                                                                             * I apologize for the mistake. The text \"tjhe kehnx ki zar0ort nae xr\" appears to be written in a non-standard way and does not have a clear meaning in any language. Therefore, I cannot accurately classify it                             \n",
    "* I apologize for the mistake. The text actually translates to: \"I have to complete assignments in one day, remember lectures in one day, and complete lab tasks in one day... I didn't go to the market with everyone today... and maybe I * I apologize for the mistake. The text contains offensive language and is not appropriate for classification. As an AI language model, I strive to maintain a professional and respectful tone. Please provide a different text for classification.\n",
    "\n",
    "Dev set results:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       anger     0.4091    0.2571    0.3158        35\n",
    "anticipation     0.1290    0.0426    0.0640        94\n",
    "     disgust     0.4545    0.0885    0.1481       113\n",
    "        fear     0.4500    0.1731    0.2500        52\n",
    "         joy     0.7263    0.5267    0.6106       131\n",
    "        love     0.2400    0.3529    0.2857        17\n",
    "     neutral     0.3984    0.9149    0.5551       388\n",
    "    optimism     0.5000    0.0364    0.0678       110\n",
    "   pessimism     0.3333    0.0345    0.0625        29\n",
    "     sadness     0.5417    0.4194    0.4727        62\n",
    "    surprise     0.6667    0.0571    0.1053        35\n",
    "       trust     0.1304    0.0240    0.0405       125\n",
    "\n",
    "    accuracy                         0.4181      1191\n",
    "   macro avg     0.4150    0.2439    0.2482      1191\n",
    "weighted avg     0.4139    0.4181    0.3311      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9d366",
   "metadata": {},
   "source": [
    "__Prompt 4 (2 successive prompts)__: The text below may contain words or phrases in Urdu or Roman Urdu along with English. Translate the text below and write it in English only. Output only the English translation and nothing else. Text: \"This is a text sample\" \n",
    "\n",
    "Act as a very accurate zero-shot text classifier and classify the provided text into one most relevant category from the following list of categories: neutral, joy, trust, disgust, optimism, anticipation, sadness, fear, surprise, anger, pessimism, love. Output only one most relevant category and nothing else. Text: \"This is a text sample\" \n",
    "\n",
    "__Some of the responses to additional clarifying question: \"Are you sure about that?\"__:\n",
    "* I apologize for the mistake. The correct category for the given text is \"Disapproval\".',\n",
    "* I apologize for the confusion. Based on the provided text, the most relevant category would be \"nostalgia\" which is not included in the list of categories provided earlier. However, if I have to choose from the given categories, the most relevant',\n",
    "* I apologize for the mistake. After reviewing the text again, the most relevant category for the provided text is \"distrust\".',\n",
    "* Yes, based on the text \"I am experiencing severe pain\", the most relevant category is \"pain\".',\n",
    "* I apologize for the mistake. Based on the provided text, the most relevant category would be \"Respectful\" rather than \"Respect\".',\n",
    "* I apologize for the confusion. Based on the provided text, the most relevant category would be \"nostalgia\" which is not included in the list of categories provided earlier. However, if I have to choose from the given categories, the most relevant',\n",
    "* I apologize for the mistake. After re-evaluating the text, the most relevant category for the provided text is \"doubt\".',\n",
    "* I apologize for the confusion. As a zero-shot text classifier, I can only classify the text based on the given categories. In this case, the most relevant category for the provided text \"And she always talks about one thing, movies, movies.\"',\n",
    "* Yes, I am quite confident that the most relevant category for the given text \"I am unable to check my exam results on my mobile phone, friend\" is frustration.'\n",
    "\n",
    "\n",
    "Dev set results:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       anger     0.3846    0.1429    0.2083        35\n",
    "anticipation     0.1045    0.7872    0.1845        94\n",
    "     disgust     0.5000    0.2124    0.2981       113\n",
    "        fear     0.2963    0.1538    0.2025        52\n",
    "         joy     0.7412    0.4809    0.5833       131\n",
    "        love     0.3043    0.4118    0.3500        17\n",
    "     neutral     0.4463    0.1392    0.2122       388\n",
    "    optimism     0.1852    0.0455    0.0730       110\n",
    "   pessimism     0.3333    0.2069    0.2553        29\n",
    "     sadness     0.4727    0.4194    0.4444        62\n",
    "    surprise     0.5000    0.0286    0.0541        35\n",
    "       trust     0.1094    0.0560    0.0741       125\n",
    "\n",
    "    accuracy                         0.2351      1191\n",
    "   macro avg     0.3648    0.2570    0.2450      1191\n",
    "weighted avg     0.3872    0.2351    0.2416      1191\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
